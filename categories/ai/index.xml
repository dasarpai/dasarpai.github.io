<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on </title>
    <link>/categories/ai/</link>
    <description>Recent content in AI on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Dr. Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Dr. Hari Thapliyaal)</webMaster>
    <copyright>© 2025 Dr. Hari Thapliyaal</copyright>
    <lastBuildDate>Sat, 24 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Understanding Vocal Frequencies in Speech-to-Text AI Applications</title>
      <link>/dsblog/understanding-vocal-frequencies/</link>
      <pubDate>Sat, 24 May 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/understanding-vocal-frequencies/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6280-understanding-vocal-frequencies.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Voice Signal Processing 
    &lt;div id=&#34;exploring-voice-signal-processing&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-voice-signal-processing&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In AI projects that involve human voice, such as mood detection, speaker identification, language recognition, and speech-to-text transcription, it&amp;rsquo;s essential to understand the characteristics of vocal frequencies and how sound behaves. This knowledge enables data scientists and engineers to develop effective models for tasks like real-time translation, sentiment analysis, and biometric recognition.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>BitNet b1.58-2B4T: Revolutionary Binary Neural Network for Efficient AI</title>
      <link>/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6263-BitNet-b1.58-2B4T.jpg&#34; alt=&#34;BitNet b1.58-2B4T&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.12285&#34; target=&#34;_blank&#34;&gt;Archive Paper Link&lt;/a&gt;&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;BitNet b1.58-2B4T: The Future of Efficient AI Processing 
    &lt;div id=&#34;bitnet-b158-2b4t-the-future-of-efficient-ai-processing&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#bitnet-b158-2b4t-the-future-of-efficient-ai-processing&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;A History of 1 bit Transformer Model 
    &lt;div id=&#34;a-history-of-1-bit-transformer-model&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#a-history-of-1-bit-transformer-model&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A paper &amp;ldquo;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&amp;rdquo; was published by Stanford University, ETH Zürich, and EPFL. It was published on October 2023 (published on arXiv on October 17, 2023). &lt;a href=&#34;https://arxiv.org/pdf/2310.11453&#34; target=&#34;_blank&#34;&gt;Standord Paper Link&lt;/a&gt;. The core Concept of 1.58 bits per parameter, was introduced here. This demonstrated that LLMs could be effectively trained and operated with extremely low-bit representation while maintaining competitive performance&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Retrieval-Augmented Generation with Conflicting Evidence</title>
      <link>/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</link>
      <pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6261-Retrieval-Augmented-Generation-with-Conflicting-Evidence.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence 
    &lt;div id=&#34;paper-summary-retrieval-augmented-generation-with-conflicting-evidence&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary-retrieval-augmented-generation-with-conflicting-evidence&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13079&#34; target=&#34;_blank&#34;&gt;arXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The hypothesis of this paper is that &lt;strong&gt;real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents&lt;/strong&gt;. The authors argue that prior work has largely addressed these challenges in isolation.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>LLM Internal Encoding of Truthfulness and Hallucinations</title>
      <link>/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6260-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations.jpg&#34; alt=&#34;LLM Internal Encoding of Truthfulness and Hallucinations&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary: LLM Internal Encoding of Truthfulness and Hallucinations 
    &lt;div id=&#34;paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;The objective of this paper is to gain a deeper understanding of errors produced by large language models (LLMs) by examining their internal representations. The authors aim to reveal how information about the truthfulness of LLM outputs is encoded internally, going beyond extrinsic, behavioral analysis. They also seek to investigate the relationship between these internal representations and the external behavior of LLMs, including their tendency to produce inaccuracies or &amp;ldquo;hallucinations&amp;rdquo;. Furthermore, the paper intends to explore whether internal representations can be used to predict the types of errors LLMs make and to detect the correct answer even when the model generates an incorrect one.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Benchmarks for 2025</title>
      <link>/dsblog/ai-benchmarks-2025/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ai-benchmarks-2025/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6256-aibenchmarks-for-2025.jpg&#34; alt=&#34;AI Benchmark for 2025&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Benchmarks for 2025 
    &lt;div id=&#34;ai-benchmarks-for-2025&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-benchmarks-for-2025&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;A term &lt;strong&gt;&amp;ldquo;AI benchmark&amp;rdquo;&lt;/strong&gt; is thrown around a lot and can be confusing because it’s used in slightly different ways depending on the context. In this artcile we will try to understand what are the different meaning of this term and what are the latest AI benchmarks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Overview of AI Benchmark Explorer Tool</title>
      <link>/dsblog/aibenchmark-explorer/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/aibenchmark-explorer/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6255-dsr-aibenchmark-explorer.jpg&#34; alt=&#34;AI Benchmark Explorer&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Overview of AI Benchmark Explorer Tool&lt;/strong&gt; 
    &lt;div id=&#34;overview-of-ai-benchmark-explorer-tool&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#overview-of-ai-benchmark-explorer-tool&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;AI professionals, including Change Drivers, Managers, and Scientists, often face challenges despite clearly understanding the problems they aim to solve. Key issues include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Identifying Appropriate Datasets&lt;/strong&gt;: Determining which datasets are best suited for a specific problem can be difficult.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selecting Evaluation Metrics&lt;/strong&gt;: Choosing the right metrics to assess the performance of a solution is crucial for accurate evaluation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Benchmarking Against Existing Models&lt;/strong&gt;: Understanding the performance metrics of existing models for the same problem helps in setting realistic expectations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploring Tried Architectures&lt;/strong&gt;: Reviewing architectures that others have implemented for similar problems can provide valuable insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assessing Problem Novelty&lt;/strong&gt;: Determining whether the problem has already been solved or requires novel approaches is essential for resource allocation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sourcing Solutions&lt;/strong&gt;: Deciding between utilizing open-source solutions or opting for proprietary alternatives impacts cost and flexibility.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addressing these challenges is vital for the successful development and implementation of AI solutions.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>A Comprehensive Guide to Evaluate Generative Models</title>
      <link>/dsblog/guide-to-evaluate-generative-models/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/guide-to-evaluate-generative-models/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6240-Guide-to-Evaluate-Generative-Models.jpg&#34; alt=&#34;Generative Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;A Comprehensive Guide to Evaluate Generative Models 
    &lt;div id=&#34;a-comprehensive-guide-to-evaluate-generative-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#a-comprehensive-guide-to-evaluate-generative-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In the rapidly evolving landscape of artificial intelligence, generative models have emerged as powerful tools capable of creating lifelike images, coherent text narratives, and even realistic audio. From large language models (LLMs) like GPT, Gemini, and Claude to image generators like Stable Diffusion, these systems are reshaping creativity, communication, and problem-solving. However, their potential is only as good as our ability to evaluate them effectively.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Reasoning Models in AI Marketplace, Feb 25</title>
      <link>/dsblog/exploring-reasoning-models/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-reasoning-models/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6229-Exploring-Reasoning-Models.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Reasoning Models in AI Marketplace - Feb&#39;2025 
    &lt;div id=&#34;exploring-reasoning-models-in-ai-marketplace---feb2025&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-reasoning-models-in-ai-marketplace---feb2025&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;What Makes a Model a &amp;ldquo;Reasoning Model&amp;rdquo;?&lt;/strong&gt; 
    &lt;div id=&#34;what-makes-a-model-a&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-makes-a-model-a&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The term &amp;ldquo;reasoning model&amp;rdquo; is not strictly defined but generally refers to models that &lt;strong&gt;explicitly demonstrate structured problem-solving abilities&lt;/strong&gt;, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logical inference&lt;/strong&gt; (deductive/inductive reasoning)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-step problem-solving&lt;/strong&gt; (e.g., math, coding, puzzles)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common-sense reasoning&lt;/strong&gt; (understanding implicit context)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Causal reasoning&lt;/strong&gt; (connecting causes and effects).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Modern models achieve this through architectural innovations (e.g., chain-of-thought prompting, sparse attention) and training on datasets enriched with reasoning tasks (e.g., math problems, logic puzzles).&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring DeepSeek R1: The AI That Thinks Like a Human</title>
      <link>/dsblog/exploring-deepseek-r1/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-deepseek-r1/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6228-Exploring-DeepSeek-R1.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring DeepSeek R1: The AI That Thinks Like a Human 
    &lt;div id=&#34;exploring-deepseek-r1-the-ai-that-thinks-like-a-human&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-deepseek-r1-the-ai-that-thinks-like-a-human&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Artificial Intelligence (AI) has come a long way, but few models have captured the imagination of researchers, developers, and businesses like &lt;strong&gt;DeepSeek R1&lt;/strong&gt;. Developed by &lt;strong&gt;DeepSeek&lt;/strong&gt;, a cutting-edge AI research organization, DeepSeek R1 is not just another AI model—it’s a reasoning engine designed to tackle complex tasks with human-like precision. Whether it’s solving intricate math problems, writing code, or analyzing data, DeepSeek R1 is reasoning in every step. In this article, we’ll explore into what makes DeepSeek R1 special, how it works, and why it’s a game-changer for industries and individuals alike.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Integrating Ollama AI Models and Open WebUI with Docker: A Step-by-Step Guide</title>
      <link>/dsblog/integrating-ollama-and-open-webui-with-docker/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/integrating-ollama-and-open-webui-with-docker/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6227-Integrating-Ollama-and-Open-WebUI-on-Docker.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Integrating Ollama AI Models and Open WebUI on Docker 
    &lt;div id=&#34;integrating-ollama-ai-models-and-open-webui-on-docker&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#integrating-ollama-ai-models-and-open-webui-on-docker&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Ollama, Opensource LLM Framework from Meta, provides a powerful way to work with large language models (LLMs) efficiently. While Open WebUI is a user-friendly interface that simplifies interaction with Ollama-hosted AI models. You can host Ollama and WebUI on your local machine. By using Docker, we can containerize these components, ensuring a seamless and reproducible setup across different environments. This guide will walk you through integrating Ollama and Open WebUI within Docker.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring the Local Location of Ollama Models on WSL2</title>
      <link>/dsblog/exploring-ollama-models-location-on-wsl2/</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-ollama-models-location-on-wsl2/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6220-Exploring-the-Local-Location-of-Ollama-Models-on-wsl2.jpg&#34; alt=&#34;Exploring the Location of Ollama Models on Local Machine&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring the Location of Ollama Models on Local Machine 
    &lt;div id=&#34;exploring-the-location-of-ollama-models-on-local-machine&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-the-location-of-ollama-models-on-local-machine&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Objective 
    &lt;div id=&#34;objective&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#objective&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Many times you may have a question like, I have installed ollama in wsl and download some ollama models. Ollama list shows me those models. I want to know where they are stored. Why it is needed? Because you want to use that location as volume in your docker container. And you don&amp;rsquo;t want to download the model everytime you start the container neither you want to have duplicate copy of the same model on your machine or network.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>State of the Art Image Generation Models in Computer Vision: A Comprehensive Overview</title>
      <link>/dsblog/state-of-the-art-image-generation-models-in-computer-vision/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/state-of-the-art-image-generation-models-in-computer-vision/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6219-State-of-the-Art-Computer-Vision-Models.jpg&#34; alt=&#34;State-of-the-Art 3D Image Generation Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;State of the Art Computer Vision Models 
    &lt;div id=&#34;state-of-the-art-computer-vision-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#state-of-the-art-computer-vision-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What are the different methods of Image generation? 
    &lt;div id=&#34;what-are-the-different-methods-of-image-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-the-different-methods-of-image-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;There are several methods for image generation. Diffusion models are currently the state-of-the-art due to their balance of quality, flexibility, and scalability. However, other methods like GANs and autoregressive models remain relevant for specific use cases. Let&amp;rsquo;s see them one by one.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Tokenization and Embedding in NLP</title>
      <link>/dsblog/exploring-tokenization-and-embedding-in-nlp/</link>
      <pubDate>Fri, 31 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-tokenization-and-embedding-in-nlp/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6215-Exploring-Tokenization-in-AI.jpg&#34; alt=&#34;Exploring Tokenization and Embedding in NLP&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Tokenization and Embedding in NLP 
    &lt;div id=&#34;exploring-tokenization-and-embedding-in-nlp&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-tokenization-and-embedding-in-nlp&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is tokenization?&lt;/li&gt;
&lt;li&gt;What are different Tokenzation schemes?&lt;/li&gt;
&lt;li&gt;What is OOV (Out-of-Vocabulary) in Tokenization?&lt;/li&gt;
&lt;li&gt;If a word does not exist in embedding model&amp;rsquo;s vocabulary, then how tokenization and embedding is done?&lt;/li&gt;
&lt;li&gt;What is criteria of splitting a word?&lt;/li&gt;
&lt;li&gt;What is Subword Tokenization?&lt;/li&gt;
&lt;li&gt;How FastText Tokenization works?&lt;/li&gt;
&lt;li&gt;What is role of [CLS] token?&lt;/li&gt;
&lt;li&gt;What is WordPiece and how it works?&lt;/li&gt;
&lt;li&gt;What is BPE (Byte Pair Encoding), and how it works?&lt;/li&gt;
&lt;li&gt;What is SentencePiece and how it works?&lt;/li&gt;
&lt;li&gt;For Indian languages what tokenization schemes is the best?&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is tokenization? 
    &lt;div id=&#34;what-is-tokenization&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-tokenization&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Tokenization is the process of breaking text into smaller units (tokens), such as words, subwords, or characters, for NLP tasks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Understanding Contextual Embedding in Transformers</title>
      <link>/dsblog/understanding-contextual-embedding-in-transformers/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/understanding-contextual-embedding-in-transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6214-Understanding-Contextual-Embedding-in-Transformer.jpg&#34; alt=&#34;Understanding Contextual Embedding in Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Embedding?&lt;/li&gt;
&lt;li&gt;What is Fixed Embedding?&lt;/li&gt;
&lt;li&gt;How Transformers Handle Context&lt;/li&gt;
&lt;li&gt;How this token &amp;lsquo;bank&amp;rsquo; and corresponding embedding is stored in embedding database?&lt;/li&gt;
&lt;li&gt;How contextural embedding is generated?&lt;/li&gt;
&lt;li&gt;What will be the output size of attention formula softmax?&lt;/li&gt;
&lt;li&gt;What is meaning of a LLM has context length of 2 million tokens?&lt;/li&gt;
&lt;li&gt;How many attention layers we keep in transformer like gpt4?&lt;/li&gt;
&lt;li&gt;What is the meaning of 96 attention layers, are they attention head count?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Embedding? 
    &lt;div id=&#34;what-is-embedding&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-embedding&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Understanding the Working of CNN</title>
      <link>/dsblog/understanding-working-of-cnn/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/understanding-working-of-cnn/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6213-Understanding-Working-of-CNN.jpg&#34; alt=&#34;Understanding the Working of CNN&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Understanding the Working of CNN 
    &lt;div id=&#34;understanding-the-working-of-cnn&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-the-working-of-cnn&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Power of Chinese AI Models</title>
      <link>/dsblog/power-of-chinese-ai-models/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/power-of-chinese-ai-models/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6212-Power-of-Chinese-AI-Models.jpg&#34; alt=&#34;Power of Chinese AI Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Power of Chinese AI Models 
    &lt;div id=&#34;power-of-chinese-ai-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#power-of-chinese-ai-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;After the Deepseek R1 turmoil in the market, there has been a shift in attention towards China. The West is now looking towards the East, and even those in the East are turning their gaze northward.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Computer Vision Research Work</title>
      <link>/dsblog/computer-vision-research-work/</link>
      <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/computer-vision-research-work/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6211-Computer-Vision-Research-Work.jpg&#34; alt=&#34;Computer Vision Research Work&amp;quot;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Computer Vision Research Work 
    &lt;div id=&#34;computer-vision-research-work&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#computer-vision-research-work&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;When we talk about &amp;ldquo;vision&amp;rdquo; capabilities, most people don&amp;rsquo;t understand how complex the brain is in processing the visual spectrum (light signals). What kind of processing happens inside our brain that allows us to understand color, depth, motion, speed, segments, objects, scenes, different kinds of art, drawings, culture, etc.? Until recently, when &amp;ldquo;computer vision&amp;rdquo; became a serious field in AI, only neurology researchers, surgeons, and brain specialists had some insights into these processes. But since 2012 (AlexNet Paper), with new papers being published almost every month, we are constantly learning how far we&amp;rsquo;ve come in computer vision. This article is not only about the chronology of computer vision but also about software engineers, computer scientists, AI engineers, and everyone who wants to understand how their phone performs certain computer visions tasks and becomes intelligent.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring AI Benchmarks &amp; Leaderboards</title>
      <link>/dsblog/exploring-ai-benchmarks-and-leaderboards/</link>
      <pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-ai-benchmarks-and-leaderboards/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6210-Exploring-AI-Benchmarks-and-Leaderboards.jpg&#34; alt=&#34;Exploring AI Benchmarks &amp;amp; Leaderboards&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring AI Benchmarks &amp;amp; Leaderboards 
    &lt;div id=&#34;exploring-ai-benchmarks--leaderboards&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-ai-benchmarks--leaderboards&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;benchmark&lt;/strong&gt; is a standardized test or set of metrics used to measure and compare the performance, capabilities, or quality of systems, models, or algorithms. In the context of &lt;strong&gt;AI and machine learning&lt;/strong&gt;, benchmarks provide a way to evaluate how well models perform on specific tasks or datasets, often with respect to predefined metrics like accuracy, speed, robustness, or resource efficiency.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Types of Models</title>
      <link>/dsblog/exploring-types-of-models/</link>
      <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-types-of-models/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6209-Exploring-Types-of-Models.jpg&#34; alt=&#34;Exploring Types of Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Understanding Different Types of Models 
    &lt;div id=&#34;understanding-different-types-of-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-different-types-of-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A model is a simplified representation or abstraction of a system, concept, or phenomenon around us. It is used to analyze, understand, predict, or simulate real-world behavior. Models can take many forms, depending on the context in which they are used. For example you also say that I have created a Data Model, Functional Model, UI Model, Simulation Model etc.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring AI Agents</title>
      <link>/dsblog/exploring-ai-agents/</link>
      <pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-ai-agents/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6207-Exploring-AI-Agents.jpg&#34; alt=&#34;Exploring AI Agents&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring AI Agents 
    &lt;div id=&#34;exploring-ai-agents&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-ai-agents&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;As of the start of 2025, we are hearing a lot of buzz around Agents, Workflow, System, AI Employee etc. In this article, we are trying to understand some important concepts around these terms in question answer format. These answers are derived from general AI knowledge, principles, and concepts based on publicly available information about AI agents, workflows, and systems. Specific sources include academic literature, industry whitepapers, and commonly understood practices in AI development and usage.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Adapting AI Models to the Latest Information: Methods and Approaches</title>
      <link>/dsblog/adapting-ai-models-to-the-latest-information/</link>
      <pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/adapting-ai-models-to-the-latest-information/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6202-Adapting-AI-Models-to-the-Latest-Information.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Adapting AI Models to the Latest Information: Methods and Approaches 
    &lt;div id=&#34;adapting-ai-models-to-the-latest-information-methods-and-approaches&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#adapting-ai-models-to-the-latest-information-methods-and-approaches&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Artificial Intelligence offers numerous advantages over traditional search engines like Google or Bing, but it also has a notable limitation: its knowledge is often frozen in the past. Unless a model is retrained or updated with newly available data, it cannot respond effectively to current business, political, social, or scientific developments.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Visualizing Transformers and Attention</title>
      <link>/dsblog/Visualizing-transformers-and-attention/</link>
      <pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Visualizing-transformers-and-attention/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6189-Visualizing-transformers-and-attention.jpg&#34; alt=&#34;Visualizing transformers and attention&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Visualizing Transformers and Attention 
    &lt;div id=&#34;visualizing-transformers-and-attention&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#visualizing-transformers-and-attention&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;This is the summary note from Grant Sanderson&amp;rsquo;s talk at TNG Big Tech 2024. My earlir article on transformers can be found &lt;a href=&#34;/dsblog/transformers-demystified-a-step-by-step-guide&#34;&gt;here&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Transformers and Their Flexibility&lt;/strong&gt; 
    &lt;div id=&#34;transformers-and-their-flexibility&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformers-and-their-flexibility&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📜 &lt;strong&gt;Origin:&lt;/strong&gt; Introduced in 2017 in the &amp;ldquo;Attention is All You Need&amp;rdquo; paper, originally for machine translation.&lt;/li&gt;
&lt;li&gt;🌍 &lt;strong&gt;Applications Beyond Translation:&lt;/strong&gt; Used in transcription (e.g., Whisper), text-to-speech, and even image classification.&lt;/li&gt;
&lt;li&gt;🤖 &lt;strong&gt;Chatbot Models:&lt;/strong&gt; Focused on models trained to predict the next token in a sequence, generating text iteratively one token at a time.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Next Token Prediction and Creativity&lt;/strong&gt; 
    &lt;div id=&#34;next-token-prediction-and-creativity&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#next-token-prediction-and-creativity&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🔮 &lt;strong&gt;Prediction Process:&lt;/strong&gt; Predicts probabilities for possible next tokens, selects one, and repeats the process.&lt;/li&gt;
&lt;li&gt;🌡️ &lt;strong&gt;Temperature Control:&lt;/strong&gt; Adjusting randomness in token selection affects creativity vs. predictability in outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Tokens and Tokenization&lt;/strong&gt; 
    &lt;div id=&#34;tokens-and-tokenization&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#tokens-and-tokenization&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🧩 &lt;strong&gt;What are Tokens?&lt;/strong&gt; Subdivisions of input data (words, subwords, punctuation, or image patches).&lt;/li&gt;
&lt;li&gt;🔡 &lt;strong&gt;Why Not Characters?&lt;/strong&gt; Using characters increases context size and computational complexity; tokens balance meaning and computational efficiency.&lt;/li&gt;
&lt;li&gt;📖 &lt;strong&gt;Byte Pair Encoding (BPE):&lt;/strong&gt; A common method for tokenization.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Embedding Tokens into Vectors&lt;/strong&gt; 
    &lt;div id=&#34;embedding-tokens-into-vectors&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#embedding-tokens-into-vectors&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📏 &lt;strong&gt;Embedding:&lt;/strong&gt; Tokens are mapped to high-dimensional vectors representing their meaning.&lt;/li&gt;
&lt;li&gt;🗺️ &lt;strong&gt;Contextual Meaning:&lt;/strong&gt; Vectors evolve through the network to capture context, disambiguate meaning, and encode relationships.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;The Attention Mechanism&lt;/strong&gt; 
    &lt;div id=&#34;the-attention-mechanism&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#the-attention-mechanism&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🔍 &lt;strong&gt;Purpose:&lt;/strong&gt; Enables tokens to &amp;ldquo;attend&amp;rdquo; to others, updating their vectors based on relevance.&lt;/li&gt;
&lt;li&gt;🔑 &lt;strong&gt;Key Components:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Query Matrix: Encodes what a token is &amp;ldquo;looking for.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Key Matrix: Encodes how a token responds to queries.&lt;/li&gt;
&lt;li&gt;Value Matrix: Encodes information passed between tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;🧮 &lt;strong&gt;Calculations:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Dot Product: Measures alignment between keys and queries.&lt;/li&gt;
&lt;li&gt;Softmax: Converts dot products into normalized weights for updates.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;⛓️ &lt;strong&gt;Masked Attention:&lt;/strong&gt; Ensures causality by blocking future tokens from influencing past ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Multi-Headed Attention&lt;/strong&gt; 
    &lt;div id=&#34;multi-headed-attention&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multi-headed-attention&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;💡 &lt;strong&gt;Parallel Heads:&lt;/strong&gt; Multiple attention heads allow different types of relationships (e.g., grammar, semantic context) to be processed simultaneously.&lt;/li&gt;
&lt;li&gt;🚀 &lt;strong&gt;Efficiency on GPUs:&lt;/strong&gt; Designed to maximize parallelization for faster computation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Multi-Layer Perceptrons (MLPs)&lt;/strong&gt; 
    &lt;div id=&#34;multi-layer-perceptrons-mlps&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multi-layer-perceptrons-mlps&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🤔 &lt;strong&gt;Role in Transformers:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Add capacity for general knowledge and non-contextual reasoning.&lt;/li&gt;
&lt;li&gt;Store facts learned during training, e.g., associations like &amp;ldquo;Michael Jordan plays basketball.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;🔢 &lt;strong&gt;Parameters:&lt;/strong&gt; MLPs hold the majority of the model’s parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Training Transformers&lt;/strong&gt; 
    &lt;div id=&#34;training-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#training-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📚 &lt;strong&gt;Learning Framework:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Models are trained on vast datasets using next-token prediction, requiring no manual labels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost Function:&lt;/strong&gt; Measures prediction accuracy using negative log probabilities, guiding parameter updates.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;🏔️ &lt;strong&gt;Optimization:&lt;/strong&gt; Gradient descent navigates a high-dimensional cost surface to minimize error.&lt;/li&gt;
&lt;li&gt;🌐 &lt;strong&gt;Pretraining:&lt;/strong&gt; Allows large-scale unsupervised learning before fine-tuning with human feedback.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Embedding Space and High Dimensions&lt;/strong&gt; 
    &lt;div id=&#34;embedding-space-and-high-dimensions&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#embedding-space-and-high-dimensions&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🔄 &lt;strong&gt;Semantic Clusters:&lt;/strong&gt; Similar words cluster together; directions in the space encode relationships (e.g., gender: King - Male + Female = Queen).&lt;/li&gt;
&lt;li&gt;🌌 &lt;strong&gt;High Dimensionality:&lt;/strong&gt; Embedding spaces have thousands of dimensions, enabling distinct representations of complex concepts.&lt;/li&gt;
&lt;li&gt;📈 &lt;strong&gt;Scaling Efficiency:&lt;/strong&gt; High-dimensional spaces allow exponentially more &amp;ldquo;almost orthogonal&amp;rdquo; directions for encoding meanings.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Practical Applications&lt;/strong&gt; 
    &lt;div id=&#34;practical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#practical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;✍️ &lt;strong&gt;Language Models:&lt;/strong&gt; Effective for chatbots, summarization, and more due to their generality and parallel processing.&lt;/li&gt;
&lt;li&gt;🖼️ &lt;strong&gt;Multimodal Models:&lt;/strong&gt; Transformers can integrate text, images, and sound by treating all as tokens in a unified framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Challenges and Limitations&lt;/strong&gt; 
    &lt;div id=&#34;challenges-and-limitations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#challenges-and-limitations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📏 &lt;strong&gt;Context Size Limitations:&lt;/strong&gt; Attention grows quadratically with context size, requiring optimization for large contexts.&lt;/li&gt;
&lt;li&gt;♻️ &lt;strong&gt;Inference Redundancy:&lt;/strong&gt; Token-by-token generation can involve redundant computations; caching mitigates this at inference time.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Engineering and Design&lt;/strong&gt; 
    &lt;div id=&#34;engineering-and-design&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#engineering-and-design&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🛠️ &lt;strong&gt;Hardware Optimization:&lt;/strong&gt; Transformers are designed to exploit GPUs&amp;rsquo; parallelism for efficient matrix multiplication.&lt;/li&gt;
&lt;li&gt;🔗 &lt;strong&gt;Residual Connections:&lt;/strong&gt; Baked into the architecture to enhance stability and ease of training.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;The Power of Scale&lt;/strong&gt; 
    &lt;div id=&#34;the-power-of-scale&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#the-power-of-scale&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📈 &lt;strong&gt;Scaling Laws:&lt;/strong&gt; Larger models and more data improve performance, often qualitatively.&lt;/li&gt;
&lt;li&gt;🔄 &lt;strong&gt;Self-Supervised Pretraining:&lt;/strong&gt; Enables training on vast unlabeled datasets before fine-tuning.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;BPE (Byte Pair Encoding)&lt;/strong&gt; 
    &lt;div id=&#34;bpe-byte-pair-encoding&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#bpe-byte-pair-encoding&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;BPE is a widely used tokenization method in natural language processing (NLP) and machine learning. It is designed to balance between breaking text into characters and full words by representing text as a sequence of subword units. This approach helps models handle rare and unseen words effectively while keeping the vocabulary size manageable.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Models and Creators</title>
      <link>/dsblog/AI-Models-and-Creators/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/AI-Models-and-Creators/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6187-ai-models-and-creators.jpg&#34; alt=&#34;AI Models and Creators&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Models and Creators 
    &lt;div id=&#34;ai-models-and-creators&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-models-and-creators&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Popular Models and Their Creator 
    &lt;div id=&#34;popular-models-and-their-creator&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#popular-models-and-their-creator&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Nova - Amazon&lt;/li&gt;
&lt;li&gt;Gemini, Gemma - Google&lt;/li&gt;
&lt;li&gt;Granite - Oracle&lt;/li&gt;
&lt;li&gt;GPT - OpenAI&lt;/li&gt;
&lt;li&gt;Phi - Microsoft Azure&lt;/li&gt;
&lt;li&gt;Einstein - Salesforce&lt;/li&gt;
&lt;li&gt;Joule - SAP&lt;/li&gt;
&lt;li&gt;Grok - X (formerly Twitter)&lt;/li&gt;
&lt;li&gt;Llama - Meta&lt;/li&gt;
&lt;li&gt;Qwen - Alibaba&lt;/li&gt;
&lt;li&gt;Claude - Anthropic&lt;/li&gt;
&lt;li&gt;Bard - Google&lt;/li&gt;
&lt;li&gt;PaLM - Google&lt;/li&gt;
&lt;li&gt;Mistral - Mistral AI&lt;/li&gt;
&lt;li&gt;Falcon - Technology Innovation Institute (TII), UAE&lt;/li&gt;
&lt;li&gt;Gato - DeepMind&lt;/li&gt;
&lt;li&gt;Jasper - Jasper AI&lt;/li&gt;
&lt;li&gt;Bloom - BigScience (collaborative project)&lt;/li&gt;
&lt;li&gt;Ernie - Baidu&lt;/li&gt;
&lt;li&gt;Alpaca - Stanford University (fine-tuned LLaMA model)&lt;/li&gt;
&lt;li&gt;Stable Diffusion - Stability AI&lt;/li&gt;
&lt;li&gt;HuggingChat - Hugging Face&lt;/li&gt;
&lt;li&gt;Cohere of Command&lt;/li&gt;
&lt;li&gt;Alpha fold of deepmind&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Models Developed by Microsoft 
    &lt;div id=&#34;models-developed-by-microsoft&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#models-developed-by-microsoft&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Microsoft has developed or collaborated on several AI models and frameworks, especially as part of its Azure AI ecosystem and its partnership with OpenAI. Below is a list of models and AI systems associated with Microsoft:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring GGUF and Other Model Formats</title>
      <link>/dsblog/exploring-gguf-and-other-model-formats/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-gguf-and-other-model-formats/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6180-exploring-gguf.jpg&#34; alt=&#34;Understanding GGUF and Other Model Formats in Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;&lt;strong&gt;Understanding GGUF and Other Model Formats in Machine Learning&lt;/strong&gt; 
    &lt;div id=&#34;understanding-gguf-and-other-model-formats-in-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-gguf-and-other-model-formats-in-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;As machine learning models continue to grow in complexity, the need for efficient, flexible, and versatile model formats becomes more pronounced. While formats like ONNX, TensorFlow’s SavedModel, and PyTorch’s native format have been around for some time, newer formats like GGUF are gaining attention for their unique benefits. This article explores these formats, their use cases, and how they support various aspects of machine learning, including deployment, compatibility, and optimization.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring AnythingLLM</title>
      <link>/dsblog/exploring-anythingllm/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/exploring-anythingllm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6179-exploring-anythingllm.jpg&#34; alt=&#34;Exploring AnythingLLM &#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring AnythingLLM 
    &lt;div id=&#34;exploring-anythingllm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-anythingllm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is AnythingLLM? 
    &lt;div id=&#34;what-is-anythingllm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-anythingllm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;AnythingLLM is an open-source project developed by Mintplex Labs that offers a highly flexible platform for creating personalized language models and knowledge databases. It operates using Retrieval-Augmented Generation (RAG), which combines language models with data from custom document collections. AnythingLLM supports embedding models (e.g., BERT), language models, and vector databases to index and query data, allowing users to fine-tune or deploy various models tailored to their needs, from local deployments to cloud integrations with OpenAI or Azure OpenAI.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Benchmarks Explained</title>
      <link>/dsblog/AI-Benchmarks-Explained/</link>
      <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/AI-Benchmarks-Explained/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6173-AI-Benchmarks-Explained.jpg&#34; alt=&#34;AI-Benchmarks-Explained&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques 
    &lt;div id=&#34;ai-benchmarks-explained-essential-components-and-leading-llm-evaluation-techniques&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-benchmarks-explained-essential-components-and-leading-llm-evaluation-techniques&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is a Benchmark in AI? 
    &lt;div id=&#34;what-is-a-benchmark-in-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-a-benchmark-in-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;benchmark&lt;/strong&gt; in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Transfer Learning Key AI Techniques Explained</title>
      <link>/dsblog/Transfer-Learning-Key-AI-Techniques-Explained/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Transfer-Learning-Key-AI-Techniques-Explained/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6172-Transfer-Learning-Key-AI-Techniques-Explained.jpg&#34; alt=&#34;Transfer Learning Key AI Techniques Explained&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Transfer Learning Key AI Techniques Explained 
    &lt;div id=&#34;transfer-learning-key-ai-techniques-explained&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transfer-learning-key-ai-techniques-explained&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In this article we will understand some important concepts used within machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is in-context Learning?&lt;/li&gt;
&lt;li&gt;What is Prompt-Engineering?&lt;/li&gt;
&lt;li&gt;What is the relationship between Prompt Engineering and In-Context Learning?&lt;/li&gt;
&lt;li&gt;What is Zero-shot learning?&lt;/li&gt;
&lt;li&gt;How Zero-shot learning is different from In-context Learning?&lt;/li&gt;
&lt;li&gt;What is Meta-Learning?&lt;/li&gt;
&lt;li&gt;What is Few-shot learning?&lt;/li&gt;
&lt;li&gt;Do we need foundational models for Meta-learning and Few-shot learning?&lt;/li&gt;
&lt;li&gt;What is transfer learning?&lt;/li&gt;
&lt;li&gt;How do we do transfer learning from existing model?&lt;/li&gt;
&lt;li&gt;What is finetuning?&lt;/li&gt;
&lt;li&gt;Which layers to update, what weight to update during finetuning?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Prompt Engineering, In Context Learning and Zero-shot Learning 
    &lt;div id=&#34;prompt-engineering-in-context-learning-and-zero-shot-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#prompt-engineering-in-context-learning-and-zero-shot-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;What is in-context Learning? 
    &lt;div id=&#34;what-is-in-context-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-in-context-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;p&gt;In-Context Learning refers to a model&amp;rsquo;s ability to adapt its responses based on the context provided in the input prompt without updating its parameters or undergoing explicit training. The model uses the examples, instructions, or context given in the input to influence its behavior during inference.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Types of Large Language Models (LLM)</title>
      <link>/dsblog/Types-of-LLM/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Types-of-LLM/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6171-Types-of-LLM.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Generative AI (GenAI) is expanding at an astonishing rate, with new models emerging almost daily, each sporting unique names, capabilities, versions, and sizes. For AI professionals, keeping track of these models can feel like a full-time job. But for business users, IT professionals, and software developers trying to make the right choice, understanding the model’s name and what it represents can seem overwhelming. Wouldn’t it be helpful if we could decode the meaning behind these names to know if a model fits our needs and is worth the investment? In this article, we’ll break down how the names of GenAI models can reveal clues about their functionality and suitability for specific tasks, helping you make informed decisions with confidence.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI/ML with Oracle Cloud</title>
      <link>/dsblog/AI-ML-With-Oracle-Cloud/</link>
      <pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/AI-ML-With-Oracle-Cloud/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6166-AI-ML-With-Oracle-Cloud.jpg&#34; alt=&#34;AI/ML with Oracle Cloud&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI/ML with Oracle Cloud 
    &lt;div id=&#34;aiml-with-oracle-cloud&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#aiml-with-oracle-cloud&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/services.htm&#34; target=&#34;_blank&#34;&gt;Oracle Infrastructure Services&lt;/a&gt; 
    &lt;div id=&#34;oracle-infrastructure-services&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#oracle-infrastructure-services&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Register for &lt;a href=&#34;https://www.oracle.com/cloud/free/?source=:ow:o:h:po:OHPPanel1nav0625&amp;amp;intcmp=:ow:o:h:po:OHPPanel1nav0625&#34; target=&#34;_blank&#34;&gt;Oracle Cloud Free Tier&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Oracle AI Main services 
    &lt;div id=&#34;oracle-ai-main-services&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#oracle-ai-main-services&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.oracle.com/digital-assistant/oda-instances?region=ap-mumbai-1&#34; target=&#34;_blank&#34;&gt;Digital Assistant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/document-understanding/using/home.htm&#34; target=&#34;_blank&#34;&gt;Document Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/language/using/pretrain-models.htm#lang-detect&#34; target=&#34;_blank&#34;&gt;Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/vision/using/pretrained-model-using-image.htm&#34; target=&#34;_blank&#34;&gt;Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/speech/home.htm&#34; target=&#34;_blank&#34;&gt;Speech&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/Streaming/home.htm&#34; target=&#34;_blank&#34;&gt;Stream&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/process-automation/oci-process-automation/overview-oci-process-automation.html&#34; target=&#34;_blank&#34;&gt;Cloud Infra Automation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm&#34; target=&#34;_blank&#34;&gt;Generative AI&lt;/a&gt; 
    &lt;div id=&#34;generative-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#generative-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Generative AI is a fully managed Oracle Cloud Infrastructure service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, including chat, text generation, summarization, and creating text embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Synthetic Data Generation Capabilities</title>
      <link>/dsblog/Exploring-Synthetic-Data-Generation-Capabilities/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Exploring-Synthetic-Data-Generation-Capabilities/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6163-Exploring-Synthetic-Data-Generation-Capabilities.jpg&#34; alt=&#34;Exploring Synthetic Data Generation Capabilities&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Synthetic Data Generation Capabilities 
    &lt;div id=&#34;exploring-synthetic-data-generation-capabilities&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-synthetic-data-generation-capabilities&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Are you looking answers to these questions?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Synthetic Data?&lt;/li&gt;
&lt;li&gt;What is data anonymization?&lt;/li&gt;
&lt;li&gt;What are different techniques for generating Anonymized Data?&lt;/li&gt;
&lt;li&gt;Why is anonymizing not sufficient to address PII data issues?&lt;/li&gt;
&lt;li&gt;When there is lots of data available around then why do we need synthetic data?&lt;/li&gt;
&lt;li&gt;What are the challenges in synthetic data generation?&lt;/li&gt;
&lt;li&gt;What are the tools for Synthetic Data Generator for Marketing&lt;/li&gt;
&lt;li&gt;What are the tools for Graph Synthetic Data generation?&lt;/li&gt;
&lt;li&gt;List of popular synthetic data generation tools for different industries&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Synthetic Data? 
    &lt;div id=&#34;what-is-synthetic-data&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-synthetic-data&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Synthetic data&lt;/strong&gt; is artificially generated data that mimics the characteristics and statistical properties of real-world data but is not derived directly from real-world events or observations. It is created using algorithms, simulations, or models to represent patterns, structures, and relationships found in actual datasets. Synthetic data can take various forms, such as text, images, audio, or structured tabular data, depending on the context. It is used across various industries to train AI models, simulate environments, and conduct research.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Dense Embedding Models in AI</title>
      <link>/dsblog/Exploring-Dense-Embedding-Models-in-AI/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Exploring-Dense-Embedding-Models-in-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6157-Exploring-Dense-Embedding-Models-in-AI.jpg&#34; alt=&#34;Exploring Dense Embedding Models in AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is dense embedding in AI? 
    &lt;div id=&#34;what-is-dense-embedding-in-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-dense-embedding-in-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Dense embeddings are critical in many AI applications, particularly in deep learning, where they help reduce data complexity and enhance the model’s ability to generalize from patterns in data.&lt;/p&gt;
&lt;p&gt;In artificial intelligence (AI), &lt;strong&gt;dense embedding&lt;/strong&gt; refers to a method of representing data (like words, sentences, images, or other inputs) as dense vectors in a continuous, lower-dimensional (lessor number of dimensions) space. These vectors, known as &lt;strong&gt;embeddings&lt;/strong&gt;, encode semantic information, enabling AI models to work with data in a more meaningful way.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introduction to Perplexity AI</title>
      <link>/dsblog/Introduction-to-Perplexity-AI/</link>
      <pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Introduction-to-Perplexity-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6156-Introduction-to-Perplexity-AI.jpg&#34; alt=&#34;Introduction to Perplexity AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Introduction to Perplexity AI 
    &lt;div id=&#34;introduction-to-perplexity-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-perplexity-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Perplexity AI? 
    &lt;div id=&#34;what-is-perplexity-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-perplexity-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Perplexity AI Founded in 2022 is based in San Francisco, California. Perplexity AI is an AI-powered search engine that uses a large language model to answer questions and provide information. It is a free, open-source search engine that is built on top of the latest advancements in AI and natural language processing. Perplexity AI distinguishes itself as a unique blend of a search engine and an AI chatbot, offering several features that set it apart from traditional search engines like Google and other AI models such as ChatGPT.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Ollama &amp; LM Studio</title>
      <link>/dsblog/Exploring-Ollama/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Exploring-Ollama/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6143-Exploring-Ollama.jpg&#34; alt=&#34;Exploring Ollama &amp;amp; LM Studio&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Ollama &amp;amp; LM Studio 
    &lt;div id=&#34;exploring-ollama--lm-studio&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-ollama--lm-studio&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;If you are looking answers to the following questions, then this article is for you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Question: What is Ollama? Is it like Docker?&lt;/li&gt;
&lt;li&gt;Question: How is Ollama different from Docker?&lt;/li&gt;
&lt;li&gt;Question: How to install ollama on my machine?&lt;/li&gt;
&lt;li&gt;Question: How to create customized LLM Model (docker like image)?&lt;/li&gt;
&lt;li&gt;Question: What are the LLM available on ollama?&lt;/li&gt;
&lt;li&gt;Question: Can we integrate these hundreds with different UI like ChatGPT?&lt;/li&gt;
&lt;li&gt;Question: If I want to use all these Ollama models via Jupyter Notebook then what to do?&lt;/li&gt;
&lt;li&gt;Question: Does Ollama have plugins like github copilot? Can I use those from my visual code?&lt;/li&gt;
&lt;li&gt;Question: What kind of software are LM Studio or Ollama?&lt;/li&gt;
&lt;li&gt;Question: What is LM Studio and how different it is from Ollama?&lt;/li&gt;
&lt;li&gt;Question: What are different formats to save model, specifically LLMs?&lt;/li&gt;
&lt;li&gt;Question: What is gguf model extention?&lt;/li&gt;
&lt;li&gt;Question: If I have finetuned my models using clouds like aws sagemaker, vertexai, azure and kept there then can I use them inside my ollama and LM Studio?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Question: What is Ollama? Is it like Docker? 
    &lt;div id=&#34;question-what-is-ollama-is-it-like-docker&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#question-what-is-ollama-is-it-like-docker&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Ollama is a platform designed to make running and interacting with large language models (LLMs) easier. It abstracts away the complexities of managing LLM models, GPU resources, and related configurations by offering a simple CLI interface. With Ollama, you can run, manage, and deploy LLMs locally or in various cloud environments without having to worry about the intricate details of setting up environments, downloading models, or configuring them.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Variations of Language Model in Huggingface</title>
      <link>/dsblog/Variations-of-Language-Model-in-Huggingface/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Variations-of-Language-Model-in-Huggingface/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6138-Variations-of-Language-Model-in-Huggingface.jpg&#34; alt=&#34;Variations-of-LanguageModel&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Variations of Language Model in Huggingface 
    &lt;div id=&#34;variations-of-language-model-in-huggingface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#variations-of-language-model-in-huggingface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What the Model variable in Huggingface? 
    &lt;div id=&#34;what-the-model-variable-in-huggingface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-the-model-variable-in-huggingface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;We know base moels like BERT, T5, GPT2, GPT3 etc are developed by researchers working with different companies. But when we look into huggingface model repository we see other models like GPT2LMHeadModel, GPT2ForSequenceClassification, etc what are these?&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>LLM Architecture and Training</title>
      <link>/dsblog/LLM-Architecture-and-Training/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/LLM-Architecture-and-Training/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg&#34; alt=&#34;LLM-Architecture-and-Training&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;&lt;strong&gt;Understanding LLM Architectures and Model Training&lt;/strong&gt; 
    &lt;div id=&#34;understanding-llm-architectures-and-model-training&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-llm-architectures-and-model-training&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. We’ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Why to Finetune LLM?</title>
      <link>/dsblog/why-to-finetune-llm/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/why-to-finetune-llm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6115-why-to-finetune-llm.jpg&#34; alt=&#34;Why to Finetune LLM?&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Finetuning, Fewshot Learning, Why and How? 
    &lt;div id=&#34;finetuning-fewshot-learning-why-and-how&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#finetuning-fewshot-learning-why-and-how&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Why to finetune a LLM? 
    &lt;div id=&#34;why-to-finetune-a-llm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#why-to-finetune-a-llm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Fine-tuning a large language model (LLM) can provide several benefits, depending on your specific needs and objectives. Here are some key reasons to consider fine-tuning an LLM:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Stanford Alpaca</title>
      <link>/dsblog/Stanford-Alpaca/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Stanford-Alpaca/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6116-Stanford-Alpaca.jpg&#34; alt=&#34;Stanford-Alpaca&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Stanford Alpaca 
    &lt;div id=&#34;stanford-alpaca&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#stanford-alpaca&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34; target=&#34;_blank&#34;&gt;Stanford Alpaca Github Report&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li&gt;
&lt;li&gt;This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:
&lt;ul&gt;
&lt;li&gt;The 52K &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34; target=&#34;_blank&#34;&gt;instruction-following data&lt;/a&gt; used for fine-tuning the model.&lt;/li&gt;
&lt;li&gt;The code for generating the data.&lt;/li&gt;
&lt;li&gt;The code for fine-tuning the model.&lt;/li&gt;
&lt;li&gt;The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Overview 
    &lt;div id=&#34;overview&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#overview&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li&gt;
&lt;li&gt;Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li&gt;
&lt;li&gt;Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li&gt;
&lt;li&gt;Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li&gt;
&lt;li&gt;Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li&gt;
&lt;li&gt;Model weights can be released if the creators of LLaMA gives permission.&lt;/li&gt;
&lt;li&gt;Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li&gt;
&lt;li&gt;Based on followin papers:
&lt;ul&gt;
&lt;li&gt;LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34; target=&#34;_blank&#34;&gt;Hugo2023&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34; target=&#34;_blank&#34;&gt;Yizhong2022&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Release
&lt;ul&gt;
&lt;li&gt;alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Highlevel Activities of the Alpaca Project 
    &lt;div id=&#34;highlevel-activities-of-the-alpaca-project&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#highlevel-activities-of-the-alpaca-project&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Understanding LLM GAN and Transformers</title>
      <link>/dsblog/Understanding-LLM-GAN-and-Transformers/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Understanding-LLM-GAN-and-Transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg&#34; alt=&#34;Understanding-LLM-GAN-Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Understanding LLM, GAN and Transformers 
    &lt;div id=&#34;understanding-llm-gan-and-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-llm-gan-and-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;LLM Layers 
    &lt;div id=&#34;llm-layers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#llm-layers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Transformers Demystified A Step-by-Step Guide</title>
      <link>/dsblog/transformers-demystified-a-step-by-step-guide/</link>
      <pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/transformers-demystified-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg&#34; alt=&#34;Transformers Demystified A Step-by-Step Guide&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Transformers Demystified A Step-by-Step Guide 
    &lt;div id=&#34;transformers-demystified-a-step-by-step-guide&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformers-demystified-a-step-by-step-guide&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;All modern Transformers are based on a paper &amp;ldquo;Attention is all you need&amp;rdquo;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>NLP BenchMarks</title>
      <link>/dsblog/NLP-BenchMarks1/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/NLP-BenchMarks1/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6120-NLP-BenchMarks.jpg&#34; alt=&#34;NLP-BenchMarks&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;NLP BenchMarks 
    &lt;div id=&#34;nlp-benchmarks&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#nlp-benchmarks&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Language Model? 
    &lt;div id=&#34;what-is-language-model&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-language-model&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;language model&lt;/strong&gt; is a computational model that understands and generates human language. It learns the patterns and structure of a language by analyzing large amounts of text data, allowing it to predict the next word in a sequence or generate coherent text. Language models are used in applications like text generation, translation, speech recognition, chatbots, and sentiment analysis.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Topic Modeling with BERT</title>
      <link>/dsblog/topic-modeling-with-bert/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/topic-modeling-with-bert/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6105-Topic-Modeling-with-BERT.jpg&#34; alt=&#34;Topic Modeling with BERT&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Topic Modeling with BERT 
    &lt;div id=&#34;topic-modeling-with-bert&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#topic-modeling-with-bert&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Key steps in BERTopic modelling are as following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &amp;ldquo;Sentence Embedding&amp;rdquo; models to embed the sentences of the article&lt;/li&gt;
&lt;li&gt;Reduce the dimensionality of embedding using UMAP&lt;/li&gt;
&lt;li&gt;Cluster these documents (reduced dimensions) using HDBSAN&lt;/li&gt;
&lt;li&gt;Use c-TF-IDF extract keywords, their frequency and IDF for each cluster.&lt;/li&gt;
&lt;li&gt;MMR: Maximize Candidate Relevance. How many words in a topic can represent the topic?&lt;/li&gt;
&lt;li&gt;Intertopic Distance Map&lt;/li&gt;
&lt;li&gt;Use similarity matrix (heatmap), dandogram (hierarchical map), to visualize the topics and key_words.&lt;/li&gt;
&lt;li&gt;Traction of topic over time period. Some may be irrelevant and for other traction may be increasing or decreasing.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Installation 
    &lt;div id=&#34;installation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#installation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Installation, with sentence-transformers, can be done using pypi:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# If you want to install BERTopic with other embedding models, you can choose one of the following:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Choose an embedding backend&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gensim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spacy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Topic modeling with images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vision&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 class=&#34;relative group&#34;&gt;Supported Topic Modelling Techniques 
    &lt;div id=&#34;supported-topic-modelling-techniques&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#supported-topic-modelling-techniques&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;BERTopic supports all kinds of topic modeling techniques as below.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Basics of Word Embedding</title>
      <link>/dsblog/basics-of-word-embedding/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/basics-of-word-embedding/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg&#34; alt=&#34;Basics of Word Embedding&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Basics of Word Embedding 
    &lt;div id=&#34;basics-of-word-embedding&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#basics-of-word-embedding&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Context, target and window? 
    &lt;div id=&#34;what-is-context-target-and-window&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-context-target-and-window&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &amp;ldquo;context&amp;rdquo; word is the surrounding word.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;target&amp;rdquo; word is the middle word.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;window distance&amp;rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s take a sentence&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Compressing Large Language Model</title>
      <link>/dsblog/compressing-llm/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/compressing-llm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6099-Compressing-LLM.jpg&#34; alt=&#34;Compressing Large Language Model&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Compressing Large Language Model 
    &lt;div id=&#34;compressing-large-language-model&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#compressing-large-language-model&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;If you are looking answers to following question then &amp;ldquo;Yes&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is LLM compression?&lt;/li&gt;
&lt;li&gt;Why is LLM compression necessary?&lt;/li&gt;
&lt;li&gt;What are the different techniques for LLM compression?&lt;/li&gt;
&lt;li&gt;How does quantization work in LLM compression?&lt;/li&gt;
&lt;li&gt;What is pruning, and how does it help in compressing LLMs?&lt;/li&gt;
&lt;li&gt;Can you explain knowledge distillation in the context of LLMs?&lt;/li&gt;
&lt;li&gt;What is low-rank factorization and its role in LLM compression?&lt;/li&gt;
&lt;li&gt;How effective are weight sharing techniques in compressing LLMs?&lt;/li&gt;
&lt;li&gt;What are the trade-offs involved in LLM compression?&lt;/li&gt;
&lt;li&gt;How does fine-tuning work in the context of compressed LLMs?&lt;/li&gt;
&lt;li&gt;What are the benefits of fine-tuning in compressed LLMs?&lt;/li&gt;
&lt;li&gt;What role does hardware play in LLM compression?&lt;/li&gt;
&lt;li&gt;What are the ethical considerations in LLM compression?&lt;/li&gt;
&lt;li&gt;What are the future directions in LLM compression?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;1. &lt;strong&gt;What is LLM Compression?&lt;/strong&gt; 
    &lt;div id=&#34;1-what-is-llm-compression&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#1-what-is-llm-compression&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;LLM (Large Language Model) compression refers to a set of techniques and methodologies aimed at reducing the size of large language models while maintaining their performance as much as possible. Large language models, such as GPT, BERT, and their variants, often contain hundreds of millions to billions of parameters, making them resource-intensive to deploy and run. The sheer size of these models poses challenges in terms of storage, computation, and real-time inference, especially when deploying on devices with limited hardware resources like mobile phones or edge devices.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is Pinecone</title>
      <link>/dsblog/What-is-Pinecone/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/What-is-Pinecone/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6097-What-is-Pinecone.jpg&#34; alt=&#34;What is Pinecone&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is pinecone? 
    &lt;div id=&#34;what-is-pinecone&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-pinecone&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Pinecone is a managed vector database that provides vector search (or “similarity search”) for developers with a straightforward API and usage-based pricing. It’s free to try. &lt;a href=&#34;https://www.pinecone.io/learn/vector-search-basics/&#34; target=&#34;_blank&#34;&gt;Introduction to Vector Search for Developers&lt;/a&gt;.&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is a &lt;a href=&#34;https://www.pinecone.io/learn/vector-database/&#34; target=&#34;_blank&#34;&gt;Vector Database&lt;/a&gt;? 
    &lt;div id=&#34;what-is-a-vector-database&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-a-vector-database&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Your must have heard about relational database, graph database, object datbase. But this article is about Vector Database.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>ML Model Respository from Pinto0309</title>
      <link>/dsblog/ML-Model-Repository-from-Pinto0309/</link>
      <pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ML-Model-Repository-from-Pinto0309/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6095-ML-Model-Repository-from-Pinto0309.jpg&#34; alt=&#34;ML Model Respository from Pinto0309&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;ML Model Repository from Pinto0309 
    &lt;div id=&#34;ml-model-repository-from-pinto0309&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ml-model-repository-from-pinto0309&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Using AI we can solve many kinds of tasks for this input can be text, structured data, image, video, audio, time-series, etc. To solve these problems we need to train model. These models may be computer vision, NLP, or traditional machine learning kind. There are hundreds of architectures and algorithms to solve business problems and create models. There a hundreds of different datasets that can be along with a particular architecture or algorithm to solve the problem. If you have any of these tasks then you can explore using these pre-trained models to solve your problem. There is a GitHub user &amp;ldquo;Katsuya Hyodo&amp;rdquo; with GitHub account &amp;ldquo;PINTO0309&amp;rdquo;. He has trained hundreds of models and created these pre-trained models for the community. You can scan and explore them from there. From there you can download the pre-trained models.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Important AI Paper List</title>
      <link>/dsblog/select-ai-papers/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/select-ai-papers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg&#34; alt=&#34;Important AI Paper List&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Important AI Paper List 
    &lt;div id=&#34;important-ai-paper-list&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#important-ai-paper-list&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduciton 
    &lt;div id=&#34;introduciton&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduciton&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors&amp;rsquo; information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like &amp;ldquo;Vivek Ramaswami, Kartikeyan Karunanidhi&amp;rdquo; it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on &amp;ldquo;google scholar&amp;rdquo;, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper with Code Resources</title>
      <link>/dsblog/paperwithcode-resources/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/paperwithcode-resources/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6091-rps-Paperwithcode-Resources.jpg&#34; alt=&#34;Paper with Code Resources&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper with Code Resources 
    &lt;div id=&#34;paper-with-code-resources&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-with-code-resources&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Papers of 2021 
    &lt;div id=&#34;trending-papers-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-papers-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;ADOP: Approximate Differentiable One-Pixel Point Rendering — Rückert et al — &lt;a href=&#34;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Bayesian Learning Rule —Khan et al &lt;a href=&#34;https://paperswithcode.com/paper/the-bayesian-learning-rule&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/the-bayesian-learning-rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Program Synthesis with Large Language Models — Austin et al &lt;a href=&#34;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Masked Autoencoders Are Scalable Vision Learners — He et al &lt;a href=&#34;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8-bit Optimizers via Block-wise Quantization — Dettmers et al &lt;a href=&#34;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Revisiting ResNets: Improved Training and Scaling Strategies — Bello et al &lt;a href=&#34;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Image Super-Resolution via Iterative Refinement — Saharia et al &lt;a href=&#34;https://paperswithcode.com/paper/image-super-resolution-via-iterative&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/image-super-resolution-via-iterative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs — Jaegle et al &lt;a href=&#34;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do Vision Transformers See Like Convolutional Neural Networks? — Raghu et al &lt;a href=&#34;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions — Niepert et al &lt;a href=&#34;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Libaries of 2021 
    &lt;div id=&#34;trending-libaries-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-libaries-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PyTorch Image Models — Ross Wightman — &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34; target=&#34;_blank&#34;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Transformers — Hugging Face — &lt;a href=&#34;https://github.com/huggingface/transformers&#34; target=&#34;_blank&#34;&gt;https://github.com/huggingface/transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyTorch-GAN — Erik Linder-Norén — &lt;a href=&#34;https://github.com/eriklindernoren/PyTorch-GAN&#34; target=&#34;_blank&#34;&gt;https://github.com/eriklindernoren/PyTorch-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMDetection — OpenMMLab — &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34; target=&#34;_blank&#34;&gt;https://github.com/open-mmlab/mmdetection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Darknet — AlexeyAB — &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vision Transformer PyTorch — lucidrains — &lt;a href=&#34;https://github.com/lucidrains/vit-pytorch&#34; target=&#34;_blank&#34;&gt;https://github.com/lucidrains/vit-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;InsightFace — DeepInsight — &lt;a href=&#34;https://github.com/deepinsight/insightface&#34; target=&#34;_blank&#34;&gt;https://github.com/deepinsight/insightface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Detectron2 — Meta AI — &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/detectron2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PaddleOCR — PaddlePaddle — &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34;&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FairSeq — Meta AI — &lt;a href=&#34;https://github.com/pytorch/fairseq&#34; target=&#34;_blank&#34;&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Top Dataset - 2021 
    &lt;div id=&#34;top-dataset---2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#top-dataset---2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MATH — Hendrycks et al &lt;a href=&#34;https://paperswithcode.com/dataset/math&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UAV-Human — Li et al &lt;a href=&#34;https://paperswithcode.com/dataset/uav-human&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/uav-human&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UPFD (User Preference-aware Fake News Detection) — Dou et al &lt;a href=&#34;https://paperswithcode.com/dataset/upfd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/upfd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OGB-LSC (OGB Large-Scale Challenge) — Hu et al &lt;a href=&#34;https://paperswithcode.com/dataset/ogb-lsc&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/ogb-lsc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CodeXGLUE —Lu et al &lt;a href=&#34;https://paperswithcode.com/dataset/codexglue&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/codexglue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AGORA — Patel et al &lt;a href=&#34;https://paperswithcode.com/dataset/agora&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/agora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEIR (Benchmarking IR) — Thakur et al &lt;a href=&#34;https://paperswithcode.com/dataset/beir&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/beir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WikiGraphs — Wang et al &lt;a href=&#34;https://paperswithcode.com/dataset/wikigraphs&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/wikigraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Few-NERD — Ding et al &lt;a href=&#34;https://paperswithcode.com/dataset/few-nerd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/few-nerd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PASS (Pictures without humAns for Self-Supervision) —Asano et al &lt;a href=&#34;https://paperswithcode.com/dataset/pass&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/pass&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Papers of 2022 
    &lt;div id=&#34;papers-of-2022&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#papers-of-2022&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Controllable Animation of Fluid Elements in Still Images&lt;/li&gt;
&lt;li&gt;F-SfT: Shape-From-Template With A Physics-Based Deformation Model&lt;/li&gt;
&lt;li&gt;TWIST: Two-Way Inter-Label Self-Training for Semi-Supervised 3D Instance Segmentation&lt;/li&gt;
&lt;li&gt;Do Learned Representations Respect Causal Relationships?&lt;/li&gt;
&lt;li&gt;ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic&lt;/li&gt;
&lt;li&gt;3D Moments From Near-Duplicate Photos&lt;/li&gt;
&lt;li&gt;Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization&lt;/li&gt;
&lt;li&gt;Blind2Unblind: Self-Supervised Image Denoising With Visible Blind Spots&lt;/li&gt;
&lt;li&gt;Balanced and Hierarchical Relation Learning for One-Shot Object Detection&lt;/li&gt;
&lt;li&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/li&gt;
&lt;li&gt;Stochastic Trajectory Prediction Via Motion Indeterminacy Diffusion&lt;/li&gt;
&lt;li&gt;CLRNet: Cross Layer Refinement Network for Lane Detection&lt;/li&gt;
&lt;li&gt;Motion-Aware Contrastive Video Representation Learning Via Foreground-Background Merging&lt;/li&gt;
&lt;li&gt;DINE: Domain Adaptation From Single and Multiple Black-Box Predictors&lt;/li&gt;
&lt;li&gt;FaceFormer: Speech-Driven 3D Facial Animation With Transformers&lt;/li&gt;
&lt;li&gt;Rotationally Equivariant 3D Object Detection&lt;/li&gt;
&lt;li&gt;Accelerating DETR Convergence Via Semantic-Aligned Matching&lt;/li&gt;
&lt;li&gt;Cloning Outfits From Real-World Images to 3D Characters for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;GeoNeRF: Generalizing NeRF With Geometry Priors&lt;/li&gt;
&lt;li&gt;ABPN: Adaptive Blend Pyramid Network for Real-Time Local Retouching of Ultra High-Resolution Photo&lt;/li&gt;
&lt;li&gt;Expanding Low-Density Latent Regions for Open-Set Object Detection&lt;/li&gt;
&lt;li&gt;Uformer: A General U-Shaped Transformer for Image Restoration&lt;/li&gt;
&lt;li&gt;Exploring Dual-Task Correlation for Pose Guided Person Image Generation&lt;/li&gt;
&lt;li&gt;Portrait Eyeglasses and Shadow Removal By Leveraging 3D Synthetic Data&lt;/li&gt;
&lt;li&gt;Modeling 3D Layout for Group Re-Identification&lt;/li&gt;
&lt;li&gt;Toward Fast, Flexible, and Robust Low-Light Image Enhancement&lt;/li&gt;
&lt;li&gt;Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos&lt;/li&gt;
&lt;li&gt;HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network&lt;/li&gt;
&lt;li&gt;Modular Action Concept Grounding in Semantic Video Prediction&lt;/li&gt;
&lt;li&gt;StyleSwin: Transformer-Based GAN for High-Resolution Image Generation&lt;/li&gt;
&lt;li&gt;Discrete Cosine Transform Network for Guided Depth Map Super-Resolution&lt;/li&gt;
&lt;li&gt;Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing&lt;/li&gt;
&lt;li&gt;TransGeo: Transformer Is All You Need for Cross-View Image Geo-Localization&lt;/li&gt;
&lt;li&gt;Contrastive Boundary Learning for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution&lt;/li&gt;
&lt;li&gt;CVNet: Contour Vibration Network for Building Extraction&lt;/li&gt;
&lt;li&gt;Swin Transformer V2: Scaling Up Capacity and Resolution&lt;/li&gt;
&lt;li&gt;Projective Manifold Gradient Layer for Deep Rotation Regression&lt;/li&gt;
&lt;li&gt;HCSC: Hierarchical Contrastive Selective Coding&lt;/li&gt;
&lt;li&gt;TransRank: Self-Supervised Video Representation Learning Via Ranking-Based Transformation Recognition&lt;/li&gt;
&lt;li&gt;DiSparse: Disentangled Sparsification for Multitask Model Compression&lt;/li&gt;
&lt;li&gt;Pushing The Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make A Difference&lt;/li&gt;
&lt;li&gt;Towards Efficient and Scalable Sharpness-Aware Minimization&lt;/li&gt;
&lt;li&gt;OSSO: Obtaining Skeletal Shape From Outside&lt;/li&gt;
&lt;li&gt;A Study on The Distribution of Social Biases in Self-Supervised Learning Visual Models&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes&lt;/li&gt;
&lt;li&gt;Comparing Correspondences: Video Prediction With Correspondence-Wise Losses&lt;/li&gt;
&lt;li&gt;Towards Fewer Annotations: Active Learning Via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding&lt;/li&gt;
&lt;li&gt;Few Shot Generative Model Adaption Via Relaxed Spatial Structural Alignment&lt;/li&gt;
&lt;li&gt;Enhancing Adversarial Training With Second-Order Statistics of Weights&lt;/li&gt;
&lt;li&gt;Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo&lt;/li&gt;
&lt;li&gt;Moving Window Regression: A Novel Approach to Ordinal Regression&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection&lt;/li&gt;
&lt;li&gt;Robust Optimization As Data Augmentation for Large-Scale Graphs&lt;/li&gt;
&lt;li&gt;Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients&lt;/li&gt;
&lt;li&gt;Improving The Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input&lt;/li&gt;
&lt;li&gt;ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer&lt;/li&gt;
&lt;li&gt;360MonoDepth: High-Resolution 360deg Monocular Depth Estimation&lt;/li&gt;
&lt;li&gt;POCO: Point Convolution for Surface Reconstruction&lt;/li&gt;
&lt;li&gt;Neural Texture Extraction and Distribution for Controllable Person Image Synthesis&lt;/li&gt;
&lt;li&gt;Classification-Then-Grounding: Reformulating Video Scene Graphs As Temporal Bipartite Graphs&lt;/li&gt;
&lt;li&gt;DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis&lt;/li&gt;
&lt;li&gt;ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes&lt;/li&gt;
&lt;li&gt;UNIST: Unpaired Neural Implicit Shape Translation Network&lt;/li&gt;
&lt;li&gt;APES: Articulated Part Extraction From Sprite Sheets&lt;/li&gt;
&lt;li&gt;SPAct: Self-Supervised Privacy Preservation for Action Recognition&lt;/li&gt;
&lt;li&gt;De-Rendering 3D Objects in The Wild&lt;/li&gt;
&lt;li&gt;Global Sensing and Measurements Reuse for Image Compressed Sensing&lt;/li&gt;
&lt;li&gt;Practical Evaluation of Adversarial Robustness Via Adaptive Auto Attack&lt;/li&gt;
&lt;li&gt;Cross-View Transformers for Real-Time Map-View Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Controllable Dynamic Multi-Task Architectures&lt;/li&gt;
&lt;li&gt;FastDOG: Fast Discrete Optimization on GPU&lt;/li&gt;
&lt;li&gt;Focal and Global Knowledge Distillation for Detectors&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Continual Learning&lt;/li&gt;
&lt;li&gt;Human Mesh Recovery From Multiple Shots&lt;/li&gt;
&lt;li&gt;Convolution of Convolution: Let Kernels Spatially Collaborate&lt;/li&gt;
&lt;li&gt;Make It Move: Controllable Image-to-Video Generation With Text Descriptions&lt;/li&gt;
&lt;li&gt;Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling&lt;/li&gt;
&lt;li&gt;Video-Text Representation Learning Via Differentiable Weak Temporal Alignment&lt;/li&gt;
&lt;li&gt;Bi-Directional Object-Context Prioritization Learning for Saliency Ranking&lt;/li&gt;
&lt;li&gt;Vehicle Trajectory Prediction Works, But Not Everywhere&lt;/li&gt;
&lt;li&gt;MonoDTR: Monocular 3D Object Detection With Depth-Aware Transformer&lt;/li&gt;
&lt;li&gt;Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-Shot Learning&lt;/li&gt;
&lt;li&gt;Generalized Category Discovery&lt;/li&gt;
&lt;li&gt;Contour-Hugging Heatmaps for Landmark Detection&lt;/li&gt;
&lt;li&gt;Voxel Field Fusion for 3D Object Detection&lt;/li&gt;
&lt;li&gt;DisARM: Displacement Aware Relation Module for 3D Detection&lt;/li&gt;
&lt;li&gt;MixFormer: Mixing Features Across Windows and Dimensions&lt;/li&gt;
&lt;li&gt;FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment&lt;/li&gt;
&lt;li&gt;HEAT: Holistic Edge Attention Transformer for Structured Reconstruction&lt;/li&gt;
&lt;li&gt;Mobile-Former: Bridging MobileNet and Transformer&lt;/li&gt;
&lt;li&gt;CycleMix: A Holistic Strategy for Medical Image Segmentation From Scribble Supervision&lt;/li&gt;
&lt;li&gt;VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution&lt;/li&gt;
&lt;li&gt;Towards End-to-End Unified Scene Text Detection and Layout Analysis&lt;/li&gt;
&lt;li&gt;AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation&lt;/li&gt;
&lt;li&gt;ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior&lt;/li&gt;
&lt;li&gt;End-to-End Referring Video Object Segmentation With Multimodal Transformers&lt;/li&gt;
&lt;li&gt;IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds&lt;/li&gt;
&lt;li&gt;Detecting Camouflaged Object in Frequency Domain&lt;/li&gt;
&lt;li&gt;SelfRecon: Self Reconstruction Your Digital Avatar From Monocular Video&lt;/li&gt;
&lt;li&gt;Equivariant Point Cloud Analysis Via Learning Orientations for Message Passing&lt;/li&gt;
&lt;li&gt;Node Representation Learning in Graph Via Node-to-Neighbourhood Mutual Information Maximization&lt;/li&gt;
&lt;li&gt;Semi-Supervised Video Semantic Segmentation With Inter-Frame Feature Reconstruction&lt;/li&gt;
&lt;li&gt;Amodal Segmentation Through Out-of-Task and Out-of-Distribution Generalization With A Bayesian Model&lt;/li&gt;
&lt;li&gt;How Well Do Sparse ImageNet Models Transfer?&lt;/li&gt;
&lt;li&gt;REX: Reasoning-Aware and Grounded Explanation&lt;/li&gt;
&lt;li&gt;Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes&lt;/li&gt;
&lt;li&gt;Object-Aware Video-Language Pre-Training for Retrieval&lt;/li&gt;
&lt;li&gt;MAT: Mask-Aware Transformer for Large Hole Image Inpainting&lt;/li&gt;
&lt;li&gt;Align and Prompt: Video-and-Language Pre-Training With Entity Prompts&lt;/li&gt;
&lt;li&gt;MSG-Transformer: Exchanging Local Spatial Information By Manipulating Messenger Tokens&lt;/li&gt;
&lt;li&gt;Cross Modal Retrieval With Querybank Normalisation&lt;/li&gt;
&lt;li&gt;Ray3D: Ray-Based 3D Human Pose Estimation for Monocular Absolute 3D Localization&lt;/li&gt;
&lt;li&gt;ASM-Loc: Action-Aware Segment Modeling for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs&lt;/li&gt;
&lt;li&gt;End-to-End Multi-Person Pose Estimation With Transformers&lt;/li&gt;
&lt;li&gt;REGTR: End-to-End Point Cloud Correspondences With Transformers&lt;/li&gt;
&lt;li&gt;Neural 3D Scene Reconstruction With The Manhattan-World Assumption&lt;/li&gt;
&lt;li&gt;V2C: Visual Voice Cloning&lt;/li&gt;
&lt;li&gt;Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection&lt;/li&gt;
&lt;li&gt;MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions&lt;/li&gt;
&lt;li&gt;Gait Recognition in The Wild With Dense 3D Representations and A Benchmark&lt;/li&gt;
&lt;li&gt;ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation Via Online Exploration and Synthesis&lt;/li&gt;
&lt;li&gt;QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection&lt;/li&gt;
&lt;li&gt;IDEA-Net: Dynamic 3D Point Cloud Interpolation Via Deep Embedding Alignment&lt;/li&gt;
&lt;li&gt;BEHAVE: Dataset and Method for Tracking Human Object Interactions&lt;/li&gt;
&lt;li&gt;Revisiting Random Channel Pruning for Neural Network Compression&lt;/li&gt;
&lt;li&gt;Generating Diverse and Natural 3D Human Motions From Text&lt;/li&gt;
&lt;li&gt;E-CIR: Event-Enhanced Continuous Intensity Recovery&lt;/li&gt;
&lt;li&gt;Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond&lt;/li&gt;
&lt;li&gt;Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation&lt;/li&gt;
&lt;li&gt;AziNorm: Exploiting The Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception&lt;/li&gt;
&lt;li&gt;Weakly Supervised Rotation-Invariant Aerial Object Detection Network&lt;/li&gt;
&lt;li&gt;Surface Reconstruction From Point Clouds By Learning Predictive Context Priors&lt;/li&gt;
&lt;li&gt;IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes&lt;/li&gt;
&lt;li&gt;DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Action Localization Via Representative Snippet Knowledge Propagation&lt;/li&gt;
&lt;li&gt;E2EC: An End-to-End Contour-Based Method for High-Quality High-Speed Instance Segmentation&lt;/li&gt;
&lt;li&gt;BatchFormer: Learning To Explore Sample Relationships for Robust Representation Learning&lt;/li&gt;
&lt;li&gt;Self-Supervised Image-Specific Prototype Exploration for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Learning Multi-View Aggregation in The Wild for Large-Scale 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition&lt;/li&gt;
&lt;li&gt;Clothes-Changing Person Re-Identification With RGB Modality Only&lt;/li&gt;
&lt;li&gt;Robust Image Forgery Detection Over Online Social Network Shared Images&lt;/li&gt;
&lt;li&gt;Representation Compensation Networks for Continual Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Tracking People By Predicting 3D Appearance, Location and Pose&lt;/li&gt;
&lt;li&gt;Text2Mesh: Text-Driven Neural Stylization for Meshes&lt;/li&gt;
&lt;li&gt;C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image&lt;/li&gt;
&lt;li&gt;Forward Compatible Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Weakly Supervised Object Localization As Domain Adaption&lt;/li&gt;
&lt;li&gt;Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation&lt;/li&gt;
&lt;li&gt;Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching&lt;/li&gt;
&lt;li&gt;Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation&lt;/li&gt;
&lt;li&gt;MatteFormer: Transformer-Based Image Matting Via Prior-Tokens&lt;/li&gt;
&lt;li&gt;Video Shadow Detection Via Spatio-Temporal Interpolation Consistency Training&lt;/li&gt;
&lt;li&gt;Robust and Accurate Superquadric Recovery: A Probabilistic Approach&lt;/li&gt;
&lt;li&gt;Grounding Answers for Visual Questions Asked By Visually Impaired People&lt;/li&gt;
&lt;li&gt;Sparse Instance Activation for Real-Time Instance Segmentation&lt;/li&gt;
&lt;li&gt;VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning&lt;/li&gt;
&lt;li&gt;MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis&lt;/li&gt;
&lt;li&gt;Towards Implicit Text-Guided 3D Shape Generation&lt;/li&gt;
&lt;li&gt;SoftCollage: A Differentiable Probabilistic Tree Generator for Image Collage&lt;/li&gt;
&lt;li&gt;Query and Attention Augmentation for Knowledge-Based Explainable Reasoning&lt;/li&gt;
&lt;li&gt;Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality&lt;/li&gt;
&lt;li&gt;Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection&lt;/li&gt;
&lt;li&gt;Fine-Grained Object Classification Via Self-Supervised Pose Alignment&lt;/li&gt;
&lt;li&gt;Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding&lt;/li&gt;
&lt;li&gt;Fine-Grained Temporal Contrastive Learning for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Relieving Long-Tailed Instance Segmentation Via Pairwise Class Balance&lt;/li&gt;
&lt;li&gt;Online Convolutional Re-Parameterization&lt;/li&gt;
&lt;li&gt;Mimicking The Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning&lt;/li&gt;
&lt;li&gt;RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition&lt;/li&gt;
&lt;li&gt;Personalized Image Aesthetics Assessment With Rich Attributes&lt;/li&gt;
&lt;li&gt;Part-Based Pseudo Label Refinement for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;HDNet: High-Resolution Dual-Domain Learning for Spectral Compressive Imaging&lt;/li&gt;
&lt;li&gt;OW-DETR: Open-World Detection Transformer&lt;/li&gt;
&lt;li&gt;Learning Deep Implicit Functions for 3D Shapes With Dynamic Code Clouds&lt;/li&gt;
&lt;li&gt;Reversible Vision Transformers&lt;/li&gt;
&lt;li&gt;Amodal Panoptic Segmentation&lt;/li&gt;
&lt;li&gt;Correlation Verification for Image Retrieval&lt;/li&gt;
&lt;li&gt;Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut&lt;/li&gt;
&lt;li&gt;Exploring Structure-Aware Transformer Over Interaction Proposals for Human-Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Decoupled Multi-Task Learning With Cyclical Self-Regulation for Face Parsing&lt;/li&gt;
&lt;li&gt;Glass: Geometric Latent Augmentation for Shape Spaces&lt;/li&gt;
&lt;li&gt;DPICT: Deep Progressive Image Compression Using Trit-Planes&lt;/li&gt;
&lt;li&gt;Text to Image Generation With Semantic-Spatial Aware GAN&lt;/li&gt;
&lt;li&gt;Generalizable Cross-Modality Medical Image Segmentation Via Style Augmentation and Dual Normalization&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model&lt;/li&gt;
&lt;li&gt;Interactive Segmentation and Visualization for Tiny Objects in Multi-Megapixel Images&lt;/li&gt;
&lt;li&gt;Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture&lt;/li&gt;
&lt;li&gt;Surface Representation for Point Clouds&lt;/li&gt;
&lt;li&gt;Implicit Motion Handling for Video Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides&lt;/li&gt;
&lt;li&gt;Learning With Twin Noisy Labels for Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;Optical Flow Estimation for Spiking Camera&lt;/li&gt;
&lt;li&gt;GradViT: Gradient Inversion of Vision Transformers&lt;/li&gt;
&lt;li&gt;Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution Via Cycle-Projected Mutual Learning&lt;/li&gt;
&lt;li&gt;Joint Global and Local Hierarchical Priors for Learned Image Compression&lt;/li&gt;
&lt;li&gt;Knowledge Distillation Via The Target-Aware Transformer&lt;/li&gt;
&lt;li&gt;Subspace Adversarial Training&lt;/li&gt;
&lt;li&gt;3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection&lt;/li&gt;
&lt;li&gt;Image Segmentation Using Text and Image Prompts&lt;/li&gt;
&lt;li&gt;AutoMine: An Unmanned Mine Dataset&lt;/li&gt;
&lt;li&gt;Background Activation Suppression for Weakly Supervised Object Localization&lt;/li&gt;
&lt;li&gt;Synthetic Generation of Face Videos With Plethysmograph Physiology&lt;/li&gt;
&lt;li&gt;Hallucinated Neural Radiance Fields in The Wild&lt;/li&gt;
&lt;li&gt;Global Tracking Transformers&lt;/li&gt;
&lt;li&gt;Backdoor Attacks on Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;GMFlow: Learning Optical Flow Via Global Matching&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Explore Spatio-Temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline&lt;/li&gt;
&lt;li&gt;Graph-Based Spatial Transformer With Memory Replay for Multi-Future Pedestrian Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Scanline Homographies for Rolling-Shutter Plane Absolute Pose&lt;/li&gt;
&lt;li&gt;AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-Time Image Enhancement&lt;/li&gt;
&lt;li&gt;Recurrent Glimpse-Based Decoder for Detection With Transformer&lt;/li&gt;
&lt;li&gt;SimMIM: A Simple Framework for Masked Image Modeling&lt;/li&gt;
&lt;li&gt;Label Matching Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;RegionCLIP: Region-Based Language-Image Pretraining&lt;/li&gt;
&lt;li&gt;Video Frame Interpolation Transformer&lt;/li&gt;
&lt;li&gt;BCOT: A Markerless High-Precision 3D Object Tracking Benchmark&lt;/li&gt;
&lt;li&gt;Omni-DETR: Omni-Supervised Object Detection With Transformers&lt;/li&gt;
&lt;li&gt;Transferable Sparse Adversarial Attack&lt;/li&gt;
&lt;li&gt;CREAM: Weakly Supervised Object Localization Via Class RE-Activation Mapping&lt;/li&gt;
&lt;li&gt;VALHALLA: Visual Hallucination for Machine Translation&lt;/li&gt;
&lt;li&gt;HINT: Hierarchical Neuron Concept Explainer&lt;/li&gt;
&lt;li&gt;Neural Face Identification in A 2D Wireframe Projection of A Manifold Object&lt;/li&gt;
&lt;li&gt;Nonuniform-to-Uniform Quantization: Towards Accurate Quantization Via Generalized Straight-Through Estimation&lt;/li&gt;
&lt;li&gt;An Empirical Study of End-to-End Temporal Action Detection&lt;/li&gt;
&lt;li&gt;Object Localization Under Single Coarse Point Supervision&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Accurate Siamese Tracking&lt;/li&gt;
&lt;li&gt;Non-Parametric Depth Distribution Modelling Based Depth Inference for Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Equalized Focal Loss for Dense Long-Tailed Object Detection&lt;/li&gt;
&lt;li&gt;DeepDPM: Deep Clustering With An Unknown Number of Clusters&lt;/li&gt;
&lt;li&gt;ISDNet: Integrating Shallow and Deep Networks for Efficient Ultra-High Resolution Segmentation&lt;/li&gt;
&lt;li&gt;Unsupervised Domain Adaptation for Nighttime Aerial Tracking&lt;/li&gt;
&lt;li&gt;RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs&lt;/li&gt;
&lt;li&gt;Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction&lt;/li&gt;
&lt;li&gt;A Variational Bayesian Method for Similarity Learning in Non-Rigid Image Registration&lt;/li&gt;
&lt;li&gt;Not Just Selection, But Exploration: Online Class-Incremental Continual Learning Via Dual View Consistency&lt;/li&gt;
&lt;li&gt;Coupling Vision and Proprioception for Navigation of Legged Robots&lt;/li&gt;
&lt;li&gt;Exploiting Rigidity Constraints for LiDAR Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;EMOCA: Emotion Driven Monocular Face Capture and Animation&lt;/li&gt;
&lt;li&gt;Quarantine: Sparsity Can Uncover The Trojan Attack Trigger for Free&lt;/li&gt;
&lt;li&gt;AlignQ: Alignment Quantization With ADMM-Based Correlation Preservation&lt;/li&gt;
&lt;li&gt;Interactive Multi-Class Tiny-Object Detection&lt;/li&gt;
&lt;li&gt;Learning From Pixel-Level Noisy Label: A New Perspective for Light Field Saliency Detection&lt;/li&gt;
&lt;li&gt;Multi-View Depth Estimation By Fusing Single-View Depth Probability With Multi-View Geometry&lt;/li&gt;
&lt;li&gt;Slimmable Domain Adaptation&lt;/li&gt;
&lt;li&gt;High-Resolution Image Harmonization Via Collaborative Dual Transformations&lt;/li&gt;
&lt;li&gt;MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Supervised Neural Articulated Shape and Appearance Models&lt;/li&gt;
&lt;li&gt;Topology Preserving Local Road Network Estimation From Single Onboard Camera Image&lt;/li&gt;
&lt;li&gt;Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes&lt;/li&gt;
&lt;li&gt;SwinTextSpotter: Scene Text Spotting Via Better Synergy Between Text Detection and Text Recognition&lt;/li&gt;
&lt;li&gt;Deblur-NeRF: Neural Radiance Fields From Blurry Images&lt;/li&gt;
&lt;li&gt;Whose Track Is It Anyway? Improving Robustness to Tracking Errors With Affinity-Based Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation&lt;/li&gt;
&lt;li&gt;Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning&lt;/li&gt;
&lt;li&gt;Blind Image Super-Resolution With Elaborate Degradation Modeling on Noise and Kernel&lt;/li&gt;
&lt;li&gt;Faithful Extreme Rescaling Via Generative Prior Reciprocated Invertible Representations&lt;/li&gt;
&lt;li&gt;Proto2Proto: Can You Recognize The Car, The Way I Do?&lt;/li&gt;
&lt;li&gt;TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing&lt;/li&gt;
&lt;li&gt;Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Habitat-Web: Learning Embodied Object-Search Strategies From Human Demonstrations at Scale&lt;/li&gt;
&lt;li&gt;Simple But Effective: CLIP Embeddings for Embodied AI&lt;/li&gt;
&lt;li&gt;NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition&lt;/li&gt;
&lt;li&gt;Collaborative Transformers for Grounded Situation Recognition&lt;/li&gt;
&lt;li&gt;CPPF: Towards Robust Category-Level 9D Pose Estimation in The Wild&lt;/li&gt;
&lt;li&gt;Continual Test-Time Domain Adaptation&lt;/li&gt;
&lt;li&gt;Dynamic MLP for Fine-Grained Image Classification By Leveraging Geographical and Temporal Information&lt;/li&gt;
&lt;li&gt;MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering&lt;/li&gt;
&lt;li&gt;Fair Contrastive Learning for Facial Attribute Classification&lt;/li&gt;
&lt;li&gt;Directional Self-Supervised Learning for Heavy Image Augmentations&lt;/li&gt;
&lt;li&gt;No-Reference Point Cloud Quality Assessment Via Domain Adaptation&lt;/li&gt;
&lt;li&gt;Comprehending and Ordering Semantics for Image Captioning&lt;/li&gt;
&lt;li&gt;A Large-Scale Comprehensive Dataset and Copy-Overlap Aware Evaluation Protocol for Segment-Level Video Copy Detection&lt;/li&gt;
&lt;li&gt;Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification&lt;/li&gt;
&lt;li&gt;HeadNeRF: A Real-Time NeRF-Based Parametric Head Model&lt;/li&gt;
&lt;li&gt;Occlusion-Robust Face Alignment Using A Viewpoint-Invariant Hierarchical Network Architecture&lt;/li&gt;
&lt;li&gt;IDR: Self-Supervised Image Denoising Via Iterative Data Refinement&lt;/li&gt;
&lt;li&gt;MogFace: Towards A Deeper Appreciation on Face Detection&lt;/li&gt;
&lt;li&gt;Learning Affinity From Attention: End-to-End Weakly-Supervised Semantic Segmentation With Transformers&lt;/li&gt;
&lt;li&gt;CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos&lt;/li&gt;
&lt;li&gt;Learning To Detect Mobile Objects From LiDAR Scans Without Labels&lt;/li&gt;
&lt;li&gt;WildNet: Learning Domain Generalized Semantic Segmentation From The Wild&lt;/li&gt;
&lt;li&gt;DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection&lt;/li&gt;
&lt;li&gt;Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Generating Diverse 3D Reconstructions From A Single Occluded Face Image&lt;/li&gt;
&lt;li&gt;Stand-Alone Inter-Frame Attention in Video Models&lt;/li&gt;
&lt;li&gt;Large-Scale Pre-Training for Person Re-Identification With Noisy Labels&lt;/li&gt;
&lt;li&gt;Semantic Segmentation By Early Region Proxy&lt;/li&gt;
&lt;li&gt;LD-ConGR: A Large RGB-D Video Dataset for Long-Distance Continuous Gesture Recognition&lt;/li&gt;
&lt;li&gt;HVH: Learning A Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture&lt;/li&gt;
&lt;li&gt;Rethinking Visual Geo-Localization for Large-Scale Applications&lt;/li&gt;
&lt;li&gt;The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy&lt;/li&gt;
&lt;li&gt;ViM: Out-of-Distribution With Virtual-Logit Matching&lt;/li&gt;
&lt;li&gt;Class-Aware Contrastive Semi-Supervised Learning&lt;/li&gt;
&lt;li&gt;Ditto: Building Digital Twins of Articulated Objects From Interaction&lt;/li&gt;
&lt;li&gt;Adaptive Early-Learning Correction for Segmentation From Noisy Annotations&lt;/li&gt;
&lt;li&gt;Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation&lt;/li&gt;
&lt;li&gt;RSTT: Real-Time Spatial Temporal Transformer for Space-Time Video Super-Resolution&lt;/li&gt;
&lt;li&gt;Partial Class Activation Attention for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Multi-Scale Memory-Based Video Deblurring&lt;/li&gt;
&lt;li&gt;A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching&lt;/li&gt;
&lt;li&gt;Geometric Structure Preserving Warp for Natural Image Stitching&lt;/li&gt;
&lt;li&gt;GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping&lt;/li&gt;
&lt;li&gt;Conditional Prompt Learning for Vision-Language Models&lt;/li&gt;
&lt;li&gt;Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;Undoing The Damage of Label Shift for Cross-Domain Semantic Segmentation&lt;/li&gt;
&lt;li&gt;FisherMatch: Semi-Supervised Rotation Regression Via Entropy-Based Filtering&lt;/li&gt;
&lt;li&gt;Affine Medical Image Registration With Coarse-To-Fine Vision Transformer&lt;/li&gt;
&lt;li&gt;A Differentiable Two-Stage Alignment Scheme for Burst Image Reconstruction With Large Shift&lt;/li&gt;
&lt;li&gt;Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes&lt;/li&gt;
&lt;li&gt;Restormer: Efficient Transformer for High-Resolution Image Restoration&lt;/li&gt;
&lt;li&gt;IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation&lt;/li&gt;
&lt;li&gt;Large Loss Matters in Weakly Supervised Multi-Label Classification&lt;/li&gt;
&lt;li&gt;Neural Inertial Localization&lt;/li&gt;
&lt;li&gt;GraftNet: Towards Domain Generalized Stereo Matching With A Broad-Spectrum and Task-Oriented Feature&lt;/li&gt;
&lt;li&gt;VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Catching Both Gray and Black Swans: Open-Set Supervised Anomaly Detection&lt;/li&gt;
&lt;li&gt;MLSLT: Towards Multilingual Sign Language Translation&lt;/li&gt;
&lt;li&gt;Towards An End-to-End Framework for Flow-Guided Video Inpainting&lt;/li&gt;
&lt;li&gt;Contrastive Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;MotionAug: Augmentation With Physical Correction for Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Modeling Indirect Illumination for Inverse Rendering&lt;/li&gt;
&lt;li&gt;TransWeather: Transformer-Based Restoration of Images Degraded By Adverse Weather Conditions&lt;/li&gt;
&lt;li&gt;H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection&lt;/li&gt;
&lt;li&gt;P3Depth: Monocular Depth Estimation With A Piecewise Planarity Prior&lt;/li&gt;
&lt;li&gt;GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection&lt;/li&gt;
&lt;li&gt;Simple Multi-Dataset Detection&lt;/li&gt;
&lt;li&gt;Proactive Image Manipulation Detection&lt;/li&gt;
&lt;li&gt;StyTr2: Image Style Transfer With Transformers&lt;/li&gt;
&lt;li&gt;Global Matching With Overlapping Attention for Optical Flow Estimation&lt;/li&gt;
&lt;li&gt;Language As Queries for Referring Video Object Segmentation&lt;/li&gt;
&lt;li&gt;MViTv2: Improved Multiscale Vision Transformers for Classification and Detection&lt;/li&gt;
&lt;li&gt;Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language&lt;/li&gt;
&lt;li&gt;Rethinking Efficient Lane Detection Via Curve Modeling&lt;/li&gt;
&lt;li&gt;Self-Supervised Arbitrary-Scale Point Clouds Upsampling Via Implicit Neural Representation&lt;/li&gt;
&lt;li&gt;Co-Advise: Cross Inductive Bias Distillation&lt;/li&gt;
&lt;li&gt;AdaMixer: A Fast-Converging Query-Based Object Detector&lt;/li&gt;
&lt;li&gt;DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification&lt;/li&gt;
&lt;li&gt;BEVT: BERT Pretraining of Video Transformers&lt;/li&gt;
&lt;li&gt;Deep Generalized Unfolding Networks for Image Restoration&lt;/li&gt;
&lt;li&gt;VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;Deep Unlearning Via Randomized Conditionally Independent Hessians&lt;/li&gt;
&lt;li&gt;Revisiting Skeleton-Based Action Recognition&lt;/li&gt;
&lt;li&gt;Stereo Depth From Events Cameras: Concentrate and Focus on The Future&lt;/li&gt;
&lt;li&gt;A Simple Data Mixing Prior for Improving Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;Knowledge Distillation As Efficient Pre-Training: Faster Convergence, Higher Data-Efficiency, and Better Transferability&lt;/li&gt;
&lt;li&gt;BigDL 2.0: Seamless Scaling of AI Pipelines From Laptops to Distributed Cluster&lt;/li&gt;
&lt;li&gt;Attentive Fine-Grained Structured Sparsity for Image Restoration&lt;/li&gt;
&lt;li&gt;Learning Fair Classifiers With Partially Annotated Group Labels&lt;/li&gt;
&lt;li&gt;NightLab: A Dual-Level Architecture With Hardness Detection for Segmentation at Night&lt;/li&gt;
&lt;li&gt;Constrained Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Threshold Matters in WSSS: Manipulating The Activation for The Robust and Accurate Segmentation Model Against Thresholds&lt;/li&gt;
&lt;li&gt;TransMVSNet: Global Context-Aware Multi-View Stereo Network With Transformers&lt;/li&gt;
&lt;li&gt;DPGEN: Differentially Private Generative Energy-Guided Network for Natural Image Synthesis&lt;/li&gt;
&lt;li&gt;The Majority Can Help The Minority: Context-Rich Minority Oversampling for Long-Tailed Classification&lt;/li&gt;
&lt;li&gt;IntentVizor: Towards Generic Query Guided Interactive Video Summarization&lt;/li&gt;
&lt;li&gt;Shape-Invariant 3D Adversarial Point Clouds&lt;/li&gt;
&lt;li&gt;Bootstrapping ViTs: Towards Liberating Vision Transformers From Pre-Training&lt;/li&gt;
&lt;li&gt;PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents&lt;/li&gt;
&lt;li&gt;Meta-Attention for ViT-Backed Continual Learning&lt;/li&gt;
&lt;li&gt;DST: Dynamic Substitute Training for Data-Free Black-Box Attack&lt;/li&gt;
&lt;li&gt;Unified Contrastive Learning in Image-Text-Label Space&lt;/li&gt;
&lt;li&gt;Unsupervised Pre-Training for Temporal Action Localization Tasks&lt;/li&gt;
&lt;li&gt;Look Outside The Room: Synthesizing A Consistent Long-Term 3D Scene Video From A Single Image&lt;/li&gt;
&lt;li&gt;High-Fidelity Human Avatars From A Single RGB Camera&lt;/li&gt;
&lt;li&gt;Multiview Transformers for Video Recognition&lt;/li&gt;
&lt;li&gt;How Good Is Aesthetic Ability of A Fashion Model?&lt;/li&gt;
&lt;li&gt;Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds&lt;/li&gt;
&lt;li&gt;Sequential Voting With Relational Box Fields for Active Object Detection&lt;/li&gt;
&lt;li&gt;Semantic-Aware Auto-Encoders for Self-Supervised Representation Learning&lt;/li&gt;
&lt;li&gt;Consistency Learning Via Decoding Path Augmentation for Transformers in Human Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Consistent Explanations By Contrastive Learning&lt;/li&gt;
&lt;li&gt;Hierarchical Modular Network for Video Captioning&lt;/li&gt;
&lt;li&gt;Depth Estimation By Combining Binocular Stereo and Monocular Structured-Light&lt;/li&gt;
&lt;li&gt;Salient-to-Broad Transition for Video Person Re-Identification&lt;/li&gt;
&lt;li&gt;DeeCap: Dynamic Early Exiting for Efficient Image Captioning&lt;/li&gt;
&lt;li&gt;RepMLPNet: Hierarchical Vision MLP With Re-Parameterized Locality&lt;/li&gt;
&lt;li&gt;DR.VIC: Decomposition and Reasoning for Video Individual Counting&lt;/li&gt;
&lt;li&gt;ARCS: Accurate Rotation and Correspondence Search&lt;/li&gt;
&lt;li&gt;Learning To Anticipate Future With Dynamic Context Removal&lt;/li&gt;
&lt;li&gt;GCFSR: A Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors&lt;/li&gt;
&lt;li&gt;On The Integration of Self-Attention and Convolution&lt;/li&gt;
&lt;li&gt;Domain Adaptation on Point Clouds Via Geometry-Aware Implicits&lt;/li&gt;
&lt;li&gt;GroupViT: Semantic Segmentation Emerges From Text Supervision&lt;/li&gt;
&lt;li&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/li&gt;
&lt;li&gt;BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks Via Image Quantization and Contrastive Adversarial Learning&lt;/li&gt;
&lt;li&gt;Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation&lt;/li&gt;
&lt;li&gt;Towards Better Plasticity-Stability Trade-Off in Incremental Learning: A Simple Linear Connector&lt;/li&gt;
&lt;li&gt;Topology-Preserving Shape Reconstruction and Registration Via Neural Diffeomorphic Flow&lt;/li&gt;
&lt;li&gt;Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection&lt;/li&gt;
&lt;li&gt;MAXIM: Multi-Axis MLP for Image Processing&lt;/li&gt;
&lt;li&gt;Learning Part Segmentation Through Unsupervised Domain Adaptation From Synthetic Vehicles&lt;/li&gt;
&lt;li&gt;PSTR: End-to-End One-Step Person Search With Transformers&lt;/li&gt;
&lt;li&gt;NFormer: Robust Person Re-Identification With Neighbor Transformer&lt;/li&gt;
&lt;li&gt;Bridging Global Context Interactions for High-Fidelity Image Completion&lt;/li&gt;
&lt;li&gt;SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning&lt;/li&gt;
&lt;li&gt;Not All Tokens Are Equal: Human-Centric Visual Analysis Via Token Clustering Transformer&lt;/li&gt;
&lt;li&gt;Temporally Efficient Vision Transformer for Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;The Devil Is in The Margin: Margin-Based Label Smoothing for Network Calibration&lt;/li&gt;
&lt;li&gt;NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks&lt;/li&gt;
&lt;li&gt;WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation&lt;/li&gt;
&lt;li&gt;Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding&lt;/li&gt;
&lt;li&gt;E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition&lt;/li&gt;
&lt;li&gt;OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization&lt;/li&gt;
&lt;li&gt;OnePose: One-Shot Object Pose Estimation Without CAD Models&lt;/li&gt;
&lt;li&gt;Rethinking Minimal Sufficient Representation in Contrastive Learning&lt;/li&gt;
&lt;li&gt;Scalable Penalized Regression for Noise Detection in Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;Federated Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Show, Deconfound and Tell: Image Captioning With Causal Inference&lt;/li&gt;
&lt;li&gt;MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image&lt;/li&gt;
&lt;li&gt;Parameter-Free Online Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;SIGMA: Semantic-Complete Graph Matching for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;No Pain, Big Gain: Classify Dynamic Point Cloud Sequences With Static Models By Fitting Feature-Level Space-Time Surfaces&lt;/li&gt;
&lt;li&gt;HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging&lt;/li&gt;
&lt;li&gt;Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space&lt;/li&gt;
&lt;li&gt;Learning To Estimate Robust 3D Human Mesh From In-the-Wild Crowded Scenes&lt;/li&gt;
&lt;li&gt;Detecting Deepfakes With Self-Blended Images&lt;/li&gt;
&lt;li&gt;Implicit Sample Extension for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;Energy-Based Latent Aligner for Incremental Learning&lt;/li&gt;
&lt;li&gt;Towards Semi-Supervised Deep Facial Expression Recognition With An Adaptive Confidence Margin&lt;/li&gt;
&lt;li&gt;Group R-CNN for Weakly Semi-Supervised Object Detection With Points&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Action Transition Learning for Stochastic Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Hybrid Relation Guided Set Matching for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Cross-Patch Dense Contrastive Learning for Semi-Supervised Segmentation of Cellular Nuclei in Histopathologic Images&lt;/li&gt;
&lt;li&gt;Generalized Binary Search Network for Highly-Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation&lt;/li&gt;
&lt;li&gt;FlexIT: Towards Flexible Semantic Image Translation&lt;/li&gt;
&lt;li&gt;CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow&lt;/li&gt;
&lt;li&gt;BoxeR: Box-Attention for 2D and 3D Transformers&lt;/li&gt;
&lt;li&gt;Neural Architecture Search With Representation Mutual Information&lt;/li&gt;
&lt;li&gt;Can Neural Nets Learn The Same Model Twice? Investigating Reproducibility and Double Descent From The Decision Boundary Perspective&lt;/li&gt;
&lt;li&gt;Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction&lt;/li&gt;
&lt;li&gt;Multi-View Transformer for 3D Visual Grounding&lt;/li&gt;
&lt;li&gt;Structured Sparse R-CNN for Direct Scene Graph Generation&lt;/li&gt;
&lt;li&gt;BARC: Learning To Regress 3D Dog Shape From Images By Exploiting Breed Information&lt;/li&gt;
&lt;li&gt;PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models&lt;/li&gt;
&lt;li&gt;Towards Understanding Adversarial Robustness of Optical Flow Networks&lt;/li&gt;
&lt;li&gt;Lifelong Graph Learning&lt;/li&gt;
&lt;li&gt;Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning&lt;/li&gt;
&lt;li&gt;Computing Wasserstein-p Distance Between Images With Linear Cost&lt;/li&gt;
&lt;li&gt;Unsupervised Representation Learning for Binary Networks By Joint Classifier Learning&lt;/li&gt;
&lt;li&gt;Large-Scale Video Panoptic Segmentation in The Wild: A Benchmark&lt;/li&gt;
&lt;li&gt;GrainSpace: A Large-Scale Dataset for Fine-Grained and Domain-Adaptive Recognition of Cereal Grains&lt;/li&gt;
&lt;li&gt;Learning Modal-Invariant and Temporal-Memory for Video-Based Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Oriented RepPoints for Aerial Object Detection&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning&lt;/li&gt;
&lt;li&gt;Low-Resource Adaptation for Personalized Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Task-Specific Inconsistency Alignment for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;MS2DG-Net: Progressive Correspondence Learning Via Multiple Sparse Semantics Dynamic Graph&lt;/li&gt;
&lt;li&gt;Learning To Listen: Modeling Non-Deterministic Dyadic Facial Motion&lt;/li&gt;
&lt;li&gt;Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation From Monocular Video&lt;/li&gt;
&lt;li&gt;MixFormer: End-to-End Tracking With Iterative Mixed Attention&lt;/li&gt;
&lt;li&gt;Plenoxels: Radiance Fields Without Neural Networks&lt;/li&gt;
&lt;li&gt;Selective-Supervised Contrastive Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;SimT: Handling Open-Set Noise for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Frequency-Driven Imperceptible Adversarial Attack on Semantic Similarity&lt;/li&gt;
&lt;li&gt;Video Demoireing With Relation-Based Temporal Consistency&lt;/li&gt;
&lt;li&gt;Industrial Style Transfer With Large-Scale Geometric Warping and Content Preservation&lt;/li&gt;
&lt;li&gt;Modeling Image Composition for Complex Scene Generation&lt;/li&gt;
&lt;li&gt;Decoupling Zero-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions&lt;/li&gt;
&lt;li&gt;Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting The Adversarial Transferability&lt;/li&gt;
&lt;li&gt;IFOR: Iterative Flow Minimization for Robotic Object Rearrangement&lt;/li&gt;
&lt;li&gt;Zero Experience Required: Plug &amp;amp; Play Modular Transfer Learning for Semantic Visual Navigation&lt;/li&gt;
&lt;li&gt;TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation&lt;/li&gt;
&lt;li&gt;The Wanderings of Odysseus in 3D Scenes&lt;/li&gt;
&lt;li&gt;All-in-One Image Restoration for Unknown Corruption&lt;/li&gt;
&lt;li&gt;PUMP: Pyramidal and Uniqueness Matching Priors for Unsupervised Learning of Local Descriptors&lt;/li&gt;
&lt;li&gt;MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video&lt;/li&gt;
&lt;li&gt;RCP: Recurrent Closest Point for Point Cloud&lt;/li&gt;
&lt;li&gt;A Dual Weighting Label Assignment Scheme for Object Detection&lt;/li&gt;
&lt;li&gt;Hyperbolic Vision Transformers: Combining Improvements in Metric Learning&lt;/li&gt;
&lt;li&gt;Instance-Aware Dynamic Neural Network Quantization&lt;/li&gt;
&lt;li&gt;Exploring Effective Data for Surrogate Training Towards Black-Box Attack&lt;/li&gt;
&lt;li&gt;JRDB-Act: A Large-Scale Dataset for Spatio-Temporal Action, Social Group and Activity Detection&lt;/li&gt;
&lt;li&gt;Investigating Top-k White-Box and Transferable Black-Box Attack&lt;/li&gt;
&lt;li&gt;Decoupling and Recoupling Spatiotemporal Representation for RGB-D-Based Motion Recognition&lt;/li&gt;
&lt;li&gt;A Self-Supervised Descriptor for Image Copy Detection&lt;/li&gt;
&lt;li&gt;Negative-Aware Attention Framework for Image-Text Matching&lt;/li&gt;
&lt;li&gt;An Image Patch Is A Wave: Phase-Aware Vision MLP&lt;/li&gt;
&lt;li&gt;Shunted Self-Attention Via Multi-Scale Token Aggregation&lt;/li&gt;
&lt;li&gt;Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression&lt;/li&gt;
&lt;li&gt;Recurrent Variational Network: A Deep Learning Inverse Problem Solver Applied to The Task of Accelerated MRI Reconstruction&lt;/li&gt;
&lt;li&gt;Surpassing The Human Accuracy: Detecting Gallbladder Cancer From USG Images With Curriculum Learning&lt;/li&gt;
&lt;li&gt;Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond&lt;/li&gt;
&lt;li&gt;TrackFormer: Multi-Object Tracking With Transformers&lt;/li&gt;
&lt;li&gt;3D Shape Reconstruction From 2D Images With Disentangled Attribute Flow&lt;/li&gt;
&lt;li&gt;Feature Statistics Mixing Regularization for Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;OpenTAL: Towards Open Set Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection&lt;/li&gt;
&lt;li&gt;Ego4D: Around The World in 3,000 Hours of Egocentric Video&lt;/li&gt;
&lt;li&gt;Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Weakly Supervised Semantic Segmentation Using Out-of-Distribution Data&lt;/li&gt;
&lt;li&gt;DAD-3DHeads: A Large-Scale Dense, Accurate and Diverse Dataset for 3D Head Alignment From A Single Image&lt;/li&gt;
&lt;li&gt;Reconstructing Surfaces for Sparse Point Clouds With On-Surface Priors&lt;/li&gt;
&lt;li&gt;VCLIMB: A Novel Video Class Incremental Learning Benchmark&lt;/li&gt;
&lt;li&gt;Robust Equivariant Imaging: A Fully Unsupervised Framework for Learning To Image From Noisy and Partial Measurements&lt;/li&gt;
&lt;li&gt;ST++: Make Self-Training Work Better for Semi-Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Interacting Attention Graph for Single Image Two-Hand Reconstruction&lt;/li&gt;
&lt;li&gt;Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task&lt;/li&gt;
&lt;li&gt;Cross-Image Relational Knowledge Distillation for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Towards Layer-Wise Image Vectorization&lt;/li&gt;
&lt;li&gt;Scenic: A JAX Library for Computer Vision Research and Beyond&lt;/li&gt;
&lt;li&gt;Real-Time Object Detection for Streaming Perception&lt;/li&gt;
&lt;li&gt;VisualHow: Multimodal Problem Solving&lt;/li&gt;
&lt;li&gt;Spatial Commonsense Graph for Object Localisation in Partial Scenes&lt;/li&gt;
&lt;li&gt;OSSGAN: Open-Set Semi-Supervised Image Generation&lt;/li&gt;
&lt;li&gt;Bi-Level Alignment for Cross-Domain Crowd Counting&lt;/li&gt;
&lt;li&gt;ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation&lt;/li&gt;
&lt;li&gt;Efficient Multi-View Stereo By Iterative Dynamic Cost Volume&lt;/li&gt;
&lt;li&gt;TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing&lt;/li&gt;
&lt;li&gt;Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework&lt;/li&gt;
&lt;li&gt;SGTR: End-to-End Scene Graph Generation With Transformer&lt;/li&gt;
&lt;li&gt;Decoupled Knowledge Distillation&lt;/li&gt;
&lt;li&gt;DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection&lt;/li&gt;
&lt;li&gt;Reusing The Task-Specific Classifier As A Discriminator: Discriminator-Free Adversarial Domain Adaptation&lt;/li&gt;
&lt;li&gt;Show Me What and Tell Me How: Video Synthesis Via Multimodal Conditioning&lt;/li&gt;
&lt;li&gt;SIMBAR: Single Image-Based Scene Relighting for Effective Data Augmentation for Automated Driving Vision Tasks&lt;/li&gt;
&lt;li&gt;Multi-Label Classification With Partial Annotations Using Class-Aware Selective Loss&lt;/li&gt;
&lt;li&gt;CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings&lt;/li&gt;
&lt;li&gt;IntraQ: Learning Synthetic Images With Intra-Class Heterogeneity for Zero-Shot Network Quantization&lt;/li&gt;
&lt;li&gt;I M Avatar: Implicit Morphable Head Avatars From Videos&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Metric Learning With Cross-Module Communications for The Classification of Anterior Chamber Angle Images&lt;/li&gt;
&lt;li&gt;A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Multi-Modal Dynamic Graph Transformer for Visual Grounding&lt;/li&gt;
&lt;li&gt;Geometric Transformer for Fast and Robust Point Cloud Registration&lt;/li&gt;
&lt;li&gt;UMT: Unified Multi-Modal Transformers for Joint Video Moment Retrieval and Highlight Detection&lt;/li&gt;
&lt;li&gt;Demystifying The Neural Tangent Kernel From A Practical Perspective: Can It Be Trusted for Neural Architecture Search Without Training?&lt;/li&gt;
&lt;li&gt;The Devil Is in The Details: Window-Based Attention for Image Compression&lt;/li&gt;
&lt;li&gt;DiLiGenT102: A Photometric Stereo Benchmark Dataset With Controlled Shape and Material Variation&lt;/li&gt;
&lt;li&gt;PolyWorld: Polygonal Building Extraction With Graph Neural Networks in Satellite Images&lt;/li&gt;
&lt;li&gt;Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Spatio-Temporal Relation Modeling for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Multi-Person Extreme Motion Prediction&lt;/li&gt;
&lt;li&gt;B-DARTS: Beta-Decay Regularization for Differentiable Architecture Search&lt;/li&gt;
&lt;li&gt;CMT: Convolutional Neural Networks Meet Vision Transformers&lt;/li&gt;
&lt;li&gt;KNN Local Attention for Image Restoration&lt;/li&gt;
&lt;li&gt;Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered By Pre-Trained Vision-Language Model&lt;/li&gt;
&lt;li&gt;TransMix: Attend To Mix for Vision Transformers&lt;/li&gt;
&lt;li&gt;Inertia-Guided Flow Completion and Style Fusion for Video Inpainting&lt;/li&gt;
&lt;li&gt;Long-Tailed Visual Recognition Via Gaussian Clouded Logit Adjustment&lt;/li&gt;
&lt;li&gt;Image Animation With Perturbed Masks&lt;/li&gt;
&lt;li&gt;Domain Generalization Via Shuffled Style Assembly for Face Anti-Spoofing&lt;/li&gt;
&lt;li&gt;OcclusionFusion: Occlusion-Aware Motion Estimation for Real-Time Dynamic 3D Reconstruction&lt;/li&gt;
&lt;li&gt;MonoScene: Monocular 3D Semantic Scene Completion&lt;/li&gt;
&lt;li&gt;AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition&lt;/li&gt;
&lt;li&gt;Continuous Scene Representations for Embodied AI&lt;/li&gt;
&lt;li&gt;Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds&lt;/li&gt;
&lt;li&gt;Non-Probability Sampling Network for Stochastic Human Trajectory Prediction&lt;/li&gt;
&lt;li&gt;ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning&lt;/li&gt;
&lt;li&gt;Human-Aware Object Placement for Visual Environment Reconstruction&lt;/li&gt;
&lt;li&gt;X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval&lt;/li&gt;
&lt;li&gt;RAMA: A Rapid Multicut Algorithm on GPU&lt;/li&gt;
&lt;li&gt;Adversarial Parametric Pose Prior&lt;/li&gt;
&lt;li&gt;Mask Transfiner for High-Quality Instance Segmentation&lt;/li&gt;
&lt;li&gt;It Is Okay To Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning By Contrastive Data Collection&lt;/li&gt;
&lt;li&gt;DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Event-Based Video Reconstruction Via Potential-Assisted Spiking Neural Network&lt;/li&gt;
&lt;li&gt;YouMVOS: An Actor-Centric Multi-Shot Video Object Segmentation Dataset&lt;/li&gt;
&lt;li&gt;DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification&lt;/li&gt;
&lt;li&gt;Self-Supervised Video Transformer&lt;/li&gt;
&lt;li&gt;AutoRF: Learning 3D Object Radiance Fields From Single View Observations&lt;/li&gt;
&lt;li&gt;Coopernaut: End-to-End Driving With Cooperative Perception for Networked Vehicles&lt;/li&gt;
&lt;li&gt;TubeR: Tubelet Transformer for Video Action Detection&lt;/li&gt;
&lt;li&gt;MUM: Mix Image Tiles and UnMix Feature Tiles for Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;Learning Non-Target Knowledge for Few-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;UKPGAN: A General Self-Supervised Keypoint Detector&lt;/li&gt;
&lt;li&gt;Raw High-Definition Radar for Multi-Task Learning&lt;/li&gt;
&lt;li&gt;Coarse-To-Fine Feature Mining for Video Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Compressing Models With Few Samples: Mimicking Then Replacing&lt;/li&gt;
&lt;li&gt;PokeBNN: A Binary Pursuit of Lightweight Accuracy&lt;/li&gt;
&lt;li&gt;Zoom in and Out: A Mixed-Scale Triplet Network for Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images&lt;/li&gt;
&lt;li&gt;EMScore: Evaluating Video Captioning Via Coarse-Grained and Fine-Grained Embedding Matching&lt;/li&gt;
&lt;li&gt;PoseTriplet: Co-Evolving 3D Human Pose Estimation, Imitation, and Hallucination Under Self-Supervision&lt;/li&gt;
&lt;li&gt;Group Contextualization for Video Recognition&lt;/li&gt;
&lt;li&gt;Single-Domain Generalized Object Detection in Urban Scene Via Cyclic-Disentangled Self-Distillation&lt;/li&gt;
&lt;li&gt;L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Augmented Unpaired Image Dehazing Via Density and Depth Decomposition&lt;/li&gt;
&lt;li&gt;Neural 3D Video Synthesis From Multi-View Video&lt;/li&gt;
&lt;li&gt;SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search&lt;/li&gt;
&lt;li&gt;HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening&lt;/li&gt;
&lt;li&gt;Structure-Aware Flow Generation for Human Body Reshaping&lt;/li&gt;
&lt;li&gt;Learning To Answer Questions in Dynamic Audio-Visual Scenarios&lt;/li&gt;
&lt;li&gt;Synthetic Aperture Imaging With Events and Frames&lt;/li&gt;
&lt;li&gt;MonoGround: Detecting Monocular 3D Objects From The Ground&lt;/li&gt;
&lt;li&gt;Deep Visual Geo-Localization Benchmark&lt;/li&gt;
&lt;li&gt;StyleGAN-V: A Continuous Video Generator With The Price, Image Quality and Perks of StyleGAN2&lt;/li&gt;
&lt;li&gt;LISA: Learning Implicit Shape and Appearance of Hands&lt;/li&gt;
&lt;li&gt;Iterative Deep Homography Estimation&lt;/li&gt;
&lt;li&gt;Learned Queries for Efficient Local Attention&lt;/li&gt;
&lt;li&gt;Colar: Effective and Efficient Online Action Detection By Consulting Exemplars&lt;/li&gt;
&lt;li&gt;SoftGroup for 3D Instance Segmentation on Point Clouds&lt;/li&gt;
&lt;li&gt;MVS2D: Efficient Multi-View Stereo Via Attention-Driven 2D Convolutions&lt;/li&gt;
&lt;li&gt;Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation Via Semantic Knowledge Transfer and Self-Refinement&lt;/li&gt;
&lt;li&gt;Deep Constrained Least Squares for Blind Image Super-Resolution&lt;/li&gt;
&lt;li&gt;EDTER: Edge Detection With Transformer&lt;/li&gt;
&lt;li&gt;AirObject: A Temporally Evolving Graph Embedding for Object Identification&lt;/li&gt;
&lt;li&gt;From Representation to Reasoning: Towards Both Evidence and Commonsense Reasoning for Video Question-Answering&lt;/li&gt;
&lt;li&gt;Semantic-Aware Domain Generalized Segmentation&lt;/li&gt;
&lt;li&gt;DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion&lt;/li&gt;
&lt;li&gt;UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection&lt;/li&gt;
&lt;li&gt;AKB-48: A Real-World Articulated Object Knowledge Base&lt;/li&gt;
&lt;li&gt;Stratified Transformer for 3D Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Aug-NeRF: Training Stronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmentations&lt;/li&gt;
&lt;li&gt;Semantic-Shape Adaptive Feature Modulation for Semantic Image Synthesis&lt;/li&gt;
&lt;li&gt;Day-to-Night Image Synthesis for Training Nighttime Neural ISPs Literature ~Highlight: To address this problem, we propose a method that synthesizes nighttime images from daytime images.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;References 
    &lt;div id=&#34;references&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#references&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&#34; target=&#34;_blank&#34;&gt;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;&lt;br&gt;
Dr Hari Thapliyaal&lt;br&gt;
dasarpai.com&lt;br&gt;
linkedin.com/in/harithapliyal&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</title>
      <link>/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg&#34; alt=&#34;Pretrained Language Models for Text Generation&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper Name :- Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt;&lt;br&gt;
Typer of Paper:- Survey Paper&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34; target=&#34;_blank&#34;&gt;Paper URL&lt;/a&gt;&lt;br&gt;
Paper title of the citations mentioned can be found at &lt;a href=&#34;/dsblog/aip&#34;&gt;AI Papers with Heading&lt;/a&gt;. Use citation code to locate.&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary :- Pretrained Language Models for Text Generation 
    &lt;div id=&#34;paper-summary---pretrained-language-models-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary---pretrained-language-models-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Paper Outcome 
    &lt;div id=&#34;paper-outcome&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-outcome&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;General task deﬁnition&lt;/li&gt;
&lt;li&gt;Describe the mainstream architectures of PLMs for text generation.&lt;/li&gt;
&lt;li&gt;How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.&lt;/li&gt;
&lt;li&gt;Summarize several important ﬁne-tuning strategies for text generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Ideas from the Paper 
    &lt;div id=&#34;ideas-from-the-paper&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ideas-from-the-paper&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Main Ideas 
    &lt;div id=&#34;main-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#main-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This paper discusses &amp;ldquo;major advances achieved in the topic of PLMs for text generation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;This survey aims to provide &amp;ldquo;text generation researchers a synthesis&amp;rdquo; and pointer to related research.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;General Ideas 
    &lt;div id=&#34;general-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#general-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).&lt;/li&gt;
&lt;li&gt;Neural generation model are deep learning models&lt;/li&gt;
&lt;li&gt;Pretrained language models (PLMs) are neural generation model&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Task Types and Typical Applications 
    &lt;div id=&#34;task-types-and-typical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#task-types-and-typical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )&lt;/li&gt;
&lt;li&gt;If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional
generation task(generate text without any constraint) &lt;a href=&#34;/dsblog/aip#radford2019&#34;&gt;Radford2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or
attribute-based generation. X plays the role of guiding the text generation. &lt;a href=&#34;/dsblog/aip#Keskar2019&#34;&gt;Keskar2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation &lt;a href=&#34;/dsblog/aip#Li2021c&#34;&gt;Li2021c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as image, the task becomes image caption &lt;a href=&#34;/dsblog/aip#Xia2020&#34;&gt;Xia2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as speech, the task become speech recognition &lt;a href=&#34;/dsblog/aip#Fan2019&#34;&gt;Fan2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.&lt;/li&gt;
&lt;li&gt;Machine translation aims to translate text from one language into another language automatically &lt;a href=&#34;/dsblog/aip#Conneau2019&#34;&gt;Conneau2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generating condensed summary of a long document &lt;a href=&#34;/dsblog/aip#Zhang2019b&#34;&gt;Zhang2019b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dialogue system to converse with humans using natural language. &lt;a href=&#34;/dsblog/aip#Wolf2019&#34;&gt;Wolf2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Architectures for Text Generation 
    &lt;div id=&#34;architectures-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#architectures-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.
&lt;ul&gt;
&lt;li&gt;MASS &lt;a href=&#34;/dsblog/aip#song2019&#34;&gt;Song2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T5 &lt;a href=&#34;/dsblog/aip#raffel2020&#34;&gt;Raffel2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BART &lt;a href=&#34;/dsblog/aip#lewis2020&#34;&gt;Lewis2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.
&lt;ul&gt;
&lt;li&gt;GPT &lt;a href=&#34;/dsblog/aip#radfordet2019&#34;&gt;Radfordet2019&lt;/a&gt;; &lt;a href=&#34;/dsblog/aip#brown2020&#34;&gt;Brown2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CTRL [Keskar2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Modeling Different Data Types from Input 
    &lt;div id=&#34;modeling-different-data-types-from-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#modeling-different-data-types-from-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Unstructured Input 
    &lt;div id=&#34;unstructured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unstructured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]&lt;/li&gt;
&lt;li&gt;Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]&lt;/li&gt;
&lt;li&gt;Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]&lt;/li&gt;
&lt;li&gt;Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Structured Input 
    &lt;div id=&#34;structured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#structured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PLMs are not designed for structured or tabular data but for sequential text/data.&lt;/li&gt;
&lt;li&gt;Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]&lt;/li&gt;
&lt;li&gt;To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]&lt;/li&gt;
&lt;li&gt;Introduced an additional graph encoder to encode the input KG. [Li2021b]&lt;/li&gt;
&lt;li&gt;Template based method to serialize input table into text sequence. [Gong2020]
&lt;ul&gt;
&lt;li&gt;For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]&lt;/li&gt;
&lt;li&gt;The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].&lt;/li&gt;
&lt;li&gt;Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Multimedia Input 
    &lt;div id=&#34;multimedia-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multimedia-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]&lt;/li&gt;
&lt;li&gt;Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]&lt;/li&gt;
&lt;li&gt;Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]&lt;/li&gt;
&lt;li&gt;Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020&lt;/li&gt;
&lt;li&gt;Image, video, speech recognition is hungry for human-transcripted supervised data.&lt;/li&gt;
&lt;li&gt;Integrate PLMs for weakly-supervised learning. For example,
&lt;ul&gt;
&lt;li&gt;Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Satisfying Special Properties for Output Text 
    &lt;div id=&#34;satisfying-special-properties-for-output-text&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#satisfying-special-properties-for-output-text&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.&lt;/li&gt;
&lt;li&gt;Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should
also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. - When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] - Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input
representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] - Approach for non-conditioned dialogue [Bao2020].&lt;/li&gt;
&lt;li&gt;Faithfulness. Means the content in generated text should not contradict the facts in input text.
&lt;ul&gt;
&lt;li&gt;PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.&lt;/li&gt;
&lt;li&gt;Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]&lt;/li&gt;
&lt;li&gt;With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.&lt;/li&gt;
&lt;li&gt;Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]&lt;/li&gt;
&lt;li&gt;Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.
&lt;ul&gt;
&lt;li&gt;When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.&lt;/li&gt;
&lt;li&gt;Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]
&lt;ul&gt;
&lt;li&gt;Extracted the word-pair alignment information from the source and target language,&lt;/li&gt;
&lt;li&gt;Aplied the extracted alignment information to enhance order-preserving.&lt;/li&gt;
&lt;li&gt;Translation across multiple languages, called multilingual machine translation [Conneau2019].&lt;/li&gt;
&lt;li&gt;mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]&lt;/li&gt;
&lt;li&gt;Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Summary from Introduction 
    &lt;div id=&#34;summary-from-introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#summary-from-introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].&lt;/li&gt;
&lt;li&gt;Machine translation generates text in a different language based on the source text [Yang2020a];&lt;/li&gt;
&lt;li&gt;Summarization generates an abridged version of the source text to include salient information [Guan2020].&lt;/li&gt;
&lt;li&gt;Text generation tasks based on
&lt;ul&gt;
&lt;li&gt;Recurrent neural networks (RNN) [Li2019],&lt;/li&gt;
&lt;li&gt;Convolutional neural networks (CNN) [Gehring2017],&lt;/li&gt;
&lt;li&gt;Graph neural networks (GNN) [Li2020],&lt;/li&gt;
&lt;li&gt;Attention mechanism [Bahdanau2015].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.&lt;/li&gt;
&lt;li&gt;Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.&lt;/li&gt;
&lt;li&gt;Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice.&lt;/li&gt;
&lt;li&gt;The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve
state-of-the-art results.&lt;/li&gt;
&lt;li&gt;PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.&lt;/li&gt;
&lt;li&gt;PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020].&lt;/li&gt;
&lt;li&gt;A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Conclusion &amp;amp; Future Recommendations 
    &lt;div id=&#34;conclusion--future-recommendations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#conclusion--future-recommendations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Model Extension.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Model Tuning with VertexAI</title>
      <link>/dsblog/Model-Tuning-with-VertexAI/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Model-Tuning-with-VertexAI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6081-Model-Tuning-with-VertexAI.jpg&#34; alt=&#34;Model Tuning with VertexAI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Tuning Large Language Model with VertexAI 
    &lt;div id=&#34;tuning-large-language-model-with-vertexai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#tuning-large-language-model-with-vertexai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Why Model Tuning? 
    &lt;div id=&#34;why-model-tuning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#why-model-tuning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Tuning is required when you want the model to learn something niche or specific that deviates from general language patterns.&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Goal of Tuning 
    &lt;div id=&#34;goal-of-tuning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#goal-of-tuning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Classification 
    &lt;div id=&#34;classification&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#classification&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;p&gt;prompt: &amp;ldquo;Classify the following text into one of the following classes: [business, entertainment].&amp;rdquo;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introduction to ML Model Deployment</title>
      <link>/dsblog/Introduction-to-ML-Model-deployment/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Introduction-to-ML-Model-deployment/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6077-Introduction-to-ML-Model-deployment.jpg&#34; alt=&#34;Introduction to AI Model Deployement&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Introduction to AI Model deployment 
    &lt;div id=&#34;introduction-to-ai-model-deployment&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-ai-model-deployment&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Big Players 
    &lt;div id=&#34;big-players&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#big-players&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon
&lt;ul&gt;
&lt;li&gt;Amazon has many products and one of their product is &lt;strong&gt;AWS Cloud&lt;/strong&gt;. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt; is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud.&lt;/li&gt;
&lt;li&gt;Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving.&lt;/li&gt;
&lt;li&gt;AWS is oldest cloud service provider in the market.&lt;/li&gt;
&lt;li&gt;AWS Sagemaker was launched in Nov&#39;17.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google
&lt;ul&gt;
&lt;li&gt;Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called &lt;strong&gt;Google Cloud&lt;/strong&gt;. Under this product they sell IT infrastrcture like Amazon sells under AWS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VertexAI&lt;/strong&gt; is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud.&lt;/li&gt;
&lt;li&gt;VertexAI can be used to train AI Model,host AI model, monitor the model etc.&lt;/li&gt;
&lt;li&gt;VertexAI was launched in Jun&#39;21&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Microsoft
&lt;ul&gt;
&lt;li&gt;Like Amazon&amp;rsquo;s cloud platform which is called AWS Cloud, Microsoft&amp;rsquo;s cloud plateform is called &lt;strong&gt;Azure&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Microsoft&amp;rsquo;s AI product is called &lt;strong&gt;Azure Machine Learning&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Today (Jul&#39;23) Azure Machine Learning has has most of the capabilites than any other player&amp;rsquo;s AI product.&lt;/li&gt;
&lt;li&gt;Azure Machine Learning was launched Feb&#39;14&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is GenAI? 
    &lt;div id=&#34;what-is-genai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-genai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights &amp;amp; biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AWS SageMaker Jumpstart Models</title>
      <link>/dsblog/AWS-SageMaker-Jumpstart-Models/</link>
      <pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/AWS-SageMaker-Jumpstart-Models/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6076-AWS-SageMaker-Jumpstart-Models.jpg&#34; alt=&#34;AWS SageMaker Jumpstart Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AWS SageMaker Jumpstart Models 
    &lt;div id=&#34;aws-sagemaker-jumpstart-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#aws-sagemaker-jumpstart-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;As of 17-Jul-23, AWS Sagemaker has 463 models in its Model Zoo. They call these models as Jumstart Models. What are the capabilities of these models, who are the developer of these models, where these models are hosted in given in the table below.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;SNo.&lt;/th&gt;
          &lt;th&gt;Task Type&lt;/th&gt;
          &lt;th&gt;Company&lt;/th&gt;
          &lt;th&gt;Model Description&lt;/th&gt;
          &lt;th&gt;Model ID&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1.&lt;/td&gt;
          &lt;td&gt;Text Generation&lt;/td&gt;
          &lt;td&gt;Huggingface&lt;/td&gt;
          &lt;td&gt;Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize It is ready-to-use chat/instruct model based on Falcon 40B&lt;/td&gt;
          &lt;td&gt;Model draft: false&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;id: huggingface-textgeneration-falcon-40b-instruct-bf16
|2. |Text Generation |Huggingface |This is a Text Generation model built upon a Transformer model from Hugging Face |Model draft: false
id: huggingface-textgeneration-open-llama
|3. |Text to Image |StabilityAI |Extend beyond just text-to-image prompting. Stable Diffusion XL offers several ways to modify the images: Inpainting - edit inside the image, Outpainting - extend the image outside of the original image, Image-to-image - prompt a new image using a sourced image. |
|4. |Text Generation |Cohere |Generative model that responds well with instruction-like prompts. This model provides businesses and enterprises with best quality, performance and accuracy in all generative tasks. And with our intuitive SDK, unlocking the full potential of LLMs for your applications has never been easier. |
|5. |Text Generation |AI21 Labs |Jurassic-2 Ultra is optimized to follow natural language instructions and context, so there is no need to provide it with any examples. |
|6. |Text Generation |AI21 Labs | |
|7. |Text Generation |AI21 Labs |Condense lengthy texts into short, easy-to-read bites that remain factually consistent with the source. No prompting needed – simply input the text that needs to be summarized. The model is specifically trained to generate summaries that capture the essence and key ideas of the original text. |
|8. |Text Generation |AI21 Labs |Get the AI21 Paraphrase model, the top-of-the-line paraphrasing engine, and deploy it in your private environment. The model aims to generate 10 alternative suggestions with every activation. It may return fewer suggestions when rewriting very short texts for which it cannot produce as many as 10 sensible paraphrases. |
|9. |Text Generation |AI21 Labs |Jurassic-2 Mid is optimized to follow natural language instructions and context, so there is no need to provide it with any examples. Pre-trained language model trained by AI21 Labs on a corpus of web text including natural language and computer programs with recent data - updated to mid 2022. This model has a 8192 token context window (i.e. the length of the prompt + completion should be at most 8192 tokens). |
|10. |Text Generation |AI21 Labs |Detects and suggests corrections for Grammar, Spelling, Punctuation mistakes, as well as word misuse, and accidental repetition or omission. |
|11. |Text to Image |StabilityAI |Extend beyond just text-to-image prompting. Stable Diffusion XL offers several ways to modify the images: Inpainting - edit inside the image, Outpainting - extend the image outside of the original image, Image-to-image - prompt a new image using a sourced image. |
|12. |Text to Image |Stabilityai |This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description |Model draft: false
id: model-txt2img-stabilityai-stable-diffusion-v2-1-base
|13. | |Huggingface |This is a Text2Text Generation model built upon a T5 model from Hugging Face The deployed model can be used for running inference on any input text |Model draft: false
id: huggingface-text2text-flan-t5-xl
|14. | |Huggingface |This is a Text Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts next words in the sequence |Model draft: false
id: huggingface-textgeneration1-gpt-j-6b
|15. | |Huggingface |This is a Text2Text Generation model built upon a T5 model from Hugging Face The deployed model can be used for running inference on any input text |Model draft: false
id: huggingface-text2text-flan-ul2-bf16
|16. |Text Generation |Pytorch |AlexaTM 20B is a multitask, multilingual, large-scale sequence-to-sequence (seq2seq) model, trained on a mixture of Common Crawl (mC4) and Wikipedia data across 12 languages, using denoising and Causal Language Modeling (CLM) tasks |Model draft: false
id: pytorch-textgeneration1-alexa20b
|17. |Text Generation |Huggingface |This is a Text Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts next words in the sequence |Model draft: false
id: huggingface-textgeneration-bloom-1b7
|18. |Image Classification |Tensorflow |This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-100-224-classification-4
|19. |Object Detection |Tensorflow |This is an object detection model from Tensorflow It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: tensorflow-od1-ssd-resnet50-v1-fpn-640x640-coco17-tpu-8
|20. |Object Detection |Pytorch |This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: pytorch-od1-fasterrcnn-resnet50-fpn
|21. |Text Classification |Tensorflow |This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review |Model draft: false
id: tensorflow-tc-bert-en-uncased-L-12-H-768-A-12-2
|22. |Question Answering |Huggingface |This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question |Model draft: false
id: huggingface-eqa-distilbert-base-uncased
|23. |Zero-Shot Text Classification |Huggingface |This is Zero Shot Text Classification model built on a Transformer model from Hugging Face It can classify sentences in English language It takes a sequence and a list of candidate labels as inputs and predicts score that the sequence is associated with the particular label |Model draft: false
id: huggingface-zstc-facebook-bart-large-mnli
|24. |Semantic Segmentation |Mxnet |This is an Semantic Segmentation model from Gluon CV It takes an image as input and returns class label for each pixel in the image |Model draft: false
id: mxnet-semseg-fcn-resnet101-coco
|25. |Sentence Pair Classification |Huggingface |This is a Sentence Pair Classification model built upon a Text Embedding model from Hugging Face It takes a pair of sentences as input and classifies the input pair to &amp;rsquo;entailment&amp;rsquo; or &amp;rsquo;no-entailment&amp;rsquo; |Model draft: false
id: huggingface-spc-distilbert-base-uncased
|26. |Named Entity Recognition |Huggingface |This is a Named Entity Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts named entities in the input text |Model draft: false
id: huggingface-ner-distilbert-base-cased-finetuned-conll03-english
|27. |Text Summarization |Huggingface |This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text |Model draft: false
id: huggingface-summarization-distilbart-xsum-1-1
|28. |Machine Translation |Huggingface |This is a Machine Translation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts its translation |Model draft: false
id: huggingface-translation-t5-small
|29. |Text Embedding |Tensorflow |This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-2-H-128-A-2-2
|30. |Text Embedding |Mxnet |This is a Text Embedding model from GluonNLP pre-trained on the decade (2010-2019) of S&amp;amp;P 500 10-K/10-Q reports It takes a text string as input and outputs an embedding vector For pre-training, the entire text of the 10K/Q filing was used, not just the MD&amp;amp;A (Management Discussion and Analysis) section, so as to ensure that a broader context of financial language is captured Embeddings from the pre-trained modelare then used for fine-tuning specific classifiers |Model draft: false
id: mxnet-tcembedding-robertafin-base-uncased
|31. |Sentence Pair Classification |Tensorflow |This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to &amp;rsquo;entailment&amp;rsquo; or &amp;rsquo;no-entailment&amp;rsquo; |Model draft: false
id: tensorflow-spc-bert-en-uncased-L-12-H-768-A-12-2
|32. |Instance Segmentation |Mxnet |This is an Instance Segmentation model from Gluon CV It detects and delineates each distinct object in the image |Model draft: false
id: mxnet-is-mask-rcnn-fpn-resnet101-v1d-coco
|33. |Image Embedding |Tensorflow |This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-100-224-featurevector-4
|34. |Image Classification |Pytorch |This is an Image Classification model from PyTorch Hub It takes an image as input and classifies the image to one of the 1000 classes |Model draft: false
id: pytorch-ic-mobilenet-v2
|35. |Object Detection |Mxnet |This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: mxnet-od-ssd-512-mobilenet1-0-coco
|36. |Object Detection |Tensorflow |This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: tensorflow-od-ssd-mobilenet-v2-fpnlite-320x320-1
|37. |Object Detection |Pytorch |This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: pytorch-od-nvidia-ssd
|38. |Image Classification |Tensorflow |This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-075-224-classification-4
|39. |Image Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-050-224-classification-4
|40. |Image Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-035-224-classification-4
|41. |Image Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-140-224-classification-4
|42. |Image Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v2-130-224-classification-4
|43. |Object Detection |Pytorch |This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: pytorch-od1-fasterrcnn-mobilenet-v3-large-320-fpn
|44. |Object Detection |Pytorch |Same |Model draft: false
id: pytorch-od1-fasterrcnn-mobilenet-v3-large-fpn
|45. |Semantic Segmentation |Mxnet |This is an Semantic Segmentation model from Gluon CV It takes an image as input and returns class label for each pixel in the image |Model draft: false
id: mxnet-semseg-fcn-resnet101-voc
|46. |Semantic Segmentation |Mxnet |Same as above |Model draft: false
id: mxnet-semseg-fcn-resnet101-ade
|47. |Instance Segmentation |Mxnet |Same as above |Model draft: false
id: mxnet-semseg-fcn-resnet50-ade
|48. |Image Classification |Pytorch |This is an Image Classification model from PyTorch Hub It takes an image as input and classifies the image to one of the 1000 classes |Model draft: false
id: pytorch-ic-resnet18
|49. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnet34
|50. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnet50
|51. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnet101
|52. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnet152
|53. |Object Detection |Mxnet |This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: mxnet-od-ssd-512-mobilenet1-0-voc
|54. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-ssd-512-resnet50-v1-coco
|55. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-ssd-512-resnet50-v1-voc
|56. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-ssd-300-vgg16-atrous-coco
|57. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-ssd-300-vgg16-atrous-voc
|58. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-efficientdet-d0-512x512-coco17-tpu-8
|59. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-efficientdet-d1-640x640-coco17-tpu-8
|60. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-efficientdet-d2-768x768-coco17-tpu-8
|61. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-efficientdet-d3-896x896-coco17-tpu-32
|62. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-mobilenet-v1-fpn-640x640-coco17-tpu-8
|63. |Instance Segmentation |Mxnet |This is an Instance Segmentation model from Gluon CV It detects and delineates each distinct object in the image |Model draft: false
id: mxnet-is-mask-rcnn-fpn-resnet50-v1b-coco
|64. |Instance Segmentation |Mxnet |Same |Model draft: false
id: mxnet-is-mask-rcnn-fpn-resnet18-v1b-coco
|65. | |Mxnet |Same |Model draft: false
id: mxnet-is-mask-rcnn-resnet18-v1b-coco
|66. |Image Embedding |Tensorflow |This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-075-224-featurevector-4
|67. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-050-224-featurevector-4
|68. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-035-224-featurevector-4
|69. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-140-224-featurevector-4
|70. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v2-130-224-featurevector-4
|71. |Object Detection |Tensorflow |This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: tensorflow-od-ssd-mobilenet-v2-fpnlite-640x640-1
|72. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-ssd-mobilenet-v2-2
|73. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-ssd-mobilenet-v1-fpn-640x640-1
|74. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet50-v1-640x640-1
|75. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet50-v1-800x1333-1
|76. |Zero-Shot Text Classification |Huggingface |This is Zero Shot Text Classification model built on a Transformer model from Hugging Face It can classify sentences in English language It takes a sequence and a list of candidate labels as inputs and predicts score that the sequence is associated with the particular label |Model draft: false
id: huggingface-zstc-narsil-deberta-large-mnli-zero-cls
|77. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-moritzlaurer-deberta-v3-large-mnli-fever-anli-ling-wanli
|78. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-cross-encoder-nli-distilroberta-base
|79. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-recognai-bert-base-spanish-wwm-cased-xnli
|80. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-moritzlaurer-mdeberta-v3-base-xnli-multilingual-nli-2mil7
|81. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-cross-encoder-nli-roberta-base
|82. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-cross-encoder-nli-deberta-base
|83. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-cross-encoder-nli-minilm2-l6-h768
|84. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-recognai-zeroshot-selectra-medium
|85. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-navteca-bart-large-mnli
|86. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-jiva-xlm-roberta-large-it-mnli
|87. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-digitalepidemiologylab-covid-twitter-bert-v2-mnli
|88. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-recognai-zeroshot-selectra-small
|89. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-distilbert-base-turkish-cased-snli-tr
|90. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-turkish-cased-allnli-tr
|91. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-turkish-cased-snli-tr
|92. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-multilingual-cased-allnli-tr
|93. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-narsil-bart-large-mnli-opti
|94. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-allnli-tr
|95. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-lighteternal-nli-xlm-r-greek
|96. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-distilbert-base-turkish-cased-allnli-tr
|97. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-multilingual-cased-multinli-tr
|98. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-eleldar-theme-classification
|99. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-turkish-cased-multinli-tr
|100. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-bert-base-multilingual-cased-snli-tr
|101. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-multinli-tr
|102. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-distilbert-base-turkish-cased-multinli-tr
|103. |Zero-Shot Text Classification |Huggingface |Same |Model draft: false
id: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-snli-tr
|104. |Image Classification |Tensorflow |This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes |Model draft: false
id: tensorflow-ic-tf2-preview-mobilenet-v2-classification-4
|105. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-inception-v3-classification-4
|106. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-inception-v2-classification-4
|107. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-inception-v1-classification-4
|108. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-tf2-preview-inception-v3-classification-4
|109. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-inception-resnet-v2-classification-4
|110. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v2-50-classification-4
|111. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v2-101-classification-4
|112. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v2-152-classification-4
|113. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v1-50-classification-4
|114. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v1-101-classification-4
|115. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-resnet-v1-152-classification-4
|116. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-resnet-50-classification-1
|117. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b0-classification-1
|118. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b1-classification-1
|119. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b2-classification-1
|120. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b3-classification-1
|121. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b4-classification-1
|122. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b5-classification-1
|123. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b6-classification-1
|124. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-b7-classification-1
|125. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-lite0-classification-2
|126. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-lite1-classification-2
|127. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-lite2-classification-2
|128. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-lite3-classification-2
|129. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-efficientnet-lite4-classification-2
|130. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-100-224-classification-4
|131. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-100-192-classification-4
|132. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-100-160-classification-4
|133. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-100-128-classification-4
|134. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4
|135. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-075-192-classification-4
|136. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-075-160-classification-4
|137. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-075-128-classification-4
|138. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-050-224-classification-4
|139. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-050-192-classification-4
|140. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-050-160-classification-4
|141. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-050-128-classification-4
|142. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-025-224-classification-4
|143. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-025-192-classification-4
|144. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-025-160-classification-4
|145. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-imagenet-mobilenet-v1-025-128-classification-4
|146. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-s-r50x1-ilsvrc2012-classification-1
|147. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-s-r50x3-ilsvrc2012-classification-1
|148. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-s-r101x1-ilsvrc2012-classification-1
|149. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-s-r101x3-ilsvrc2012-classification-1
|150. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r50x1-ilsvrc2012-classification-1
|151. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r50x3-ilsvrc2012-classification-1
|152. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r101x1-ilsvrc2012-classification-1
|153. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r101x3-ilsvrc2012-classification-1
|154. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r50x1-imagenet21k-classification-1
|155. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r50x3-imagenet21k-classification-1
|156. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r101x1-imagenet21k-classification-1
|157. |Image Classification |Tensorflow |Same |Model draft: false
id: tensorflow-ic-bit-m-r101x3-imagenet21k-classification-1
|158. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-alexnet
|159. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-densenet121
|160. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-densenet169
|161. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-densenet201
|162. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-densenet161
|163. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnext50-32x4d
|164. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-resnext101-32x8d
|165. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-shufflenet-v2-x1-0
|166. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-squeezenet1-0
|167. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-squeezenet1-1
|168. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg11
|169. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg11-bn
|170. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg13
|171. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg13-bn
|172. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg16
|173. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg16-bn
|174. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg19
|175. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-vgg19-bn
|176. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-wide-resnet50-2
|177. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-wide-resnet101-2
|178. |Image Classification |Pytorch |Same |Model draft: false
id: pytorch-ic-googlenet
|179. |Object Detection |Mxnet |This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: mxnet-od-ssd-512-vgg16-atrous-coco
|180. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-ssd-512-vgg16-atrous-voc
|181. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-yolo3-darknet53-voc
|182. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-yolo3-mobilenet1-0-voc
|183. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-yolo3-darknet53-coco
|184. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-yolo3-mobilenet1-0-coco
|185. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-faster-rcnn-resnet50-v1b-voc
|186. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-faster-rcnn-resnet50-v1b-coco
|187. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-faster-rcnn-resnet101-v1d-coco
|188. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-faster-rcnn-fpn-resnet50-v1b-coco
|189. |Object Detection |Mxnet |Same |Model draft: false
id: mxnet-od-faster-rcnn-fpn-resnet101-v1d-coco
|190. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-mobilenet-v2-fpnlite-320x320-coco17-tpu-8
|191. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-mobilenet-v2-fpnlite-640x640-coco17-tpu-8
|192. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-resnet50-v1-fpn-1024x1024-coco17-tpu-8
|193. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-resnet101-v1-fpn-640x640-coco17-tpu-8
|194. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-resnet101-v1-fpn-1024x1024-coco17-tpu-8
|195. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-resnet152-v1-fpn-640x640-coco17-tpu-8
|196. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od1-ssd-resnet152-v1-fpn-1024x1024-coco17-tpu-8
|197. |Image Embedding |Tensorflow |This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image |Model draft: false
id: tensorflow-icembedding-imagenet-inception-v3-featurevector-4
|198. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-inception-v2-featurevector-4
|199. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-inception-v1-featurevector-4
|200. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-tf2-preview-inception-v3-featurevector-4
|201. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-tf2-preview-mobilenet-v2-featurevector-4
|202. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v2-50-featurevector-4
|203. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v2-101-featurevector-4
|204. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v2-152-featurevector-4
|205. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v1-50-featurevector-4
|206. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v1-101-featurevector-4
|207. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-resnet-v1-152-featurevector-4
|208. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-resnet-50-featurevector-1
|209. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-b0-featurevector-1
|210. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-b1-featurevector-1
|211. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-b2-featurevector-1
|212. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-b3-featurevector-1
|213. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-b6-featurevector-1
|214. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-lite0-featurevector-2
|215. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-lite1-featurevector-2
|216. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-lite2-featurevector-2
|217. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-lite3-featurevector-2
|218. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-efficientnet-lite4-featurevector-2
|219. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-100-224-featurevector-4
|220. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-100-192-featurevector-4
|221. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-100-160-featurevector-4
|222. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-100-128-featurevector-4
|223. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-075-224-featurevector-4
|224. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-075-192-featurevector-4
|225. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-075-160-featurevector-4
|226. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-075-128-featurevector-4
|227. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-050-224-featurevector-4
|228. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-050-192-featurevector-4
|229. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-050-160-featurevector-4
|230. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-050-128-featurevector-4
|231. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-025-224-featurevector-4
|232. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-025-192-featurevector-4
|233. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-025-160-featurevector-4
|234. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-imagenet-mobilenet-v1-025-128-featurevector-4
|235. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-s-r50x1-ilsvrc2012-featurevector-1
|236. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-s-r50x3-ilsvrc2012-featurevector-1
|237. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-s-r101x1-ilsvrc2012-featurevector-1
|238. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-s-r101x3-ilsvrc2012-featurevector-1
|239. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-m-r50x1-ilsvrc2012-featurevector-1
|240. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-m-r50x3-imagenet21k-featurevector-1
|241. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-m-r101x1-ilsvrc2012-featurevector-1
|242. |Image Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-icembedding-bit-m-r101x3-imagenet21k-featurevector-1
|243. |Object Detection |Tensorflow |This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image |Model draft: false
id: tensorflow-od-faster-rcnn-resnet50-v1-1024x1024-1
|244. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet101-v1-640x640-1
|245. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet101-v1-800x1333-1
|246. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet101-v1-1024x1024-1
|247. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet152-v1-640x640-1
|248. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet152-v1-800x1333-1
|249. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-resnet152-v1-1024x1024-1
|250. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-inception-resnet-v2-640x640-1
|251. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-faster-rcnn-inception-resnet-v2-1024x1024-1
|252. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d0-1
|253. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d1-1
|254. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d2-1
|255. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d3-1
|256. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d4-1
|257. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-efficientdet-d5-1
|258. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet50-v1-fpn-640x640-1
|259. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet50-v1-fpn-1024x1024-1
|260. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet101-v1-fpn-640x640-1
|261. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet101-v1-fpn-1024x1024-1
|262. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet152-v1-fpn-640x640-1
|263. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-retinanet-resnet152-v1-fpn-1024x1024-1
|264. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-hourglass-512x512-1
|265. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-hourglass-512x512-kpts-1
|266. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-hourglass-1024x1024-1
|267. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-hourglass-1024x1024-kpts-1
|268. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-resnet50v1-fpn-512x512-1
|269. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-resnet50v1-fpn-512x512-kpts-1
|270. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-resnet50v2-512x512-1
|271. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-resnet50v2-512x512-kpts-1
|272. |Object Detection |Tensorflow |Same |Model draft: false
id: tensorflow-od-centernet-resnet101v1-fpn-512x512-1
|273. |Text Classification |Tensorflow |This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review |Model draft: false
id: tensorflow-tc-bert-en-cased-L-12-H-768-A-12-2
|274. |Text Classification |Tensorflow |Same |Model draft: false
id: tensorflow-tc-bert-multi-cased-L-12-H-768-A-12-2
|275. |Text Classification |Tensorflow |Same |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-128-A-2
|276. |Text Classification |Tensorflow |Same |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-256-A-4
|277. |Text Classification |Tensorflow |Same |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-512-A-8
|278. |Question Answering |Huggingface |This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question |Model draft: false
id: huggingface-eqa-distilbert-base-cased
|279. |Question Answering |Huggingface |Same |Model draft: false
id: huggingface-eqa-distilbert-base-multilingual-cased
|280. |Question Answering |Huggingface |Same |Model draft: false
id: huggingface-eqa-bert-base-uncased
|281. |Question Answering |Huggingface |Same |Model draft: false
id: huggingface-eqa-bert-base-cased
|282. |Question Answering |Huggingface |Same |Model draft: false
id: huggingface-eqa-bert-base-multilingual-uncased
|283. |Sentence Pair Classification |Tensorflow |This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to &amp;rsquo;entailment&amp;rsquo; or &amp;rsquo;no-entailment&amp;rsquo; |Model draft: false
id: tensorflow-spc-bert-en-cased-L-12-H-768-A-12-2
|284. |Sentence Pair Classification |Tensorflow |Same |Model draft: false
id: tensorflow-spc-bert-multi-cased-L-12-H-768-A-12-2
|285. |Sentence Pair Classification |Tensorflow |Same |Model draft: false
id: tensorflow-spc-bert-en-uncased-L-24-H-1024-A-16-2
|286. |Sentence Pair Classification |Tensorflow |Same |Model draft: false
id: tensorflow-spc-electra-small-1
|287. |Sentence Pair Classification |Tensorflow |Same |Model draft: false
id: tensorflow-spc-electra-base-1
|288. |Sentence Pair Classification |Huggingface |Same |Model draft: false
id: huggingface-spc-distilbert-base-cased
|289. |Sentence Pair Classification |Huggingface |Same |Model draft: false
id: huggingface-spc-distilbert-base-multilingual-cased
|290. |Sentence Pair Classification |Huggingface |Same |Model draft: false
id: huggingface-spc-bert-base-uncased
|291. |Sentence Pair Classification |Huggingface |Same |Model draft: false
id: huggingface-spc-bert-base-cased
|292. |Sentence Pair Classification |Huggingface |Same |Model draft: false
id: huggingface-spc-bert-base-multilingual-uncased
|293. |Named Entity Recognition |Huggingface |This is a Named Entity Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts named entities in the input text |Model draft: false
id: huggingface-ner-distilbert-base-uncased-finetuned-conll03-english
|294. |Text Generation |Huggingface |Same |Model draft: false
id: huggingface-textgeneration-bloom-1b1
|295. |Text Generation |Huggingface |Same |Model draft: false
id: huggingface-textgeneration-bloom-560m
|296. |Text Generation |Huggingface |Same |Model draft: false
id: huggingface-textgeneration-gpt2
|297. |Text Generation |Huggingface |Same |Model draft: false
id: huggingface-textgeneration-distilgpt2
|298. |Text Summarization |Huggingface |This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text |Model draft: false
id: huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization
|299. |Text Summarization |Huggingface |Same |Model draft: false
id: huggingface-summarization-distilbart-cnn-6-6
|300. |Text Summarization |Huggingface |Same |Model draft: false
id: huggingface-summarization-distilbart-xsum-12-3
|301. |Text Summarization |Huggingface |Same |Model draft: false
id: huggingface-summarization-distilbart-cnn-12-6
|302. |Text Summarization |Huggingface |Same |Model draft: false
id: huggingface-summarization-bart-large-cnn-samsum
|303. |Machine Translation |Huggingface |This is a Machine Translation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts its translation |Model draft: false
id: huggingface-translation-t5-base
|304. |Machine Translation |Huggingface |Same |Model draft: false
id: huggingface-translation-t5-large
|305. |Machine Translation |Huggingface |Same |Model draft: false
id: huggingface-translation-opus-mt-en-es
|306. |Machine Translation |Huggingface |Same |Model draft: false
id: huggingface-translation-opus-mt-en-vi
|307. |Text Embedding |Tensorflow |This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-2-H-256-A-4
|308. |Text Embedding |Tensorflow |same |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-2-H-512-A-8-2
|309. |Text Embedding |Tensorflow |same |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-2-H-768-A-12-2
|310. |Text to Image |Stabilityai |This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description |Model draft: false
id: model-txt2img-stabilityai-stable-diffusion-v2
|311. |Text Embedding |Tensorflow |This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-4-H-128-A-2-2
|312. |Text Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-4-H-256-A-4-2
|313. |Text Embedding |Mxnet |Same |Model draft: false
id: mxnet-tcembedding-robertafin-base-wiki-uncased
|314. |Text Embedding |Mxnet |Same |Model draft: false
id: mxnet-tcembedding-robertafin-large-uncased
|315. |Text Embedding |Mxnet |Same |Model draft: false
id: mxnet-tcembedding-robertafin-large-wiki-uncased
|316. |Text Classification |Tensorflow |This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-768-A-12
|317. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-128-A-2
|318. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-256-A-4
|319. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-512-A-8
|320. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-768-A-12
|321. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-128-A-2
|322. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-256-A-4
|323. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-512-A-8
|324. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-768-A-12
|325. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-128-A-2
|326. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-256-A-4
|327. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-512-A-8
|328. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-768-A-12
|329. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-128-A-2
|330. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-256-A-4
|331. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-512-A-8
|332. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-768-A-12
|333. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-128-A-2
|334. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-256-A-4
|335. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-512-A-8
|336. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-768-A-12
|337. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-bert-en-uncased-L-24-H-1024-A-16-2
|338. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-bert-en-cased-L-24-H-1024-A-16-2
|339. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-bert-en-wwm-uncased-L-24-H-1024-A-16-2
|340. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-bert-en-wwm-cased-L-24-H-1024-A-16-2
|341. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-albert-en-base
|342. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-electra-small-1
|343. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-electra-base-1
|344. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-experts-bert-wiki-books-1
|345. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-experts-bert-pubmed-1
|346. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-talking-heads-base
|347. |Text Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-tc-talking-heads-large
|348. |Question Answering |Huggingface |This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question |Model draft: false
id: huggingface-eqa-bert-base-multilingual-cased
|349. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-bert-large-uncased
|350. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-bert-large-cased
|351. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-bert-large-uncased-whole-word-masking
|352. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-bert-large-cased-whole-word-masking
|353. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-distilroberta-base
|354. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-roberta-base
|355. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-roberta-base-openai-detector
|356. |Question Answering |Huggingface |Same as above |Model draft: false
id: huggingface-eqa-roberta-large
|357. |Sentence Pair Classification |Tensorflow |This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to &amp;rsquo;entailment&amp;rsquo; or &amp;rsquo;no-entailment&amp;rsquo; |Model draft: false
id: tensorflow-spc-bert-en-wwm-uncased-L-24-H-1024-A-16-2
|358. |Sentence Pair Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-spc-bert-en-wwm-cased-L-24-H-1024-A-16-2
|359. |Sentence Pair Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-spc-experts-bert-wiki-books-1
|360. |Sentence Pair Classification |Tensorflow |Same as above |Model draft: false
id: tensorflow-spc-experts-bert-pubmed-1
|361. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-bert-base-multilingual-cased
|362. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-bert-large-uncased
|363. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-bert-large-cased
|364. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-bert-large-uncased-whole-word-masking
|365. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-bert-large-cased-whole-word-masking
|366. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-distilroberta-base
|367. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-roberta-base
|368. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-roberta-base-openai-detector
|369. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-roberta-large
|370. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-roberta-large-openai-detector
|371. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-xlm-mlm-ende-1024
|372. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-xlm-mlm-enro-1024
|373. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-xlm-mlm-xnli15-1024
|374. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-xlm-mlm-tlm-xnli15-1024
|375. |Sentence Pair Classification |Huggingface |Same as above |Model draft: false
id: huggingface-spc-xlm-clm-ende-1024
|376. |Text Summarization |Huggingface |This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text |Model draft: false
id: huggingface-summarization-bigbird-pegasus-large-arxiv
|377. |Text Summarization |Huggingface |Same as above |Model draft: false
id: huggingface-summarization-bigbird-pegasus-large-pubmed
|378. |Text Embedding |Tensorflow |This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-4-H-512-A-8-2
|379. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-4-H-768-A-12-2
|380. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-6-H-128-A-2-2
|381. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-6-H-256-A-4
|382. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-6-H-512-A-8-2
|383. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-6-H-768-A-12-2
|384. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-8-H-256-A-4-2
|385. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-8-H-512-A-8-2
|386. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-8-H-768-A-12-2
|387. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-10-H-128-A-2-2
|388. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-10-H-256-A-4-2
|389. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-10-H-512-A-8-2
|390. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-10-H-768-A-12-2
|391. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-12-H-128-A-2-2
|392. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-12-H-256-A-4
|393. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-12-H-512-A-8-2
|394. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-12-H-768-A-12-2
|395. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-en-uncased-L-12-H-768-A-12-4
|396. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-wiki-books-sst2
|397. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-bert-wiki-books-mnli-2
|398. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1
|399. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-base-1
|400. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-talkheads-ggelu-bert-en-base-2
|401. |Text Embedding |Tensorflow |Same as above |Model draft: false
id: tensorflow-tcembedding-talkheads-ggelu-bert-en-large-2
|402. |Tabular Classification | |This is the LightGBM algorithm for tabular classification task LightGBM is a gradient boosting framework that uses tree based learning algorithms |Model draft: false
id: lightgbm-classification-model
|403. |Tabular Classification |Catboost |This is the CatBoost algorithm for tabular classification task CatBoost is a machine learning algorithm that uses gradient boosting on decision trees |Model draft: false
id: catboost-classification-model
|404. |Tabular Classification | |This is the AutoGluon-Tabular algorithm for tabular classification task AutoGluon-Tabular is an open-source AutoML framework that trains highly accurate machine learning models on an unprocessed tabular dataset Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers |Model draft: false
id: autogluon-classification-ensemble
|405. |Tabular Classification | |This is the TabTransformer algorithm for tabular classification task TabTransformer is a deep tabular data modeling architecture that built upon self-attention based Transformers |Model draft: false
id: pytorch-tabtransformerclassification-model
|406. |Tabular Classification |Sklearn |This is the scikit-learn linear algorithm for tabular classification task Linear Classification is a linear approach to classify data into labels (targets) based on a linear combination of its input features (predictors) |Model draft: false
id: sklearn-classification-linear
|407. |Tabular Classification |Xgboost |This is the XGBoost algorithm for tabular classification task XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable It implements machine learning algorithms under the Gradient Boosting framework |Model draft: false
id: xgboost-classification-model
|408. |Tabular Regression | |This is the LightGBM algorithm for tabular regression task LightGBM is a gradient boosting framework that uses tree based learning algorithms |Model draft: false
id: lightgbm-regression-model
|409. |Tabular Regression |Catboost |This is the CatBoost algorithm for tabular regression task CatBoost is a machine learning algorithm that uses gradient boosting on decision trees |Model draft: false
id: catboost-regression-model
|410. |Tabular Regression | |This is the AutoGluon-Tabular algorithm for tabular regression task AutoGluon-Tabular is an open-source AutoML framework that trains highly accurate machine learning models on an unprocessed tabular dataset Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers |Model draft: false
id: autogluon-regression-ensemble
|411. |Tabular Regression | |This is the TabTransformer algorithm for tabular regression task TabTransformer is a deep tabular data modeling architecture that built upon self-attention based Transformers |Model draft: false
id: pytorch-tabtransformerregression-model
|412. |Tabular Regression |Sklearn |This is the scikit-learn linear algorithm for tabular regression task Linear Regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables |Model draft: false
id: sklearn-regression-linear
|413. |Tabular Regression |Xgboost |This is the XGBoost algorithm for tabular regression task XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable It implements machine learning algorithms under the Gradient Boosting framework |Model draft: false
id: xgboost-regression-model
|414. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-distilbert-base-uncased
|415. |Question Answering |Pytorch |Same as above |Model draft: false
id: pytorch-eqa-bert-large-uncased-whole-word-masking
|416. |Question Answering |Pytorch |Same as above |Model draft: false
id: pytorch-eqa-bert-large-uncased
|417. |Question Answering |Pytorch |Same as above |Model draft: false
id: pytorch-eqa-bert-large-cased
|418. |Question Answering |Pytorch |Same as above |Model draft: false
id: pytorch-eqa-roberta-base
|419. |Question Answering |Pytorch |Same as above |Model draft: false
id: pytorch-eqa-distilbert-base-multilingual-cased
|420. |Object detection |SageMaker |Identify birds species in a scene using a SageMaker object detection model. |
|421. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-distilroberta-base
|422. |Audio Embedding |Tensorflow |This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector |Model draft: false
id: tensorflow-audioembedding-trill-distilled-3
|423. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-roberta-large-openai-detector
|424. |Object detection |SageMaker |Identify defective regions in product images either by training an object detection model from scratch or fine-tuning pretrained SageMaker models. |
|425. |Audio Embedding |Tensorflow |This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector |Model draft: false
id: tensorflow-audioembedding-trillsson2-1
|426. |Tabular classification |SageMaker |Automatically detect potentially fraudulent activity in transactions using SageMaker XGBoost with the over-sampling technique Synthetic Minority Over-sampling (SMOTE). |
|427. |Feature importance using shap |SageMaker | |
|428. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-distilbert-base-cased
|429. |Graph neural network classification |SageMaker |Detect fraud in financial transactions by training a graph convolutional network with the deep graph library and a SageMaker XGBoost model. |
|430. |Tabular classification |SageMaker |Classify financial payments based on transaction information using SageMaker XGBoost. Use this solution template as an intermediate step in fraud detection, personalization, or anomaly detection. |
|431. |Tabular classification |SageMaker |Identify unhappy mobile phone customers using SageMaker XGBoost. |
|432. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-bert-base-cased
|433. |RL |SageMaker |Distributed reinforcement learning starter kit for NeurIPS 2020 Procgen Reinforcement learning challenge. |
|434. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-bert-large-cased-whole-word-masking-finetuned-squad
|435. |Tabular classification |SageMaker | |
|436. |RL |SageMaker | |
|437. |Entity resolution |SageMaker | |
|438. |Tabular classification |SageMaker | |
|439. |Tabular and text classification |SageMaker | |
|440. |Text classification |SageMaker |Anonymize text to better preserve user privacy in sentiment classification. |
|441. |Tabular, image, and text classification. |SageMaker | |
|442. |Tabular classification |SageMaker | |
|443. |Text to Image |Stabilityai |This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description |Model draft: false
id: model-txt2img-stabilityai-stable-diffusion-v2-fp16
|444. |Text to Image |Stabilityai |Same |Model draft: false
id: model-txt2img-stabilityai-stable-diffusion-v1-4-fp16
|445. |ext to Image |Stabilityai |Same |Model draft: false
id: model-txt2img-stabilityai-stable-diffusion-v1-4
|446. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-bert-base-multilingual-cased
|447. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-roberta-large
|448. |Audio Embedding |Tensorflow |This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector |Model draft: false
id: tensorflow-audioembedding-frill-1
|449. |Audio Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-audioembedding-trillsson3-1
|450. |Audio Embedding |Tensorflow |Same |Model draft: false
id: tensorflow-audioembedding-trill-3
|451. |Tabular and text classification |SageMaker | |
|452. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-roberta-base-openai-detector
|453. |Question Answering |Pytorch |Same |Model draft: false
id: pytorch-eqa-bert-large-cased-whole-word-masking
|454. |Time series |SageMaker |Demand forecasting for multivariate time series data using three state-of-the-art time series forecasting algorithms: LSTNet, Prophet, and SageMaker DeepAR. |
|455. |Question Answering |Pytorch |This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question |Model draft: false
id: pytorch-eqa-bert-large-uncased-whole-word-masking-finetuned-squad
|456. |Question Answering |Pytorch |Same |Model draft: false
id: pytorch-eqa-bert-base-multilingual-uncased
|457. |Question Answering |Pytorch |Same |Model draft: false
id: pytorch-eqa-bert-base-uncased
|458. |Audio Embedding |Tensorflow |This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector |Model draft: false
id: tensorflow-audioembedding-trillsson1-1
|459. |Object detection |SageMaker | |
|460. |Causal inference |SageMaker |Generate a counterfactual analysis of corn response to nitrogen. This solution learns the crop phenology cycle in its entirety using multi-spectral satellite imagery and ground-level observations. |
|461. |Price optimization |SageMaker |Estimate price elasticity using Double Machine Learning (ML) for causal inference and the Prophet forecasting procedure. Use these estimates to optimize daily prices. |
|462. |Tabular and text classification |SageMaker | |
|463. |Upscaling |Stabilityai |This is a upscaling model from Stability AI downloaded from HuggingFace with FP16 precision Given a low resolution image and a textual prompt, it generates a higher resolution image with size up to four times the original image size |Model draft: false
id: model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Embedding with FastText</title>
      <link>/dsblog/Embedding-with-FastText/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Embedding-with-FastText/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6073-Embedding-with-FastText.jpg&#34; alt=&#34;Embedding with FastText&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Embedding with FastText 
    &lt;div id=&#34;embedding-with-fasttext&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#embedding-with-fasttext&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;/dsblog/what-is-nlp#what-is-embedding&#34;&gt;What is Embedding?&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;/dsblog/what-is-nlp#what-are-different-embedding-types&#34;&gt;What are Different Types of Embedding&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is FastText? 
    &lt;div id=&#34;what-is-fasttext&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-fasttext&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;using word embeddings&lt;/strong&gt;.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Major LLM Developers Shaping the AI Landscape</title>
      <link>/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6075-Major-LLM-Developers-Reshaping-NLP-Advancements.jpg&#34; alt=&#34;Major LLM Developers Shaping the AI Landscape&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Major LLM Developers Shaping the AI Landscape 
    &lt;div id=&#34;major-llm-developers-shaping-the-ai-landscape&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#major-llm-developers-shaping-the-ai-landscape&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;From Text to Intelligence: Major LLM Developers Shaping the AI Landscape&lt;/strong&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction: 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Artificial Intelligence (AI) has experienced an exponential growth, fueled by groundbreaking research and the efforts of innovative developers. Among the key players, Large Language Model (LLM) developers have taken center stage, creating powerful language models that have revolutionized natural language processing and understanding. In this article, we delve into the major LLM developers, their key contributions.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is GAN Architecture?</title>
      <link>/dsblog/What-is-GAN-Architecture/</link>
      <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/What-is-GAN-Architecture/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6069-What-is-GAN-Architecture.jpg&#34; alt=&#34;What is GAN Architecture?&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is GAN Architecture? 
    &lt;div id=&#34;what-is-gan-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Generative Adversarial Networks (GANs) are a powerful class of neural networks that are used for unsupervised learning. It was developed and introduced by Ian J. Goodfellow in 2014. It is a type of artificial intelligence (AI) model that consists of two neural networks: a generator and a discriminator. GANs are used for generative tasks, such as creating realistic images, videos, or even audio.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>A Guide to Model Fine Tuning with OpenAI API</title>
      <link>/dsblog/Model-Fine-Tuning-with-OpenAI-API/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Model-Fine-Tuning-with-OpenAI-API/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6068-A-Guide-to-Model-Fine-Tuning-with-OpenAI-API.jpg&#34; alt=&#34;A Guide to Model Fine Tuning with OpenAI API&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;A Guide to Model Fine Tuning with OpenAI API 
    &lt;div id=&#34;a-guide-to-model-fine-tuning-with-openai-api&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#a-guide-to-model-fine-tuning-with-openai-api&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Account Setup and API Key Generation 
    &lt;div id=&#34;account-setup-and-api-key-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#account-setup-and-api-key-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Go to &lt;a href=&#34;https://platform.openai.com/&#34; target=&#34;_blank&#34;&gt;openai&lt;/a&gt;, sign up there and create your account. After than you need to create an API using &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34; target=&#34;_blank&#34;&gt;API Key Link&lt;/a&gt;. You need to copy the api key and you replace the text below &amp;lt;OPENAI_API_KEY&amp;gt;. Being string the key should be within &amp;ldquo;&amp;rdquo;. Keeping security in mind it is highly recommended that you do not put the API in the code file. Keep it at some secured place and read that file to fetch the API key. OpenAI gives you USD 5 free usage. After that you need to pay. For that you need to setup your credit card details on their system. They are very fair on the charges, just keep track of your usage. If you don&amp;rsquo;t use any of their service they won&amp;rsquo;t charge anything for just having account with them. While doing any model finetuning or prediction openai tells you how much they will charge you for that particular command. My suggestion is if you are just experimenting then keep your dataset small so that you can manage your learning with USD 10-20.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Capabilities of AI Transformers</title>
      <link>/dsblog/Capabilities-of-AI-Transformers/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Capabilities-of-AI-Transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6067-Capabilities-of-AI-Transformers.jpg&#34; alt=&#34;Capabilities of AI Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Capabilities of AI Transformers 
    &lt;div id=&#34;capabilities-of-ai-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#capabilities-of-ai-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Model Garden of VertexAI</title>
      <link>/dsblog/Model-Garden-of-VertexAI/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Model-Garden-of-VertexAI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6065-Model-Garden-of-VertexAI.jpg&#34; alt=&#34;All Resources to Learn Data Science&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Model Garden of VertexAI: 
    &lt;div id=&#34;model-garden-of-vertexai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#model-garden-of-vertexai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Unlocking the Power of Google&amp;rsquo;s VertexAI: Exploring the World of Pre-Built Models for AI Tasks 
    &lt;div id=&#34;unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction: 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Artificial Intelligence (AI) has transformed numerous industries, from healthcare and finance to e-commerce, logistic, eduction and entertainment. But the complexity of developing machine learning models often poses a challenge. As the demand for AI-powered solutions continues to rise, data scientists seek efficient ways to leverage pre-trained models or build custom models to address specific tasks. In this regard, Google&amp;rsquo;s VertexAI emerges as a robust platform that offers an extensive selection of pre-built models for a wide range of AI tasks. VertexAI platform has revolutionized the landscape by seamlessly leveraging LLM (Large Language Models) and Prompt Engineering techniques to perform complex machine learning tasks effortlessly. With VertexAI, data scientists can harness the power of state-of-the-art language models, such as LLM, to accelerate their ML development process. Additionally, the innovative concept of Prompt Engineering enables users to effectively communicate with the models, guiding them to deliver precise and accurate results. From computer vision and natural language processing to speech processing and structured tabular data analysis, Vertex AI&amp;rsquo;s repertoire includes over 100 models catering to diverse application domains. This article explores how Vertex AI, through its integration of LLM and Prompt Engineering, empowers users to effortlessly tackle intricate machine learning tasks across diverse domains, revolutionizing the AI development experience.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>God Fathers of AI</title>
      <link>/dsblog/God-Fathers-of-AI/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/God-Fathers-of-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6058-God-Fathers-of-AI.jpg&#34; alt=&#34;God Fathers of AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;God Fathers of AI 
    &lt;div id=&#34;god-fathers-of-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#god-fathers-of-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In other fields of studies or in religion, there is only one god or only one godfather. But in the field of AI, that is not the case. There are many pioneers or Godfathers who have done significant work in this field. Recently, the resignation of Dr. Geoffrey Hinton from Google raised eyebrows in the business world and in Governments the world over. Technology is good or bad, it depends upon whose hand it is. Geoffrey raised that concern and for that, he wants better controls in place. What will happen, we need to follow the progress and raise our voices around. In this article, I am mentioning some godfathers of AI, their workplaces, and their contributions. I am sure this will inspire many young minds.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Types of Machine Learning</title>
      <link>/dsblog/Types-of-Machine-Learning/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Types-of-Machine-Learning/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6056-Types-of-Machine-Learning.jpg&#34; alt=&#34;Types of Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Types of Machine Learning 
    &lt;div id=&#34;types-of-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#types-of-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introduction to Neural Network</title>
      <link>/dsblog/Introduction-to-Neural-Network/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Introduction-to-Neural-Network/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg&#34; alt=&#34;Introduction to Neural Network&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Introduction to Neural Network 
    &lt;div id=&#34;introduction-to-neural-network&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-neural-network&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction to a Perceptron 
    &lt;div id=&#34;introduction-to-a-perceptron&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-a-perceptron&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is GAN?</title>
      <link>/dsblog/What-is-GAN/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/What-is-GAN/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6043-gan.jpg&#34; alt=&#34;Partial Dependence Plots&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is GAN? 
    &lt;div id=&#34;what-is-gan&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is GAN (Generative Adversarial Network)? 
    &lt;div id=&#34;what-is-gan-generative-adversarial-network&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan-generative-adversarial-network&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Generative adversarial networks (GANs) are besing used to generate images, videos, text, audio and music. GAN is a class of machine-learning models introduced by Ian Goodfellow and his colleagues in 2014. The GANs became popular among researchers quickly because of their property to generate new data with the same statistics as the input training set. It can be applied to images, videos, textual data, tabular data and more, proving useful for semi-supervised, fully supervised, and reinforcement learning.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Linear Regression Interview Questions</title>
      <link>/dsblog/Linear-Regression-Interview-Questions/</link>
      <pubDate>Sat, 07 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Linear-Regression-Interview-Questions/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6022-Linear-Regression-Interview-Questions.jpg&#34; alt=&#34;Prompt Engineering for GPT4&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Linear Regression Interview Questions and Answers 
    &lt;div id=&#34;linear-regression-interview-questions-and-answers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#linear-regression-interview-questions-and-answers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;In this question-answer article, I will try that the start of every answer from example rather than theory (some unavoidable variation may be possible). I firmly believe if examples are clear, human mind is smart enough in generlization and creating theories.&lt;/p&gt;&lt;/blockquote&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Question 1: What is linear regression? What is the difference between simple linear regression and multiple linear regression? 
    &lt;div id=&#34;question-1-what-is-linear-regression-what-is-the-difference-between-simple-linear-regression-and-multiple-linear-regression&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#question-1-what-is-linear-regression-what-is-the-difference-between-simple-linear-regression-and-multiple-linear-regression&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables. It is used to predict the value of the dependent variable based on the values of the independent variables.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Generalized AI Model for Prediction</title>
      <link>/dsblog/Generalized-AI-Model-for-Prediction/</link>
      <pubDate>Fri, 17 Sep 2021 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/Generalized-AI-Model-for-Prediction/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6009-Generalized-AI-Model-for-Prediction.jpg&#34; alt=&#34;Generalized AI Model for Prediction&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Can we really Develop AI solutions that can predict human behavior? If you are not a technical person then don’t get overwhelmed by the next paragraph, you can read further, and it will make sense to you.&lt;/p&gt;
&lt;p&gt;We know the basic equation, y = mx + c. This comes from algebra and trigonometry. Here, y is the predicted value, and x is the input. The x can be a simple scalar value or a vector. Similarly, m is the coefficient in this equation, and it can be a simple scalar value or a vector. If m or x is a vector, then it can hold multiple values. The value of m corresponding to x is also called slope in trigonometry. If a plane is 2 dimensional, then you have one m and one x. But if a plane is complex, and it has, let us say, 10 dimensions then it has 9 m and 9 x. 10th dimensions is predicted by these 9 m and 9 x, using the earlier formula. How that multiplication happens is easy for those who know vector and matrix multiplication, but for others, it is really complicated. So, you can leave it for the time being.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What Are Transformers in AI</title>
      <link>/dsblog/What-Are-Transformers-in-AI/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/What-Are-Transformers-in-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg&#34; alt=&#34;What-are-Transformers-in-AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What Are Transformers in AI 
    &lt;div id=&#34;what-are-transformers-in-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-transformers-in-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Transformer Architecture 
    &lt;div id=&#34;transformer-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformer-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/transformer/transformer-arch.jpg&#34; alt=&#34;Transformer&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Machine Learning Tasks and Model Evaluation</title>
      <link>/dsblog/ml-tasks-and-model-evaluation/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ml-tasks-and-model-evaluation/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg&#34; alt=&#34;Deep Learning Tasks and Models&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Machine Learning Tasks and Model Evaluation 
    &lt;div id=&#34;machine-learning-tasks-and-model-evaluation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#machine-learning-tasks-and-model-evaluation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Machine learning is a subject where we study how to create &amp;amp; evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI, ML, Deep Learning, NLP Conferences &amp; Journals</title>
      <link>/dsblog/ai-ml-dl-nlp-conferences/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/ai-ml-dl-nlp-conferences/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dsresources/dsr108-AI-ML-Deep-Learning-NLP-Conferences-Journals.jpg&#34; alt=&#34;AI, ML, Deep Learning, NLP Conferences &amp;amp; Journals&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI, ML, Deep Learning, NLP Conferences &amp;amp; Journals 
    &lt;div id=&#34;ai-ml-deep-learning-nlp-conferences--journals&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-ml-deep-learning-nlp-conferences--journals&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Content on this page keeps changing. These links are taken from my chrome browser favorites.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A list of 30+ AI conferences and journals related to AI, NLP, Deep Learning. Almost all major happening in the world of AI/ML is presented in these conferences and published in these magazines.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Important AI Research Papers</title>
      <link>/dsblog/important-ai-research-papers/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/important-ai-research-papers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dsresources/dsr105-Important-AI-Research-Papers.jpg&#34; alt=&#34;Important AI Research Papers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Important AI Research Papers 
    &lt;div id=&#34;important-ai-research-papers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#important-ai-research-papers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Content from this page is migrated to &lt;a href=&#34;https://dasarpai.com/dsblog/select-ai-papers&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>How Naive Bayes Classifier Works</title>
      <link>/dsblog/How-Naive-Bayes-Classifier-Works/</link>
      <pubDate>Wed, 31 Mar 2021 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/How-Naive-Bayes-Classifier-Works/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6005-How-Naive-Bayes-Work-for-Recommendation.jpg&#34; alt=&#34;Naive Bayes&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;How Naive Bayes Classifier Works? 
    &lt;div id=&#34;how-naive-bayes-classifier-works&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#how-naive-bayes-classifier-works&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Naive Bayes classifier example 
    &lt;div id=&#34;naive-bayes-classifier-example&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#naive-bayes-classifier-example&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In this presentation, I am not going into the depth of the Naive Bayes algorithm. I am assuming you have heard this term many times but are not able to visualize it mentally or struggling to comprehend this. If that is the case, then you are on the right page.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Reinforcement Learning Git Repositories</title>
      <link>/dsblog/rl-git-repo/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/rl-git-repo/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dsresources/dsr101-Reinforcement-Learning-Git-Repositories.jpg&#34; alt=&#34;Reinforcement Learning Git Repositories&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Reinforcement Learning Git Repositories 
    &lt;div id=&#34;reinforcement-learning-git-repositories&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#reinforcement-learning-git-repositories&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Sno.&lt;/th&gt;
          &lt;th&gt;URL&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;OpenAI Baselines: high-quality implementations of reinforcement learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/hill-a/stable-baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A fork of OpenAI Baselines, implementations of reinforcement learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/spinningup&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/spinningup&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;An educational resource to help anyone learn deep reinforcement learning.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/google/dopamine&#34; target=&#34;_blank&#34;&gt;https://github.com/google/dopamine&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Dopamine is a research framework for fast prototyping of reinforcement learning algorithms.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorflow/agents&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorflow/agents&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TF-Agents is a library for Reinforcement Learning in TensorFlow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/trfl&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/trfl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TensorFlow Reinforcement Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/Horizon&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/Horizon&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A platform for Applied Reinforcement Learning (Applied RL)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ELF&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/ELF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;An End-To-End, Lightweight and Flexible Platform for Game Research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/NervanaSystems/coach&#34; target=&#34;_blank&#34;&gt;https://github.com/NervanaSystems/coach&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Reinforcement Learning Coach by Intel AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ray-project/ray/tree/master/python/ray/rllib&#34; target=&#34;_blank&#34;&gt;https://github.com/ray-project/ray/tree/master/python/ray/rllib&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A fast and simple framework for building and running distributed applications.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/keras-rl/keras-rl&#34; target=&#34;_blank&#34;&gt;https://github.com/keras-rl/keras-rl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Deep Reinforcement Learning for Keras.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&#34; target=&#34;_blank&#34;&gt;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Kaixhin/Rainbow&#34; target=&#34;_blank&#34;&gt;https://github.com/Kaixhin/Rainbow&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Rainbow: Combining Improvements in Deep Reinforcement Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MillionIntegrals/vel&#34; target=&#34;_blank&#34;&gt;https://github.com/MillionIntegrals/vel&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Velocity in deep-learning research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorforce/tensorforce&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorforce/tensorforce&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorforce: A TensorFlow library for applied reinforcement learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/kengz/SLM-Lab&#34; target=&#34;_blank&#34;&gt;https://github.com/kengz/SLM-Lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Modular Deep Reinforcement Learning framework in PyTorch.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/rlworkgroup/garage&#34; target=&#34;_blank&#34;&gt;https://github.com/rlworkgroup/garage&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A framework for reproducible reinforcement learning research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34; target=&#34;_blank&#34;&gt;https://github.com/catalyst-team/catalyst&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Reproducible and fast DL &amp;amp; RL.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/higgsfield/RL-Adventure&#34; target=&#34;_blank&#34;&gt;https://github.com/higgsfield/RL-Adventure&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Pytorch Implementation of DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchical RL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/qfettes/DeepRL-Tutorials&#34; target=&#34;_blank&#34;&gt;https://github.com/qfettes/DeepRL-Tutorials&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Contains high quality implementations of Deep Reinforcement Learning algorithms written in PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/gym&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A toolkit for developing and comparing reinforcement learning algorithms.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/lab&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A customisable 3D platform for agent-based AI research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Microsoft/malmo&#34; target=&#34;_blank&#34;&gt;https://github.com/Microsoft/malmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Project Malmo is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. We aim to inspire a new generation of research into challenging new problems presented by this unique environment. — For installation instruct&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/retro&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Retro Games in Gym&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/dm_control&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/dm_control&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;The DeepMind Control Suite and Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;26&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/neural-mmo&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/neural-mmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Neural MMO – A Massively Multiagent Game Environment&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;27&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/gym&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Gym @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;28&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/lab&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Lab @ DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;29&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Microsoft/malmo&#34; target=&#34;_blank&#34;&gt;https://github.com/Microsoft/malmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Project Malmo @ Microsoft&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;30&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/retro&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Retro @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;31&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/dm_control&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/dm_control&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Control Suite @ DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;32&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/neural-mmo&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/neural-mmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Neural MMO @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;33&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;34&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/hill-a/stable-baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Antonin Raffin, Ashley Hill&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;35&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34; target=&#34;_blank&#34;&gt;https://github.com/catalyst-team/catalyst&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Sergey Kolesnikov&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;36&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ray-project/ray/tree/master/python/ray/rllib&#34; target=&#34;_blank&#34;&gt;https://github.com/ray-project/ray/tree/master/python/ray/rllib&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Ray Team&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;37&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorflow/agents&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorflow/agents&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Google&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;38&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/Horizon&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/Horizon&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Facebook&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;39&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/NervanaSystems/coach&#34; target=&#34;_blank&#34;&gt;https://github.com/NervanaSystems/coach&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Intel&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;40&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/rlworkgroup/garage&#34; target=&#34;_blank&#34;&gt;https://github.com/rlworkgroup/garage&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by community&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;41&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/kengz/SLM-Lab&#34; target=&#34;_blank&#34;&gt;https://github.com/kengz/SLM-Lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Wah Loon Keng, Laura Graesser&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;42&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/google/dopamine&#34; target=&#34;_blank&#34;&gt;https://github.com/google/dopamine&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Google&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;43&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/spinningup&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/spinningup&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;44&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/trfl&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/trfl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;45&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/scalable_agent&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/scalable_agent&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;46&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ELF&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/ELF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Facebook&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;47&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/keras-rl/keras-rl&#34; target=&#34;_blank&#34;&gt;https://github.com/keras-rl/keras-rl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Matthias Plappert&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;48&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&#34; target=&#34;_blank&#34;&gt;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Ilya Kostrikov&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;49&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Kaixhin/Rainbow&#34; target=&#34;_blank&#34;&gt;https://github.com/Kaixhin/Rainbow&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Kai Arulkumaran&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;50&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MillionIntegrals/vel&#34; target=&#34;_blank&#34;&gt;https://github.com/MillionIntegrals/vel&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Jerry (?)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;51&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Khrylx/PyTorch-RL&#34; target=&#34;_blank&#34;&gt;https://github.com/Khrylx/PyTorch-RL&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;52&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorforce/tensorforce&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorforce/tensorforce&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;53&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/higgsfield/RL-Adventure&#34; target=&#34;_blank&#34;&gt;https://github.com/higgsfield/RL-Adventure&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;54&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/qfettes/DeepRL-Tutorials&#34; target=&#34;_blank&#34;&gt;https://github.com/qfettes/DeepRL-Tutorials&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;55&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/SurrealAI/surreal&#34; target=&#34;_blank&#34;&gt;https://github.com/SurrealAI/surreal&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TorchX&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;56&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/zuoxingdong/lagom&#34; target=&#34;_blank&#34;&gt;https://github.com/zuoxingdong/lagom&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;57&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/dennybritz/reinforcement-learning&#34; target=&#34;_blank&#34;&gt;https://github.com/dennybritz/reinforcement-learning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;58&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/unixpickle/anyrl-py&#34; target=&#34;_blank&#34;&gt;https://github.com/unixpickle/anyrl-py&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;59&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Scitator/rl-course-experiments&#34; target=&#34;_blank&#34;&gt;https://github.com/Scitator/rl-course-experiments&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;60&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/oxwhirl/pymarl&#34; target=&#34;_blank&#34;&gt;https://github.com/oxwhirl/pymarl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
      
    </item>
    
    <item>
      <title>What is XAI?</title>
      <link>/dsblog/What-is-XAI/</link>
      <pubDate>Fri, 15 May 2020 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/What-is-XAI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6003-XAI.jpg&#34; alt=&#34;XAI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is XAI? 
    &lt;div id=&#34;what-is-xai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-xai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;XAI in Simple Language! 
    &lt;div id=&#34;xai-in-simple-language&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#xai-in-simple-language&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The discipline of Data Science and AI has introduced many terms into discussions that might seem complicated at first. In reality, many of these terms are intuitive and straightforward when considered from a natural intelligence perspective. However, from a technological standpoint, they can be complex. To understand XAI, let’s explore a few examples.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
