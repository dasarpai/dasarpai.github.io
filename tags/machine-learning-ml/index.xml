<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning (ML) on </title>
    <link>http://localhost:1313/tags/machine-learning-ml/</link>
    <description>Recent content in Machine Learning (ML) on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Dr. Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Dr. Hari Thapliyaal)</webMaster>
    <copyright>¬© 2025 Dr. Hari Thapliyaal</copyright>
    <lastBuildDate>Fri, 28 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/machine-learning-ml/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Audio Video Processing Concepts</title>
      <link>http://localhost:1313/dsblog/audio-video-processing-concepts/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/audio-video-processing-concepts/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6252-audio-video-processing-concepts.jpg&#34; alt=&#34;Audio Video Processing Concepts&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Foundational Concepts of Audio and Video Processing 
    &lt;div id=&#34;foundational-concepts-of-audio-and-video-processing&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#foundational-concepts-of-audio-and-video-processing&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Whether you are multimedia professional or deep learning Engineer, if you are dealing with audio and video processing, you will need to understand the core concepts of audio and video processing. My this guide is focussed on some of the key concepts of Audio Video processing.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Quantum Physics with Deeper Questions with ChatGPT</title>
      <link>http://localhost:1313/dsblog/quantum-physics-with-deeper-questions/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/quantum-physics-with-deeper-questions/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6253-quantum-physics-with-deeper-questions.jpg&#34; alt=&#34;Quantum Physics with Deeper Questions&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Quantum Physics with Deeper Questions with ChatGPT 
    &lt;div id=&#34;quantum-physics-with-deeper-questions-with-chatgpt&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#quantum-physics-with-deeper-questions-with-chatgpt&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;I was a physics student during my college years, and I‚Äôve always loved the subject‚Äîeven today. However, Quantum Physics, despite all my reading and learning, remains a mystery to me. At this stage, who can teach me Quantum Physics effectively? Finding an able professor, aligning my availability with theirs, and hoping they‚Äôd be willing to teach me for free seems impossible. So, I turned to a Large Language Model (LLM) for help. I could have explored other LLMs like Grok, Claude, DeepSeek, and many more. I‚Äôm not saying the others are bad, nor am I claiming that ChatGPT is the best at providing plausible answers. Whether an answer is correct or plausible also depends on the learner‚Äôs ability.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring Graphics Processing Units (GPUs)</title>
      <link>http://localhost:1313/dsblog/Exploring-GPUs/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Exploring-GPUs/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6188-Exploring-GPUs.jpg&#34; alt=&#34;Exploring Graphics Processing Units (GPUs)&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring Graphics Processing Units (GPUs) 
    &lt;div id=&#34;exploring-graphics-processing-units-gpus&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-graphics-processing-units-gpus&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Overall Computational Power of GPUs&lt;/strong&gt; 
    &lt;div id=&#34;overall-computational-power-of-gpus&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#overall-computational-power-of-gpus&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ö° &lt;strong&gt;Incredible Calculation Speed:&lt;/strong&gt; Modern GPUs can perform tens of trillions of calculations per second (e.g., 36 trillion for Cyberpunk 2077).&lt;/li&gt;
&lt;li&gt;üåç &lt;strong&gt;Human Comparison:&lt;/strong&gt; Achieving this manually would require the equivalent of over 4,400 Earths full of people, each doing one calculation every second.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;GPU vs. CPU&lt;/strong&gt; 
    &lt;div id=&#34;gpu-vs-cpu&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#gpu-vs-cpu&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üö¢ &lt;strong&gt;Cargo Ship vs. Airplane Analogy:&lt;/strong&gt; GPUs are like cargo ships (massive capacity, slower), and CPUs are like jets (fast, versatile, fewer tasks at once).&lt;/li&gt;
&lt;li&gt;‚öñÔ∏è &lt;strong&gt;Different Strengths:&lt;/strong&gt; CPUs handle operating systems, flexible tasks, and fewer but more complex instructions. GPUs excel at huge amounts of simple, repetitive calculations.&lt;/li&gt;
&lt;li&gt;üîÄ &lt;strong&gt;Parallel vs. General Purpose:&lt;/strong&gt; GPUs are less flexible but highly parallel, CPUs are more general-purpose and can run a wide variety of programs and instructions.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;GPU Architecture &amp;amp; Components (GA102 Example)&lt;/strong&gt; 
    &lt;div id=&#34;gpu-architecture--components-ga102-example&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#gpu-architecture--components-ga102-example&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üíΩ &lt;strong&gt;Central GPU Die (GA102):&lt;/strong&gt; A large chip with 28.3 billion transistors organized into Graphics Processing Clusters (GPCs), Streaming Multiprocessors (SMs), and cores.&lt;/li&gt;
&lt;li&gt;üèóÔ∏è &lt;strong&gt;Hierarchical Structure:&lt;/strong&gt; GA102 has 7 GPCs ‚Üí 12 SMs per GPC ‚Üí 4 Warps per SM ‚Üí 32 CUDA Per Wrap and 4 Tensor Per Warmp and 1 Ray Tracing Per GPC.&lt;/li&gt;
&lt;li&gt;üî¢ &lt;strong&gt;Types of Cores:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;‚öôÔ∏è CUDA Cores: Handle basic arithmetic (addition, multiplication) most commonly used in gaming.&lt;/li&gt;
&lt;li&gt;üß© Tensor Cores: Perform massive matrix calculations for AI and neural networks.&lt;/li&gt;
&lt;li&gt;üíé Ray Tracing Cores: Specialized for lighting and reflection calculations in real-time graphics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Manufacturing &amp;amp; Binning&lt;/strong&gt; 
    &lt;div id=&#34;manufacturing--binning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#manufacturing--binning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üîß &lt;strong&gt;Shared Chip Design:&lt;/strong&gt; Different GPU models (e.g., 3080, 3090, 3090 Ti) share the same GA102 design.&lt;/li&gt;
&lt;li&gt;üï≥Ô∏è &lt;strong&gt;Defects &amp;amp; Binning:&lt;/strong&gt; Manufacturing imperfections result in some cores being disabled. This leads to different ‚Äútiers‚Äù of the same GPU architecture.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;CUDA Core Internals&lt;/strong&gt; 
    &lt;div id=&#34;cuda-core-internals&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#cuda-core-internals&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ûï &lt;strong&gt;Simple Calculator Design:&lt;/strong&gt; Each CUDA core is basically a tiny calculator that does fused multiply-add (FMA) and a few other operations.&lt;/li&gt;
&lt;li&gt;üíª &lt;strong&gt;Common Operations:&lt;/strong&gt; Primarily handles 32-bit floating-point and integer arithmetic. More complex math (division, trignometry) is done by fewer, special function units.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Memory Systems: GDDR6X &amp;amp; GDDR7&lt;/strong&gt; 
    &lt;div id=&#34;memory-systems-gddr6x--gddr7&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#memory-systems-gddr6x--gddr7&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üíæ &lt;strong&gt;Graphics Memory:&lt;/strong&gt; GDDR6X chips (by Micron) feed terabytes of data per second into the GPU‚Äôs thousands of cores.&lt;/li&gt;
&lt;li&gt;üöÄ &lt;strong&gt;High Bandwidth:&lt;/strong&gt; GPU memory operates at huge bandwidths (over 1 terabyte/s) compared to typical CPU memory (~64 GB/s).&lt;/li&gt;
&lt;li&gt;üî¢ &lt;strong&gt;Beyond Binary:&lt;/strong&gt; GDDR6X and GDDR7 use multiple voltage levels (PAM-4 and PAM-3) to encode more data per signal, increasing transfer rates.&lt;/li&gt;
&lt;li&gt;üèóÔ∏è &lt;strong&gt;Future Memory Tech:&lt;/strong&gt; Micron also develops HBM (High Bandwidth Memory) for AI accelerators, stacking memory chips in 3D, greatly boosting capacity and speed while reducing power.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Parallel Computing Concepts (SIMD &amp;amp; SIMT)&lt;/strong&gt; 
    &lt;div id=&#34;parallel-computing-concepts-simd--simt&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#parallel-computing-concepts-simd--simt&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ôªÔ∏è &lt;strong&gt;Embarrassingly Parallel:&lt;/strong&gt; Tasks like graphics rendering, Bitcoin mining, or AI training are easily split into millions of independent calculations.&lt;/li&gt;
&lt;li&gt;üìú &lt;strong&gt;Single Instruction Multiple Data (SIMD):&lt;/strong&gt; Apply the same instruction to many data points at once‚Äîperfect for transforming millions of vertices in a 3D scene.&lt;/li&gt;
&lt;li&gt;üîì &lt;strong&gt;From SIMD to SIMT:&lt;/strong&gt; Newer GPUs use Single Instruction Multiple Threads (SIMT), allowing threads to progress independently and handle complex branching more efficiently.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Thread &amp;amp; Warp Organization&lt;/strong&gt; 
    &lt;div id=&#34;thread--warp-organization&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#thread--warp-organization&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üì¶ &lt;strong&gt;Thread Hierarchy:&lt;/strong&gt; Threads ‚Üí Warps (groups of 32 threads) ‚Üí Thread Blocks ‚Üí Grids.&lt;/li&gt;
&lt;li&gt;üéõÔ∏è &lt;strong&gt;Gigathread Engine:&lt;/strong&gt; Manages the allocation of thread blocks to streaming multiprocessors, optimizing parallel processing.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Practical Applications&lt;/strong&gt; 
    &lt;div id=&#34;practical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#practical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üéÆ &lt;strong&gt;Video Games:&lt;/strong&gt; GPUs transform coordinates, apply textures, shading, and handle complex rendering pipelines. Millions of identical operations on different vertices and pixels are done in parallel.&lt;/li&gt;
&lt;li&gt;‚Çø &lt;strong&gt;Bitcoin Mining:&lt;/strong&gt; GPUs can run the SHA-256 hashing algorithm in parallel many millions of times per second. Though now replaced by ASIC miners, GPUs were initially very efficient at this.&lt;/li&gt;
&lt;li&gt;ü§ñ &lt;strong&gt;AI &amp;amp; Neural Networks:&lt;/strong&gt; Tensor cores accelerate matrix multiplications critical for training neural nets and powering generative AI.&lt;/li&gt;
&lt;li&gt;üí° &lt;strong&gt;Ray Tracing:&lt;/strong&gt; Specialized cores handle ray tracing calculations for realistic lighting and reflections in real-time graphics.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Micron‚Äôs Role &amp;amp; Advancements&lt;/strong&gt; 
    &lt;div id=&#34;microns-role--advancements&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#microns-role--advancements&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üè≠ &lt;strong&gt;Micron Memory Chips:&lt;/strong&gt; GDDR6X and future GDDR7 designed by Micron power high-speed data transfers on GPUs.&lt;/li&gt;
&lt;li&gt;üîÆ &lt;strong&gt;Innovations in Memory:&lt;/strong&gt; High Bandwidth Memory (HBM) for AI chips stacks DRAM vertically, creating high-capacity, high-throughput solutions at lower energy costs.&lt;/li&gt;
&lt;li&gt;üìö &lt;strong&gt;Technological Marvel:&lt;/strong&gt; Modern graphics cards are a blend of advanced materials, clever architectures, and innovative manufacturing. They enable astonishing levels of visual realism, parallel computation, and AI capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=h9Z4oGN89MU&#34; target=&#34;_blank&#34;&gt;How do Graphics Cards Work? Exploring GPU Architecture&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Models and Creators</title>
      <link>http://localhost:1313/dsblog/AI-Models-and-Creators/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-Models-and-Creators/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6187-ai-models-and-creators.jpg&#34; alt=&#34;AI Models and Creators&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Models and Creators 
    &lt;div id=&#34;ai-models-and-creators&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-models-and-creators&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Popular Models and Their Creator 
    &lt;div id=&#34;popular-models-and-their-creator&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#popular-models-and-their-creator&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Nova - Amazon&lt;/li&gt;
&lt;li&gt;Gemini, Gemma - Google&lt;/li&gt;
&lt;li&gt;Granite - Oracle&lt;/li&gt;
&lt;li&gt;GPT - OpenAI&lt;/li&gt;
&lt;li&gt;Phi - Microsoft Azure&lt;/li&gt;
&lt;li&gt;Einstein - Salesforce&lt;/li&gt;
&lt;li&gt;Joule - SAP&lt;/li&gt;
&lt;li&gt;Grok - X (formerly Twitter)&lt;/li&gt;
&lt;li&gt;Llama - Meta&lt;/li&gt;
&lt;li&gt;Qwen - Alibaba&lt;/li&gt;
&lt;li&gt;Claude - Anthropic&lt;/li&gt;
&lt;li&gt;Bard - Google&lt;/li&gt;
&lt;li&gt;PaLM - Google&lt;/li&gt;
&lt;li&gt;Mistral - Mistral AI&lt;/li&gt;
&lt;li&gt;Falcon - Technology Innovation Institute (TII), UAE&lt;/li&gt;
&lt;li&gt;Gato - DeepMind&lt;/li&gt;
&lt;li&gt;Jasper - Jasper AI&lt;/li&gt;
&lt;li&gt;Bloom - BigScience (collaborative project)&lt;/li&gt;
&lt;li&gt;Ernie - Baidu&lt;/li&gt;
&lt;li&gt;Alpaca - Stanford University (fine-tuned LLaMA model)&lt;/li&gt;
&lt;li&gt;Stable Diffusion - Stability AI&lt;/li&gt;
&lt;li&gt;HuggingChat - Hugging Face&lt;/li&gt;
&lt;li&gt;Cohere of Command&lt;/li&gt;
&lt;li&gt;Alpha fold of deepmind&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Models Developed by Microsoft 
    &lt;div id=&#34;models-developed-by-microsoft&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#models-developed-by-microsoft&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Microsoft has developed or collaborated on several AI models and frameworks, especially as part of its Azure AI ecosystem and its partnership with OpenAI. Below is a list of models and AI systems associated with Microsoft:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring GGUF and Other Model Formats</title>
      <link>http://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6180-exploring-gguf.jpg&#34; alt=&#34;Understanding GGUF and Other Model Formats in Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;&lt;strong&gt;Understanding GGUF and Other Model Formats in Machine Learning&lt;/strong&gt; 
    &lt;div id=&#34;understanding-gguf-and-other-model-formats-in-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-gguf-and-other-model-formats-in-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;As machine learning models continue to grow in complexity, the need for efficient, flexible, and versatile model formats becomes more pronounced. While formats like ONNX, TensorFlow‚Äôs SavedModel, and PyTorch‚Äôs native format have been around for some time, newer formats like GGUF are gaining attention for their unique benefits. This article explores these formats, their use cases, and how they support various aspects of machine learning, including deployment, compatibility, and optimization.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Exploring AnythingLLM</title>
      <link>http://localhost:1313/dsblog/exploring-anythingllm/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/exploring-anythingllm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6179-exploring-anythingllm.jpg&#34; alt=&#34;Exploring AnythingLLM &#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Exploring AnythingLLM 
    &lt;div id=&#34;exploring-anythingllm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#exploring-anythingllm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is AnythingLLM? 
    &lt;div id=&#34;what-is-anythingllm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-anythingllm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;AnythingLLM is an open-source project developed by Mintplex Labs that offers a highly flexible platform for creating personalized language models and knowledge databases. It operates using Retrieval-Augmented Generation (RAG), which combines language models with data from custom document collections. AnythingLLM supports embedding models (e.g., BERT), language models, and vector databases to index and query data, allowing users to fine-tune or deploy various models tailored to their needs, from local deployments to cloud integrations with OpenAI or Azure OpenAI.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Types of Large Language Models (LLM)</title>
      <link>http://localhost:1313/dsblog/Types-of-LLM/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Types-of-LLM/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6171-Types-of-LLM.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Generative AI (GenAI) is expanding at an astonishing rate, with new models emerging almost daily, each sporting unique names, capabilities, versions, and sizes. For AI professionals, keeping track of these models can feel like a full-time job. But for business users, IT professionals, and software developers trying to make the right choice, understanding the model‚Äôs name and what it represents can seem overwhelming. Wouldn‚Äôt it be helpful if we could decode the meaning behind these names to know if a model fits our needs and is worth the investment? In this article, we‚Äôll break down how the names of GenAI models can reveal clues about their functionality and suitability for specific tasks, helping you make informed decisions with confidence.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI/ML with Oracle Cloud</title>
      <link>http://localhost:1313/dsblog/AI-ML-With-Oracle-Cloud/</link>
      <pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-ML-With-Oracle-Cloud/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6166-AI-ML-With-Oracle-Cloud.jpg&#34; alt=&#34;AI/ML with Oracle Cloud&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI/ML with Oracle Cloud 
    &lt;div id=&#34;aiml-with-oracle-cloud&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#aiml-with-oracle-cloud&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/services.htm&#34; target=&#34;_blank&#34;&gt;Oracle Infrastructure Services&lt;/a&gt; 
    &lt;div id=&#34;oracle-infrastructure-services&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#oracle-infrastructure-services&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Register for &lt;a href=&#34;https://www.oracle.com/cloud/free/?source=:ow:o:h:po:OHPPanel1nav0625&amp;amp;intcmp=:ow:o:h:po:OHPPanel1nav0625&#34; target=&#34;_blank&#34;&gt;Oracle Cloud Free Tier&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Oracle AI Main services 
    &lt;div id=&#34;oracle-ai-main-services&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#oracle-ai-main-services&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.oracle.com/digital-assistant/oda-instances?region=ap-mumbai-1&#34; target=&#34;_blank&#34;&gt;Digital Assistant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/document-understanding/using/home.htm&#34; target=&#34;_blank&#34;&gt;Document Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/language/using/pretrain-models.htm#lang-detect&#34; target=&#34;_blank&#34;&gt;Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/vision/using/pretrained-model-using-image.htm&#34; target=&#34;_blank&#34;&gt;Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/speech/home.htm&#34; target=&#34;_blank&#34;&gt;Speech&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/Streaming/home.htm&#34; target=&#34;_blank&#34;&gt;Stream&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/process-automation/oci-process-automation/overview-oci-process-automation.html&#34; target=&#34;_blank&#34;&gt;Cloud Infra Automation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;a href=&#34;https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm&#34; target=&#34;_blank&#34;&gt;Generative AI&lt;/a&gt; 
    &lt;div id=&#34;generative-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#generative-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Generative AI is a fully managed Oracle Cloud Infrastructure service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, including chat, text generation, summarization, and creating text embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Machine Learning Key Concepts</title>
      <link>http://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</link>
      <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg&#34; alt=&#34;Exploring Docker and VS Code Integration&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Machine Learning Key Concepts 
    &lt;div id=&#34;machine-learning-key-concepts&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#machine-learning-key-concepts&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In this article Essential Machine Learning Techniques/Concepts are Explained, some of them are are Cross-Validation, Hyperparameter Optimization, Machine learning types and much More.&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;If you are looking for the answer to any of the following questions, then the answer is &amp;lsquo;Yes.&amp;rsquo;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Automated Machine Learning</title>
      <link>http://localhost:1313/dsblog/AutoML-Tools/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AutoML-Tools/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6150-AutoML-Tools.jpg&#34; alt=&#34;What is AutoML&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Automated Machine Learning 
    &lt;div id=&#34;automated-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#automated-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;This is article is for you, if you know&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;About Machine learning, ML models building&lt;/li&gt;
&lt;li&gt;That machines are capable of building these models themselves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But you don&amp;rsquo;t know how it happens and what are different libraries available for this work.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Everything About Developer Console</title>
      <link>http://localhost:1313/dsblog/Everything-About-Developer-Console/</link>
      <pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Everything-About-Developer-Console/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6148-Everything-About-Developer-Console.jpg&#34; alt=&#34;Everything About Developer Console&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Console Types Every Programmer Should Know 
    &lt;div id=&#34;console-types-every-programmer-should-know&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#console-types-every-programmer-should-know&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;If you are you confused about a term &amp;ldquo;console&amp;rdquo; which you heard at many places and in many context, and you want to know the following answers, then continue reading.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Navigating Google Cloud Security: Key Components, Roles, and Best Practices</title>
      <link>http://localhost:1313/dsblog/Google-Cloud-Security-Components/</link>
      <pubDate>Sat, 28 Sep 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Google-Cloud-Security-Components/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6147-Google-Cloud-Security-Components.jpg&#34; alt=&#34;Navigating Google Cloud Security&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Navigating Google Cloud Security 
    &lt;div id=&#34;navigating-google-cloud-security&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#navigating-google-cloud-security&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Is this article for me? 
    &lt;div id=&#34;is-this-article-for-me&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#is-this-article-for-me&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;If you are looking answers of these questions then continue reading.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are various components of GCP security architecture?&lt;/li&gt;
&lt;li&gt;What is overall hierarchy of GCP Security Components?&lt;/li&gt;
&lt;li&gt;What are Principal, Permission, Roles and Policies in GCP and how are they interconnected?&lt;/li&gt;
&lt;li&gt;Can you give examples of Permissions in GCP Security architecture?&lt;/li&gt;
&lt;li&gt;What are different types of resources available on GCP?&lt;/li&gt;
&lt;li&gt;Can you help me visualizing organization, folders and project of GCP?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Question: What are various components of GCP security architecture? 
    &lt;div id=&#34;question-what-are-various-components-of-gcp-security-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#question-what-are-various-components-of-gcp-security-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Google Cloud Platform (GCP) has a complex security architecture that consists of various components. Here‚Äôs a list of key components and their hierarchy in GCP&amp;rsquo;s security model:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Variations of Language Model in Huggingface</title>
      <link>http://localhost:1313/dsblog/Variations-of-Language-Model-in-Huggingface/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Variations-of-Language-Model-in-Huggingface/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6138-Variations-of-Language-Model-in-Huggingface.jpg&#34; alt=&#34;Variations-of-LanguageModel&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Variations of Language Model in Huggingface 
    &lt;div id=&#34;variations-of-language-model-in-huggingface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#variations-of-language-model-in-huggingface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What the Model variable in Huggingface? 
    &lt;div id=&#34;what-the-model-variable-in-huggingface&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-the-model-variable-in-huggingface&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;We know base moels like BERT, T5, GPT2, GPT3 etc are developed by researchers working with different companies. But when we look into huggingface model repository we see other models like GPT2LMHeadModel, GPT2ForSequenceClassification, etc what are these?&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>MLOps Tools</title>
      <link>http://localhost:1313/dsblog/MLOps-Tools/</link>
      <pubDate>Tue, 13 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/MLOps-Tools/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6137-MLOps-Tools.jpg&#34; alt=&#34;MLOps-Tools&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;MLOps Tools 
    &lt;div id=&#34;mlops-tools&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#mlops-tools&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;MLOps (Machine Learning Operations) is a set of practices and tools designed to streamline and automate the deployment, monitoring, and management of machine learning models in production environments. It combines principles from both DevOps (Development Operations) and machine learning to ensure that ML models are deployed efficiently, managed effectively, and maintained reliably throughout their lifecycle.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Usecases in Cybersecurity</title>
      <link>http://localhost:1313/dsblog/AI-Usecases-in-Cybersecurity/</link>
      <pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-Usecases-in-Cybersecurity/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6135-AI-Usecases-in-Cybersecurity.jpg&#34; alt=&#34;AI-Usecases-in-Cybersecurity&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Usecases in Cybersecurity 
    &lt;div id=&#34;ai-usecases-in-cybersecurity&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-usecases-in-cybersecurity&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI in Cyber Security, Ethics Related Challenges and Usecases 
    &lt;div id=&#34;ai-in-cyber-security-ethics-related-challenges-and-usecases&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-in-cyber-security-ethics-related-challenges-and-usecases&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;AI Usecases in Cyber Security 
    &lt;div id=&#34;ai-usecases-in-cyber-security&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-usecases-in-cyber-security&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Threat Detection and Response
AI can enhance the detection and response to cybersecurity threats by:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Open Source vs Closed Source AI</title>
      <link>http://localhost:1313/dsblog/Open-Source-vs-Closed-Source-AI/</link>
      <pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Open-Source-vs-Closed-Source-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6134-Open-Source-vs-Closed-Source-AI.jpg&#34; alt=&#34;Open-Source-vs-Closed-Source-AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Open Source AI vs Closed Source AI 
    &lt;div id=&#34;open-source-ai-vs-closed-source-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#open-source-ai-vs-closed-source-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Major players in the AI industry, such as Google, Microsoft, IBM, Salesforce, etc each have their own proprietary models and infrastructure to host these models. They offer AI services that companies use to develop AI products for either their end customers or internal use. Training or developing AI models requires expensive hardware and highly skilled personnel, making it a costly process. However, the deployment and inference stages are even more expensive, as they involve ongoing costs for hardware and monitoring.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>LLM Architecture and Training</title>
      <link>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg&#34; alt=&#34;LLM-Architecture-and-Training&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;&lt;strong&gt;Understanding LLM Architectures and Model Training&lt;/strong&gt; 
    &lt;div id=&#34;understanding-llm-architectures-and-model-training&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-llm-architectures-and-model-training&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. We‚Äôll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Why to Finetune LLM?</title>
      <link>http://localhost:1313/dsblog/why-to-finetune-llm/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/why-to-finetune-llm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6115-why-to-finetune-llm.jpg&#34; alt=&#34;Why to Finetune LLM?&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Finetuning, Fewshot Learning, Why and How? 
    &lt;div id=&#34;finetuning-fewshot-learning-why-and-how&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#finetuning-fewshot-learning-why-and-how&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Why to finetune a LLM? 
    &lt;div id=&#34;why-to-finetune-a-llm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#why-to-finetune-a-llm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Fine-tuning a large language model (LLM) can provide several benefits, depending on your specific needs and objectives. Here are some key reasons to consider fine-tuning an LLM:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Stanford Alpaca</title>
      <link>http://localhost:1313/dsblog/Stanford-Alpaca/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Stanford-Alpaca/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6116-Stanford-Alpaca.jpg&#34; alt=&#34;Stanford-Alpaca&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Stanford Alpaca 
    &lt;div id=&#34;stanford-alpaca&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#stanford-alpaca&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34; target=&#34;_blank&#34;&gt;Stanford Alpaca Github Report&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li&gt;
&lt;li&gt;This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:
&lt;ul&gt;
&lt;li&gt;The 52K &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34; target=&#34;_blank&#34;&gt;instruction-following data&lt;/a&gt; used for fine-tuning the model.&lt;/li&gt;
&lt;li&gt;The code for generating the data.&lt;/li&gt;
&lt;li&gt;The code for fine-tuning the model.&lt;/li&gt;
&lt;li&gt;The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Overview 
    &lt;div id=&#34;overview&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#overview&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li&gt;
&lt;li&gt;Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li&gt;
&lt;li&gt;Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li&gt;
&lt;li&gt;Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li&gt;
&lt;li&gt;Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li&gt;
&lt;li&gt;Model weights can be released if the creators of LLaMA gives permission.&lt;/li&gt;
&lt;li&gt;Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li&gt;
&lt;li&gt;Based on followin papers:
&lt;ul&gt;
&lt;li&gt;LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34; target=&#34;_blank&#34;&gt;Hugo2023&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34; target=&#34;_blank&#34;&gt;Yizhong2022&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Release
&lt;ul&gt;
&lt;li&gt;alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Highlevel Activities of the Alpaca Project 
    &lt;div id=&#34;highlevel-activities-of-the-alpaca-project&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#highlevel-activities-of-the-alpaca-project&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Understanding LLM GAN and Transformers</title>
      <link>http://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg&#34; alt=&#34;Understanding-LLM-GAN-Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Understanding LLM, GAN and Transformers 
    &lt;div id=&#34;understanding-llm-gan-and-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-llm-gan-and-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;LLM Layers 
    &lt;div id=&#34;llm-layers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#llm-layers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Transformers Demystified A Step-by-Step Guide</title>
      <link>http://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</link>
      <pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg&#34; alt=&#34;Transformers Demystified A Step-by-Step Guide&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Transformers Demystified A Step-by-Step Guide 
    &lt;div id=&#34;transformers-demystified-a-step-by-step-guide&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformers-demystified-a-step-by-step-guide&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;All modern Transformers are based on a paper &amp;ldquo;Attention is all you need&amp;rdquo;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Dimensionality Reduction and Visualization</title>
      <link>http://localhost:1313/dsblog/Dimensionality-Reduction-and-Visualization/</link>
      <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Dimensionality-Reduction-and-Visualization/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6126-Dimensionality-Reduction-and-Visualization.jpg&#34; alt=&#34;Dimensionality-Reduction-and-Visualization&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Dimensionality Reduction and Visualization 
    &lt;div id=&#34;dimensionality-reduction-and-visualization&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#dimensionality-reduction-and-visualization&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What are the popular methods of dimensionality reduction? 
    &lt;div id=&#34;what-are-the-popular-methods-of-dimensionality-reduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-the-popular-methods-of-dimensionality-reduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Dimensionality reduction is a crucial step in data preprocessing, particularly when dealing with high-dimensional datasets. It helps in reducing the number of features while retaining the essential information, improving computational efficiency, and facilitating data visualization. Here are some popular methods of dimensionality reduction:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>NLP BenchMarks</title>
      <link>http://localhost:1313/dsblog/NLP-BenchMarks1/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/NLP-BenchMarks1/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6120-NLP-BenchMarks.jpg&#34; alt=&#34;NLP-BenchMarks&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;NLP BenchMarks 
    &lt;div id=&#34;nlp-benchmarks&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#nlp-benchmarks&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Language Model? 
    &lt;div id=&#34;what-is-language-model&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-language-model&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;language model&lt;/strong&gt; is a computational model that understands and generates human language. It learns the patterns and structure of a language by analyzing large amounts of text data, allowing it to predict the next word in a sequence or generate coherent text. Language models are used in applications like text generation, translation, speech recognition, chatbots, and sentiment analysis.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Empowering Language with AI NLP Capabilities</title>
      <link>http://localhost:1313/dsblog/empowering-language-with-ainlp-capabilities/</link>
      <pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/empowering-language-with-ainlp-capabilities/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6106-Empowering-Language-with-AI-NLP-Capabilities.jpg&#34; alt=&#34;Empowering-Language-with-AI-NLP-Capabilities&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Empowering-Language-with-AI-NLP-Capabilities 
    &lt;div id=&#34;empowering-language-with-ai-nlp-capabilities&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#empowering-language-with-ai-nlp-capabilities&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;When envisioning artificial intelligence (AI), the initial images that often come to mind are humanoid robots. However, this perception oversimplifies the vast realm of AI, which is fundamentally distinct from natural intelligence‚Äîthe inherent cognitive capacity found in living organisms shaped by Mother Nature. Life, in all its forms, from microscopic bacteria to complex human beings, possesses an innate intelligence derived from hydrocarbon-based living cells.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Topic Modeling with BERT</title>
      <link>http://localhost:1313/dsblog/topic-modeling-with-bert/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/topic-modeling-with-bert/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6105-Topic-Modeling-with-BERT.jpg&#34; alt=&#34;Topic Modeling with BERT&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Topic Modeling with BERT 
    &lt;div id=&#34;topic-modeling-with-bert&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#topic-modeling-with-bert&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Key steps in BERTopic modelling are as following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &amp;ldquo;Sentence Embedding&amp;rdquo; models to embed the sentences of the article&lt;/li&gt;
&lt;li&gt;Reduce the dimensionality of embedding using UMAP&lt;/li&gt;
&lt;li&gt;Cluster these documents (reduced dimensions) using HDBSAN&lt;/li&gt;
&lt;li&gt;Use c-TF-IDF extract keywords, their frequency and IDF for each cluster.&lt;/li&gt;
&lt;li&gt;MMR: Maximize Candidate Relevance. How many words in a topic can represent the topic?&lt;/li&gt;
&lt;li&gt;Intertopic Distance Map&lt;/li&gt;
&lt;li&gt;Use similarity matrix (heatmap), dandogram (hierarchical map), to visualize the topics and key_words.&lt;/li&gt;
&lt;li&gt;Traction of topic over time period. Some may be irrelevant and for other traction may be increasing or decreasing.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Installation 
    &lt;div id=&#34;installation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#installation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Installation, with sentence-transformers, can be done using pypi:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# If you want to install BERTopic with other embedding models, you can choose one of the following:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Choose an embedding backend&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gensim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spacy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Topic modeling with images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bertopic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vision&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 class=&#34;relative group&#34;&gt;Supported Topic Modelling Techniques 
    &lt;div id=&#34;supported-topic-modelling-techniques&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#supported-topic-modelling-techniques&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;BERTopic supports all kinds of topic modeling techniques as below.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Basics of Word Embedding</title>
      <link>http://localhost:1313/dsblog/basics-of-word-embedding/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/basics-of-word-embedding/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg&#34; alt=&#34;Basics of Word Embedding&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Basics of Word Embedding 
    &lt;div id=&#34;basics-of-word-embedding&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#basics-of-word-embedding&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is Context, target and window? 
    &lt;div id=&#34;what-is-context-target-and-window&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-context-target-and-window&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &amp;ldquo;context&amp;rdquo; word is the surrounding word.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;target&amp;rdquo; word is the middle word.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;window distance&amp;rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s take a sentence&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Graph of Thoughts</title>
      <link>http://localhost:1313/dsblog/graph-of-thoughts/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/graph-of-thoughts/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6103-Graph-of-Thoughts.jpg&#34; alt=&#34;Graph of Thoughts&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Graph of Thoughts 
    &lt;div id=&#34;graph-of-thoughts&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#graph-of-thoughts&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;This is a valuable resource for learning Graph of Thoughts (GoT) concepts. The YouTube video is from code_your_own_AI. I&amp;rsquo;m utilizing the comments made by @wesleychang2005 on the video, which provide an excellent summary of GoT. If you&amp;rsquo;re interested in this topic and find the summary below intriguing, I recommend watching the entire 41-minute video.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is Pinecone</title>
      <link>http://localhost:1313/dsblog/What-is-Pinecone/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-is-Pinecone/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6097-What-is-Pinecone.jpg&#34; alt=&#34;What is Pinecone&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is pinecone? 
    &lt;div id=&#34;what-is-pinecone&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-pinecone&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Pinecone is a managed vector database that provides vector search (or ‚Äúsimilarity search‚Äù) for developers with a straightforward API and usage-based pricing. It‚Äôs free to try. &lt;a href=&#34;https://www.pinecone.io/learn/vector-search-basics/&#34; target=&#34;_blank&#34;&gt;Introduction to Vector Search for Developers&lt;/a&gt;.&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is a &lt;a href=&#34;https://www.pinecone.io/learn/vector-database/&#34; target=&#34;_blank&#34;&gt;Vector Database&lt;/a&gt;? 
    &lt;div id=&#34;what-is-a-vector-database&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-a-vector-database&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Your must have heard about relational database, graph database, object datbase. But this article is about Vector Database.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Distances in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Distances-in-Machine-Learning/</link>
      <pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Distances-in-Machine-Learning/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6093-Distances-in-Machine-Learning.jpg&#34; alt=&#34;Distances in Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Distances in Machine Learning 
    &lt;div id=&#34;distances-in-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#distances-in-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Every sample, record, word, sentence, object, image etc in the Machine learning language is called vector. If we want to measure the similarity or dissimilarity between two data points then we need distance function.&lt;/p&gt;
&lt;p&gt;Distance metrics play a crucial role in various machine learning algorithms, including clustering, classification, and anomaly detection. Different distance measures capture different types of relationships between data points.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Important AI Paper List</title>
      <link>http://localhost:1313/dsblog/select-ai-papers/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/select-ai-papers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg&#34; alt=&#34;Important AI Paper List&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Important AI Paper List 
    &lt;div id=&#34;important-ai-paper-list&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#important-ai-paper-list&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduciton 
    &lt;div id=&#34;introduciton&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduciton&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors&amp;rsquo; information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like &amp;ldquo;Vivek Ramaswami, Kartikeyan Karunanidhi&amp;rdquo; it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on &amp;ldquo;google scholar&amp;rdquo;, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper with Code Resources</title>
      <link>http://localhost:1313/dsblog/paperwithcode-resources/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/paperwithcode-resources/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6091-rps-Paperwithcode-Resources.jpg&#34; alt=&#34;Paper with Code Resources&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper with Code Resources 
    &lt;div id=&#34;paper-with-code-resources&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-with-code-resources&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Papers of 2021 
    &lt;div id=&#34;trending-papers-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-papers-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;ADOP: Approximate Differentiable One-Pixel Point Rendering ‚Äî R√ºckert et al ‚Äî &lt;a href=&#34;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Bayesian Learning Rule ‚ÄîKhan et al &lt;a href=&#34;https://paperswithcode.com/paper/the-bayesian-learning-rule&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/the-bayesian-learning-rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Program Synthesis with Large Language Models ‚Äî Austin et al &lt;a href=&#34;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Masked Autoencoders Are Scalable Vision Learners ‚Äî He et al &lt;a href=&#34;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8-bit Optimizers via Block-wise Quantization ‚Äî Dettmers et al &lt;a href=&#34;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Revisiting ResNets: Improved Training and Scaling Strategies ‚Äî Bello et al &lt;a href=&#34;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Image Super-Resolution via Iterative Refinement ‚Äî Saharia et al &lt;a href=&#34;https://paperswithcode.com/paper/image-super-resolution-via-iterative&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/image-super-resolution-via-iterative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs ‚Äî Jaegle et al &lt;a href=&#34;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do Vision Transformers See Like Convolutional Neural Networks? ‚Äî Raghu et al &lt;a href=&#34;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions ‚Äî Niepert et al &lt;a href=&#34;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Libaries of 2021 
    &lt;div id=&#34;trending-libaries-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-libaries-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PyTorch Image Models ‚Äî Ross Wightman ‚Äî &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34; target=&#34;_blank&#34;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Transformers ‚Äî Hugging Face ‚Äî &lt;a href=&#34;https://github.com/huggingface/transformers&#34; target=&#34;_blank&#34;&gt;https://github.com/huggingface/transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyTorch-GAN ‚Äî Erik Linder-Nor√©n ‚Äî &lt;a href=&#34;https://github.com/eriklindernoren/PyTorch-GAN&#34; target=&#34;_blank&#34;&gt;https://github.com/eriklindernoren/PyTorch-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMDetection ‚Äî OpenMMLab ‚Äî &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34; target=&#34;_blank&#34;&gt;https://github.com/open-mmlab/mmdetection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Darknet ‚Äî AlexeyAB ‚Äî &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vision Transformer PyTorch ‚Äî lucidrains ‚Äî &lt;a href=&#34;https://github.com/lucidrains/vit-pytorch&#34; target=&#34;_blank&#34;&gt;https://github.com/lucidrains/vit-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;InsightFace ‚Äî DeepInsight ‚Äî &lt;a href=&#34;https://github.com/deepinsight/insightface&#34; target=&#34;_blank&#34;&gt;https://github.com/deepinsight/insightface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Detectron2 ‚Äî Meta AI ‚Äî &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/detectron2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PaddleOCR ‚Äî PaddlePaddle ‚Äî &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34;&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FairSeq ‚Äî Meta AI ‚Äî &lt;a href=&#34;https://github.com/pytorch/fairseq&#34; target=&#34;_blank&#34;&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Top Dataset - 2021 
    &lt;div id=&#34;top-dataset---2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#top-dataset---2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MATH ‚Äî Hendrycks et al &lt;a href=&#34;https://paperswithcode.com/dataset/math&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UAV-Human ‚Äî Li et al &lt;a href=&#34;https://paperswithcode.com/dataset/uav-human&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/uav-human&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UPFD (User Preference-aware Fake News Detection) ‚Äî Dou et al &lt;a href=&#34;https://paperswithcode.com/dataset/upfd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/upfd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OGB-LSC (OGB Large-Scale Challenge) ‚Äî Hu et al &lt;a href=&#34;https://paperswithcode.com/dataset/ogb-lsc&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/ogb-lsc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CodeXGLUE ‚ÄîLu et al &lt;a href=&#34;https://paperswithcode.com/dataset/codexglue&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/codexglue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AGORA ‚Äî Patel et al &lt;a href=&#34;https://paperswithcode.com/dataset/agora&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/agora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEIR (Benchmarking IR) ‚Äî Thakur et al &lt;a href=&#34;https://paperswithcode.com/dataset/beir&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/beir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WikiGraphs ‚Äî Wang et al &lt;a href=&#34;https://paperswithcode.com/dataset/wikigraphs&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/wikigraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Few-NERD ‚Äî Ding et al &lt;a href=&#34;https://paperswithcode.com/dataset/few-nerd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/few-nerd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PASS (Pictures without humAns for Self-Supervision) ‚ÄîAsano et al &lt;a href=&#34;https://paperswithcode.com/dataset/pass&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/pass&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Papers of 2022 
    &lt;div id=&#34;papers-of-2022&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#papers-of-2022&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Controllable Animation of Fluid Elements in Still Images&lt;/li&gt;
&lt;li&gt;F-SfT: Shape-From-Template With A Physics-Based Deformation Model&lt;/li&gt;
&lt;li&gt;TWIST: Two-Way Inter-Label Self-Training for Semi-Supervised 3D Instance Segmentation&lt;/li&gt;
&lt;li&gt;Do Learned Representations Respect Causal Relationships?&lt;/li&gt;
&lt;li&gt;ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic&lt;/li&gt;
&lt;li&gt;3D Moments From Near-Duplicate Photos&lt;/li&gt;
&lt;li&gt;Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization&lt;/li&gt;
&lt;li&gt;Blind2Unblind: Self-Supervised Image Denoising With Visible Blind Spots&lt;/li&gt;
&lt;li&gt;Balanced and Hierarchical Relation Learning for One-Shot Object Detection&lt;/li&gt;
&lt;li&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/li&gt;
&lt;li&gt;Stochastic Trajectory Prediction Via Motion Indeterminacy Diffusion&lt;/li&gt;
&lt;li&gt;CLRNet: Cross Layer Refinement Network for Lane Detection&lt;/li&gt;
&lt;li&gt;Motion-Aware Contrastive Video Representation Learning Via Foreground-Background Merging&lt;/li&gt;
&lt;li&gt;DINE: Domain Adaptation From Single and Multiple Black-Box Predictors&lt;/li&gt;
&lt;li&gt;FaceFormer: Speech-Driven 3D Facial Animation With Transformers&lt;/li&gt;
&lt;li&gt;Rotationally Equivariant 3D Object Detection&lt;/li&gt;
&lt;li&gt;Accelerating DETR Convergence Via Semantic-Aligned Matching&lt;/li&gt;
&lt;li&gt;Cloning Outfits From Real-World Images to 3D Characters for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;GeoNeRF: Generalizing NeRF With Geometry Priors&lt;/li&gt;
&lt;li&gt;ABPN: Adaptive Blend Pyramid Network for Real-Time Local Retouching of Ultra High-Resolution Photo&lt;/li&gt;
&lt;li&gt;Expanding Low-Density Latent Regions for Open-Set Object Detection&lt;/li&gt;
&lt;li&gt;Uformer: A General U-Shaped Transformer for Image Restoration&lt;/li&gt;
&lt;li&gt;Exploring Dual-Task Correlation for Pose Guided Person Image Generation&lt;/li&gt;
&lt;li&gt;Portrait Eyeglasses and Shadow Removal By Leveraging 3D Synthetic Data&lt;/li&gt;
&lt;li&gt;Modeling 3D Layout for Group Re-Identification&lt;/li&gt;
&lt;li&gt;Toward Fast, Flexible, and Robust Low-Light Image Enhancement&lt;/li&gt;
&lt;li&gt;Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos&lt;/li&gt;
&lt;li&gt;HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network&lt;/li&gt;
&lt;li&gt;Modular Action Concept Grounding in Semantic Video Prediction&lt;/li&gt;
&lt;li&gt;StyleSwin: Transformer-Based GAN for High-Resolution Image Generation&lt;/li&gt;
&lt;li&gt;Discrete Cosine Transform Network for Guided Depth Map Super-Resolution&lt;/li&gt;
&lt;li&gt;Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing&lt;/li&gt;
&lt;li&gt;TransGeo: Transformer Is All You Need for Cross-View Image Geo-Localization&lt;/li&gt;
&lt;li&gt;Contrastive Boundary Learning for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution&lt;/li&gt;
&lt;li&gt;CVNet: Contour Vibration Network for Building Extraction&lt;/li&gt;
&lt;li&gt;Swin Transformer V2: Scaling Up Capacity and Resolution&lt;/li&gt;
&lt;li&gt;Projective Manifold Gradient Layer for Deep Rotation Regression&lt;/li&gt;
&lt;li&gt;HCSC: Hierarchical Contrastive Selective Coding&lt;/li&gt;
&lt;li&gt;TransRank: Self-Supervised Video Representation Learning Via Ranking-Based Transformation Recognition&lt;/li&gt;
&lt;li&gt;DiSparse: Disentangled Sparsification for Multitask Model Compression&lt;/li&gt;
&lt;li&gt;Pushing The Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make A Difference&lt;/li&gt;
&lt;li&gt;Towards Efficient and Scalable Sharpness-Aware Minimization&lt;/li&gt;
&lt;li&gt;OSSO: Obtaining Skeletal Shape From Outside&lt;/li&gt;
&lt;li&gt;A Study on The Distribution of Social Biases in Self-Supervised Learning Visual Models&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes&lt;/li&gt;
&lt;li&gt;Comparing Correspondences: Video Prediction With Correspondence-Wise Losses&lt;/li&gt;
&lt;li&gt;Towards Fewer Annotations: Active Learning Via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding&lt;/li&gt;
&lt;li&gt;Few Shot Generative Model Adaption Via Relaxed Spatial Structural Alignment&lt;/li&gt;
&lt;li&gt;Enhancing Adversarial Training With Second-Order Statistics of Weights&lt;/li&gt;
&lt;li&gt;Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo&lt;/li&gt;
&lt;li&gt;Moving Window Regression: A Novel Approach to Ordinal Regression&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection&lt;/li&gt;
&lt;li&gt;Robust Optimization As Data Augmentation for Large-Scale Graphs&lt;/li&gt;
&lt;li&gt;Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients&lt;/li&gt;
&lt;li&gt;Improving The Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input&lt;/li&gt;
&lt;li&gt;ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer&lt;/li&gt;
&lt;li&gt;360MonoDepth: High-Resolution 360deg Monocular Depth Estimation&lt;/li&gt;
&lt;li&gt;POCO: Point Convolution for Surface Reconstruction&lt;/li&gt;
&lt;li&gt;Neural Texture Extraction and Distribution for Controllable Person Image Synthesis&lt;/li&gt;
&lt;li&gt;Classification-Then-Grounding: Reformulating Video Scene Graphs As Temporal Bipartite Graphs&lt;/li&gt;
&lt;li&gt;DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis&lt;/li&gt;
&lt;li&gt;ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes&lt;/li&gt;
&lt;li&gt;UNIST: Unpaired Neural Implicit Shape Translation Network&lt;/li&gt;
&lt;li&gt;APES: Articulated Part Extraction From Sprite Sheets&lt;/li&gt;
&lt;li&gt;SPAct: Self-Supervised Privacy Preservation for Action Recognition&lt;/li&gt;
&lt;li&gt;De-Rendering 3D Objects in The Wild&lt;/li&gt;
&lt;li&gt;Global Sensing and Measurements Reuse for Image Compressed Sensing&lt;/li&gt;
&lt;li&gt;Practical Evaluation of Adversarial Robustness Via Adaptive Auto Attack&lt;/li&gt;
&lt;li&gt;Cross-View Transformers for Real-Time Map-View Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Controllable Dynamic Multi-Task Architectures&lt;/li&gt;
&lt;li&gt;FastDOG: Fast Discrete Optimization on GPU&lt;/li&gt;
&lt;li&gt;Focal and Global Knowledge Distillation for Detectors&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Continual Learning&lt;/li&gt;
&lt;li&gt;Human Mesh Recovery From Multiple Shots&lt;/li&gt;
&lt;li&gt;Convolution of Convolution: Let Kernels Spatially Collaborate&lt;/li&gt;
&lt;li&gt;Make It Move: Controllable Image-to-Video Generation With Text Descriptions&lt;/li&gt;
&lt;li&gt;Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling&lt;/li&gt;
&lt;li&gt;Video-Text Representation Learning Via Differentiable Weak Temporal Alignment&lt;/li&gt;
&lt;li&gt;Bi-Directional Object-Context Prioritization Learning for Saliency Ranking&lt;/li&gt;
&lt;li&gt;Vehicle Trajectory Prediction Works, But Not Everywhere&lt;/li&gt;
&lt;li&gt;MonoDTR: Monocular 3D Object Detection With Depth-Aware Transformer&lt;/li&gt;
&lt;li&gt;Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-Shot Learning&lt;/li&gt;
&lt;li&gt;Generalized Category Discovery&lt;/li&gt;
&lt;li&gt;Contour-Hugging Heatmaps for Landmark Detection&lt;/li&gt;
&lt;li&gt;Voxel Field Fusion for 3D Object Detection&lt;/li&gt;
&lt;li&gt;DisARM: Displacement Aware Relation Module for 3D Detection&lt;/li&gt;
&lt;li&gt;MixFormer: Mixing Features Across Windows and Dimensions&lt;/li&gt;
&lt;li&gt;FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment&lt;/li&gt;
&lt;li&gt;HEAT: Holistic Edge Attention Transformer for Structured Reconstruction&lt;/li&gt;
&lt;li&gt;Mobile-Former: Bridging MobileNet and Transformer&lt;/li&gt;
&lt;li&gt;CycleMix: A Holistic Strategy for Medical Image Segmentation From Scribble Supervision&lt;/li&gt;
&lt;li&gt;VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution&lt;/li&gt;
&lt;li&gt;Towards End-to-End Unified Scene Text Detection and Layout Analysis&lt;/li&gt;
&lt;li&gt;AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation&lt;/li&gt;
&lt;li&gt;ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior&lt;/li&gt;
&lt;li&gt;End-to-End Referring Video Object Segmentation With Multimodal Transformers&lt;/li&gt;
&lt;li&gt;IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds&lt;/li&gt;
&lt;li&gt;Detecting Camouflaged Object in Frequency Domain&lt;/li&gt;
&lt;li&gt;SelfRecon: Self Reconstruction Your Digital Avatar From Monocular Video&lt;/li&gt;
&lt;li&gt;Equivariant Point Cloud Analysis Via Learning Orientations for Message Passing&lt;/li&gt;
&lt;li&gt;Node Representation Learning in Graph Via Node-to-Neighbourhood Mutual Information Maximization&lt;/li&gt;
&lt;li&gt;Semi-Supervised Video Semantic Segmentation With Inter-Frame Feature Reconstruction&lt;/li&gt;
&lt;li&gt;Amodal Segmentation Through Out-of-Task and Out-of-Distribution Generalization With A Bayesian Model&lt;/li&gt;
&lt;li&gt;How Well Do Sparse ImageNet Models Transfer?&lt;/li&gt;
&lt;li&gt;REX: Reasoning-Aware and Grounded Explanation&lt;/li&gt;
&lt;li&gt;Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes&lt;/li&gt;
&lt;li&gt;Object-Aware Video-Language Pre-Training for Retrieval&lt;/li&gt;
&lt;li&gt;MAT: Mask-Aware Transformer for Large Hole Image Inpainting&lt;/li&gt;
&lt;li&gt;Align and Prompt: Video-and-Language Pre-Training With Entity Prompts&lt;/li&gt;
&lt;li&gt;MSG-Transformer: Exchanging Local Spatial Information By Manipulating Messenger Tokens&lt;/li&gt;
&lt;li&gt;Cross Modal Retrieval With Querybank Normalisation&lt;/li&gt;
&lt;li&gt;Ray3D: Ray-Based 3D Human Pose Estimation for Monocular Absolute 3D Localization&lt;/li&gt;
&lt;li&gt;ASM-Loc: Action-Aware Segment Modeling for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Scaling Up Your Kernels to 31√ó31: Revisiting Large Kernel Design in CNNs&lt;/li&gt;
&lt;li&gt;End-to-End Multi-Person Pose Estimation With Transformers&lt;/li&gt;
&lt;li&gt;REGTR: End-to-End Point Cloud Correspondences With Transformers&lt;/li&gt;
&lt;li&gt;Neural 3D Scene Reconstruction With The Manhattan-World Assumption&lt;/li&gt;
&lt;li&gt;V2C: Visual Voice Cloning&lt;/li&gt;
&lt;li&gt;Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection&lt;/li&gt;
&lt;li&gt;MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions&lt;/li&gt;
&lt;li&gt;Gait Recognition in The Wild With Dense 3D Representations and A Benchmark&lt;/li&gt;
&lt;li&gt;ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation Via Online Exploration and Synthesis&lt;/li&gt;
&lt;li&gt;QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection&lt;/li&gt;
&lt;li&gt;IDEA-Net: Dynamic 3D Point Cloud Interpolation Via Deep Embedding Alignment&lt;/li&gt;
&lt;li&gt;BEHAVE: Dataset and Method for Tracking Human Object Interactions&lt;/li&gt;
&lt;li&gt;Revisiting Random Channel Pruning for Neural Network Compression&lt;/li&gt;
&lt;li&gt;Generating Diverse and Natural 3D Human Motions From Text&lt;/li&gt;
&lt;li&gt;E-CIR: Event-Enhanced Continuous Intensity Recovery&lt;/li&gt;
&lt;li&gt;Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond&lt;/li&gt;
&lt;li&gt;Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation&lt;/li&gt;
&lt;li&gt;AziNorm: Exploiting The Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception&lt;/li&gt;
&lt;li&gt;Weakly Supervised Rotation-Invariant Aerial Object Detection Network&lt;/li&gt;
&lt;li&gt;Surface Reconstruction From Point Clouds By Learning Predictive Context Priors&lt;/li&gt;
&lt;li&gt;IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes&lt;/li&gt;
&lt;li&gt;DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Action Localization Via Representative Snippet Knowledge Propagation&lt;/li&gt;
&lt;li&gt;E2EC: An End-to-End Contour-Based Method for High-Quality High-Speed Instance Segmentation&lt;/li&gt;
&lt;li&gt;BatchFormer: Learning To Explore Sample Relationships for Robust Representation Learning&lt;/li&gt;
&lt;li&gt;Self-Supervised Image-Specific Prototype Exploration for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Learning Multi-View Aggregation in The Wild for Large-Scale 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition&lt;/li&gt;
&lt;li&gt;Clothes-Changing Person Re-Identification With RGB Modality Only&lt;/li&gt;
&lt;li&gt;Robust Image Forgery Detection Over Online Social Network Shared Images&lt;/li&gt;
&lt;li&gt;Representation Compensation Networks for Continual Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Tracking People By Predicting 3D Appearance, Location and Pose&lt;/li&gt;
&lt;li&gt;Text2Mesh: Text-Driven Neural Stylization for Meshes&lt;/li&gt;
&lt;li&gt;C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image&lt;/li&gt;
&lt;li&gt;Forward Compatible Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Weakly Supervised Object Localization As Domain Adaption&lt;/li&gt;
&lt;li&gt;Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation&lt;/li&gt;
&lt;li&gt;Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching&lt;/li&gt;
&lt;li&gt;Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation&lt;/li&gt;
&lt;li&gt;MatteFormer: Transformer-Based Image Matting Via Prior-Tokens&lt;/li&gt;
&lt;li&gt;Video Shadow Detection Via Spatio-Temporal Interpolation Consistency Training&lt;/li&gt;
&lt;li&gt;Robust and Accurate Superquadric Recovery: A Probabilistic Approach&lt;/li&gt;
&lt;li&gt;Grounding Answers for Visual Questions Asked By Visually Impaired People&lt;/li&gt;
&lt;li&gt;Sparse Instance Activation for Real-Time Instance Segmentation&lt;/li&gt;
&lt;li&gt;VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning&lt;/li&gt;
&lt;li&gt;MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis&lt;/li&gt;
&lt;li&gt;Towards Implicit Text-Guided 3D Shape Generation&lt;/li&gt;
&lt;li&gt;SoftCollage: A Differentiable Probabilistic Tree Generator for Image Collage&lt;/li&gt;
&lt;li&gt;Query and Attention Augmentation for Knowledge-Based Explainable Reasoning&lt;/li&gt;
&lt;li&gt;Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality&lt;/li&gt;
&lt;li&gt;Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection&lt;/li&gt;
&lt;li&gt;Fine-Grained Object Classification Via Self-Supervised Pose Alignment&lt;/li&gt;
&lt;li&gt;Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding&lt;/li&gt;
&lt;li&gt;Fine-Grained Temporal Contrastive Learning for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Relieving Long-Tailed Instance Segmentation Via Pairwise Class Balance&lt;/li&gt;
&lt;li&gt;Online Convolutional Re-Parameterization&lt;/li&gt;
&lt;li&gt;Mimicking The Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning&lt;/li&gt;
&lt;li&gt;RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition&lt;/li&gt;
&lt;li&gt;Personalized Image Aesthetics Assessment With Rich Attributes&lt;/li&gt;
&lt;li&gt;Part-Based Pseudo Label Refinement for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;HDNet: High-Resolution Dual-Domain Learning for Spectral Compressive Imaging&lt;/li&gt;
&lt;li&gt;OW-DETR: Open-World Detection Transformer&lt;/li&gt;
&lt;li&gt;Learning Deep Implicit Functions for 3D Shapes With Dynamic Code Clouds&lt;/li&gt;
&lt;li&gt;Reversible Vision Transformers&lt;/li&gt;
&lt;li&gt;Amodal Panoptic Segmentation&lt;/li&gt;
&lt;li&gt;Correlation Verification for Image Retrieval&lt;/li&gt;
&lt;li&gt;Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut&lt;/li&gt;
&lt;li&gt;Exploring Structure-Aware Transformer Over Interaction Proposals for Human-Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Decoupled Multi-Task Learning With Cyclical Self-Regulation for Face Parsing&lt;/li&gt;
&lt;li&gt;Glass: Geometric Latent Augmentation for Shape Spaces&lt;/li&gt;
&lt;li&gt;DPICT: Deep Progressive Image Compression Using Trit-Planes&lt;/li&gt;
&lt;li&gt;Text to Image Generation With Semantic-Spatial Aware GAN&lt;/li&gt;
&lt;li&gt;Generalizable Cross-Modality Medical Image Segmentation Via Style Augmentation and Dual Normalization&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model&lt;/li&gt;
&lt;li&gt;Interactive Segmentation and Visualization for Tiny Objects in Multi-Megapixel Images&lt;/li&gt;
&lt;li&gt;Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture&lt;/li&gt;
&lt;li&gt;Surface Representation for Point Clouds&lt;/li&gt;
&lt;li&gt;Implicit Motion Handling for Video Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides&lt;/li&gt;
&lt;li&gt;Learning With Twin Noisy Labels for Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;Optical Flow Estimation for Spiking Camera&lt;/li&gt;
&lt;li&gt;GradViT: Gradient Inversion of Vision Transformers&lt;/li&gt;
&lt;li&gt;Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution Via Cycle-Projected Mutual Learning&lt;/li&gt;
&lt;li&gt;Joint Global and Local Hierarchical Priors for Learned Image Compression&lt;/li&gt;
&lt;li&gt;Knowledge Distillation Via The Target-Aware Transformer&lt;/li&gt;
&lt;li&gt;Subspace Adversarial Training&lt;/li&gt;
&lt;li&gt;3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection&lt;/li&gt;
&lt;li&gt;Image Segmentation Using Text and Image Prompts&lt;/li&gt;
&lt;li&gt;AutoMine: An Unmanned Mine Dataset&lt;/li&gt;
&lt;li&gt;Background Activation Suppression for Weakly Supervised Object Localization&lt;/li&gt;
&lt;li&gt;Synthetic Generation of Face Videos With Plethysmograph Physiology&lt;/li&gt;
&lt;li&gt;Hallucinated Neural Radiance Fields in The Wild&lt;/li&gt;
&lt;li&gt;Global Tracking Transformers&lt;/li&gt;
&lt;li&gt;Backdoor Attacks on Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;GMFlow: Learning Optical Flow Via Global Matching&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Explore Spatio-Temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline&lt;/li&gt;
&lt;li&gt;Graph-Based Spatial Transformer With Memory Replay for Multi-Future Pedestrian Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Scanline Homographies for Rolling-Shutter Plane Absolute Pose&lt;/li&gt;
&lt;li&gt;AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-Time Image Enhancement&lt;/li&gt;
&lt;li&gt;Recurrent Glimpse-Based Decoder for Detection With Transformer&lt;/li&gt;
&lt;li&gt;SimMIM: A Simple Framework for Masked Image Modeling&lt;/li&gt;
&lt;li&gt;Label Matching Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;RegionCLIP: Region-Based Language-Image Pretraining&lt;/li&gt;
&lt;li&gt;Video Frame Interpolation Transformer&lt;/li&gt;
&lt;li&gt;BCOT: A Markerless High-Precision 3D Object Tracking Benchmark&lt;/li&gt;
&lt;li&gt;Omni-DETR: Omni-Supervised Object Detection With Transformers&lt;/li&gt;
&lt;li&gt;Transferable Sparse Adversarial Attack&lt;/li&gt;
&lt;li&gt;CREAM: Weakly Supervised Object Localization Via Class RE-Activation Mapping&lt;/li&gt;
&lt;li&gt;VALHALLA: Visual Hallucination for Machine Translation&lt;/li&gt;
&lt;li&gt;HINT: Hierarchical Neuron Concept Explainer&lt;/li&gt;
&lt;li&gt;Neural Face Identification in A 2D Wireframe Projection of A Manifold Object&lt;/li&gt;
&lt;li&gt;Nonuniform-to-Uniform Quantization: Towards Accurate Quantization Via Generalized Straight-Through Estimation&lt;/li&gt;
&lt;li&gt;An Empirical Study of End-to-End Temporal Action Detection&lt;/li&gt;
&lt;li&gt;Object Localization Under Single Coarse Point Supervision&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Accurate Siamese Tracking&lt;/li&gt;
&lt;li&gt;Non-Parametric Depth Distribution Modelling Based Depth Inference for Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Equalized Focal Loss for Dense Long-Tailed Object Detection&lt;/li&gt;
&lt;li&gt;DeepDPM: Deep Clustering With An Unknown Number of Clusters&lt;/li&gt;
&lt;li&gt;ISDNet: Integrating Shallow and Deep Networks for Efficient Ultra-High Resolution Segmentation&lt;/li&gt;
&lt;li&gt;Unsupervised Domain Adaptation for Nighttime Aerial Tracking&lt;/li&gt;
&lt;li&gt;RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs&lt;/li&gt;
&lt;li&gt;Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction&lt;/li&gt;
&lt;li&gt;A Variational Bayesian Method for Similarity Learning in Non-Rigid Image Registration&lt;/li&gt;
&lt;li&gt;Not Just Selection, But Exploration: Online Class-Incremental Continual Learning Via Dual View Consistency&lt;/li&gt;
&lt;li&gt;Coupling Vision and Proprioception for Navigation of Legged Robots&lt;/li&gt;
&lt;li&gt;Exploiting Rigidity Constraints for LiDAR Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;EMOCA: Emotion Driven Monocular Face Capture and Animation&lt;/li&gt;
&lt;li&gt;Quarantine: Sparsity Can Uncover The Trojan Attack Trigger for Free&lt;/li&gt;
&lt;li&gt;AlignQ: Alignment Quantization With ADMM-Based Correlation Preservation&lt;/li&gt;
&lt;li&gt;Interactive Multi-Class Tiny-Object Detection&lt;/li&gt;
&lt;li&gt;Learning From Pixel-Level Noisy Label: A New Perspective for Light Field Saliency Detection&lt;/li&gt;
&lt;li&gt;Multi-View Depth Estimation By Fusing Single-View Depth Probability With Multi-View Geometry&lt;/li&gt;
&lt;li&gt;Slimmable Domain Adaptation&lt;/li&gt;
&lt;li&gt;High-Resolution Image Harmonization Via Collaborative Dual Transformations&lt;/li&gt;
&lt;li&gt;MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Supervised Neural Articulated Shape and Appearance Models&lt;/li&gt;
&lt;li&gt;Topology Preserving Local Road Network Estimation From Single Onboard Camera Image&lt;/li&gt;
&lt;li&gt;Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes&lt;/li&gt;
&lt;li&gt;SwinTextSpotter: Scene Text Spotting Via Better Synergy Between Text Detection and Text Recognition&lt;/li&gt;
&lt;li&gt;Deblur-NeRF: Neural Radiance Fields From Blurry Images&lt;/li&gt;
&lt;li&gt;Whose Track Is It Anyway? Improving Robustness to Tracking Errors With Affinity-Based Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation&lt;/li&gt;
&lt;li&gt;Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning&lt;/li&gt;
&lt;li&gt;Blind Image Super-Resolution With Elaborate Degradation Modeling on Noise and Kernel&lt;/li&gt;
&lt;li&gt;Faithful Extreme Rescaling Via Generative Prior Reciprocated Invertible Representations&lt;/li&gt;
&lt;li&gt;Proto2Proto: Can You Recognize The Car, The Way I Do?&lt;/li&gt;
&lt;li&gt;TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing&lt;/li&gt;
&lt;li&gt;Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Habitat-Web: Learning Embodied Object-Search Strategies From Human Demonstrations at Scale&lt;/li&gt;
&lt;li&gt;Simple But Effective: CLIP Embeddings for Embodied AI&lt;/li&gt;
&lt;li&gt;NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition&lt;/li&gt;
&lt;li&gt;Collaborative Transformers for Grounded Situation Recognition&lt;/li&gt;
&lt;li&gt;CPPF: Towards Robust Category-Level 9D Pose Estimation in The Wild&lt;/li&gt;
&lt;li&gt;Continual Test-Time Domain Adaptation&lt;/li&gt;
&lt;li&gt;Dynamic MLP for Fine-Grained Image Classification By Leveraging Geographical and Temporal Information&lt;/li&gt;
&lt;li&gt;MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering&lt;/li&gt;
&lt;li&gt;Fair Contrastive Learning for Facial Attribute Classification&lt;/li&gt;
&lt;li&gt;Directional Self-Supervised Learning for Heavy Image Augmentations&lt;/li&gt;
&lt;li&gt;No-Reference Point Cloud Quality Assessment Via Domain Adaptation&lt;/li&gt;
&lt;li&gt;Comprehending and Ordering Semantics for Image Captioning&lt;/li&gt;
&lt;li&gt;A Large-Scale Comprehensive Dataset and Copy-Overlap Aware Evaluation Protocol for Segment-Level Video Copy Detection&lt;/li&gt;
&lt;li&gt;Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification&lt;/li&gt;
&lt;li&gt;HeadNeRF: A Real-Time NeRF-Based Parametric Head Model&lt;/li&gt;
&lt;li&gt;Occlusion-Robust Face Alignment Using A Viewpoint-Invariant Hierarchical Network Architecture&lt;/li&gt;
&lt;li&gt;IDR: Self-Supervised Image Denoising Via Iterative Data Refinement&lt;/li&gt;
&lt;li&gt;MogFace: Towards A Deeper Appreciation on Face Detection&lt;/li&gt;
&lt;li&gt;Learning Affinity From Attention: End-to-End Weakly-Supervised Semantic Segmentation With Transformers&lt;/li&gt;
&lt;li&gt;CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos&lt;/li&gt;
&lt;li&gt;Learning To Detect Mobile Objects From LiDAR Scans Without Labels&lt;/li&gt;
&lt;li&gt;WildNet: Learning Domain Generalized Semantic Segmentation From The Wild&lt;/li&gt;
&lt;li&gt;DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection&lt;/li&gt;
&lt;li&gt;Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Generating Diverse 3D Reconstructions From A Single Occluded Face Image&lt;/li&gt;
&lt;li&gt;Stand-Alone Inter-Frame Attention in Video Models&lt;/li&gt;
&lt;li&gt;Large-Scale Pre-Training for Person Re-Identification With Noisy Labels&lt;/li&gt;
&lt;li&gt;Semantic Segmentation By Early Region Proxy&lt;/li&gt;
&lt;li&gt;LD-ConGR: A Large RGB-D Video Dataset for Long-Distance Continuous Gesture Recognition&lt;/li&gt;
&lt;li&gt;HVH: Learning A Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture&lt;/li&gt;
&lt;li&gt;Rethinking Visual Geo-Localization for Large-Scale Applications&lt;/li&gt;
&lt;li&gt;The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy&lt;/li&gt;
&lt;li&gt;ViM: Out-of-Distribution With Virtual-Logit Matching&lt;/li&gt;
&lt;li&gt;Class-Aware Contrastive Semi-Supervised Learning&lt;/li&gt;
&lt;li&gt;Ditto: Building Digital Twins of Articulated Objects From Interaction&lt;/li&gt;
&lt;li&gt;Adaptive Early-Learning Correction for Segmentation From Noisy Annotations&lt;/li&gt;
&lt;li&gt;Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation&lt;/li&gt;
&lt;li&gt;RSTT: Real-Time Spatial Temporal Transformer for Space-Time Video Super-Resolution&lt;/li&gt;
&lt;li&gt;Partial Class Activation Attention for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Multi-Scale Memory-Based Video Deblurring&lt;/li&gt;
&lt;li&gt;A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching&lt;/li&gt;
&lt;li&gt;Geometric Structure Preserving Warp for Natural Image Stitching&lt;/li&gt;
&lt;li&gt;GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping&lt;/li&gt;
&lt;li&gt;Conditional Prompt Learning for Vision-Language Models&lt;/li&gt;
&lt;li&gt;Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;Undoing The Damage of Label Shift for Cross-Domain Semantic Segmentation&lt;/li&gt;
&lt;li&gt;FisherMatch: Semi-Supervised Rotation Regression Via Entropy-Based Filtering&lt;/li&gt;
&lt;li&gt;Affine Medical Image Registration With Coarse-To-Fine Vision Transformer&lt;/li&gt;
&lt;li&gt;A Differentiable Two-Stage Alignment Scheme for Burst Image Reconstruction With Large Shift&lt;/li&gt;
&lt;li&gt;Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes&lt;/li&gt;
&lt;li&gt;Restormer: Efficient Transformer for High-Resolution Image Restoration&lt;/li&gt;
&lt;li&gt;IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation&lt;/li&gt;
&lt;li&gt;Large Loss Matters in Weakly Supervised Multi-Label Classification&lt;/li&gt;
&lt;li&gt;Neural Inertial Localization&lt;/li&gt;
&lt;li&gt;GraftNet: Towards Domain Generalized Stereo Matching With A Broad-Spectrum and Task-Oriented Feature&lt;/li&gt;
&lt;li&gt;VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Catching Both Gray and Black Swans: Open-Set Supervised Anomaly Detection&lt;/li&gt;
&lt;li&gt;MLSLT: Towards Multilingual Sign Language Translation&lt;/li&gt;
&lt;li&gt;Towards An End-to-End Framework for Flow-Guided Video Inpainting&lt;/li&gt;
&lt;li&gt;Contrastive Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;MotionAug: Augmentation With Physical Correction for Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Modeling Indirect Illumination for Inverse Rendering&lt;/li&gt;
&lt;li&gt;TransWeather: Transformer-Based Restoration of Images Degraded By Adverse Weather Conditions&lt;/li&gt;
&lt;li&gt;H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection&lt;/li&gt;
&lt;li&gt;P3Depth: Monocular Depth Estimation With A Piecewise Planarity Prior&lt;/li&gt;
&lt;li&gt;GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection&lt;/li&gt;
&lt;li&gt;Simple Multi-Dataset Detection&lt;/li&gt;
&lt;li&gt;Proactive Image Manipulation Detection&lt;/li&gt;
&lt;li&gt;StyTr2: Image Style Transfer With Transformers&lt;/li&gt;
&lt;li&gt;Global Matching With Overlapping Attention for Optical Flow Estimation&lt;/li&gt;
&lt;li&gt;Language As Queries for Referring Video Object Segmentation&lt;/li&gt;
&lt;li&gt;MViTv2: Improved Multiscale Vision Transformers for Classification and Detection&lt;/li&gt;
&lt;li&gt;Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language&lt;/li&gt;
&lt;li&gt;Rethinking Efficient Lane Detection Via Curve Modeling&lt;/li&gt;
&lt;li&gt;Self-Supervised Arbitrary-Scale Point Clouds Upsampling Via Implicit Neural Representation&lt;/li&gt;
&lt;li&gt;Co-Advise: Cross Inductive Bias Distillation&lt;/li&gt;
&lt;li&gt;AdaMixer: A Fast-Converging Query-Based Object Detector&lt;/li&gt;
&lt;li&gt;DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification&lt;/li&gt;
&lt;li&gt;BEVT: BERT Pretraining of Video Transformers&lt;/li&gt;
&lt;li&gt;Deep Generalized Unfolding Networks for Image Restoration&lt;/li&gt;
&lt;li&gt;VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;Deep Unlearning Via Randomized Conditionally Independent Hessians&lt;/li&gt;
&lt;li&gt;Revisiting Skeleton-Based Action Recognition&lt;/li&gt;
&lt;li&gt;Stereo Depth From Events Cameras: Concentrate and Focus on The Future&lt;/li&gt;
&lt;li&gt;A Simple Data Mixing Prior for Improving Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;Knowledge Distillation As Efficient Pre-Training: Faster Convergence, Higher Data-Efficiency, and Better Transferability&lt;/li&gt;
&lt;li&gt;BigDL 2.0: Seamless Scaling of AI Pipelines From Laptops to Distributed Cluster&lt;/li&gt;
&lt;li&gt;Attentive Fine-Grained Structured Sparsity for Image Restoration&lt;/li&gt;
&lt;li&gt;Learning Fair Classifiers With Partially Annotated Group Labels&lt;/li&gt;
&lt;li&gt;NightLab: A Dual-Level Architecture With Hardness Detection for Segmentation at Night&lt;/li&gt;
&lt;li&gt;Constrained Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Threshold Matters in WSSS: Manipulating The Activation for The Robust and Accurate Segmentation Model Against Thresholds&lt;/li&gt;
&lt;li&gt;TransMVSNet: Global Context-Aware Multi-View Stereo Network With Transformers&lt;/li&gt;
&lt;li&gt;DPGEN: Differentially Private Generative Energy-Guided Network for Natural Image Synthesis&lt;/li&gt;
&lt;li&gt;The Majority Can Help The Minority: Context-Rich Minority Oversampling for Long-Tailed Classification&lt;/li&gt;
&lt;li&gt;IntentVizor: Towards Generic Query Guided Interactive Video Summarization&lt;/li&gt;
&lt;li&gt;Shape-Invariant 3D Adversarial Point Clouds&lt;/li&gt;
&lt;li&gt;Bootstrapping ViTs: Towards Liberating Vision Transformers From Pre-Training&lt;/li&gt;
&lt;li&gt;PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents&lt;/li&gt;
&lt;li&gt;Meta-Attention for ViT-Backed Continual Learning&lt;/li&gt;
&lt;li&gt;DST: Dynamic Substitute Training for Data-Free Black-Box Attack&lt;/li&gt;
&lt;li&gt;Unified Contrastive Learning in Image-Text-Label Space&lt;/li&gt;
&lt;li&gt;Unsupervised Pre-Training for Temporal Action Localization Tasks&lt;/li&gt;
&lt;li&gt;Look Outside The Room: Synthesizing A Consistent Long-Term 3D Scene Video From A Single Image&lt;/li&gt;
&lt;li&gt;High-Fidelity Human Avatars From A Single RGB Camera&lt;/li&gt;
&lt;li&gt;Multiview Transformers for Video Recognition&lt;/li&gt;
&lt;li&gt;How Good Is Aesthetic Ability of A Fashion Model?&lt;/li&gt;
&lt;li&gt;Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds&lt;/li&gt;
&lt;li&gt;Sequential Voting With Relational Box Fields for Active Object Detection&lt;/li&gt;
&lt;li&gt;Semantic-Aware Auto-Encoders for Self-Supervised Representation Learning&lt;/li&gt;
&lt;li&gt;Consistency Learning Via Decoding Path Augmentation for Transformers in Human Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Consistent Explanations By Contrastive Learning&lt;/li&gt;
&lt;li&gt;Hierarchical Modular Network for Video Captioning&lt;/li&gt;
&lt;li&gt;Depth Estimation By Combining Binocular Stereo and Monocular Structured-Light&lt;/li&gt;
&lt;li&gt;Salient-to-Broad Transition for Video Person Re-Identification&lt;/li&gt;
&lt;li&gt;DeeCap: Dynamic Early Exiting for Efficient Image Captioning&lt;/li&gt;
&lt;li&gt;RepMLPNet: Hierarchical Vision MLP With Re-Parameterized Locality&lt;/li&gt;
&lt;li&gt;DR.VIC: Decomposition and Reasoning for Video Individual Counting&lt;/li&gt;
&lt;li&gt;ARCS: Accurate Rotation and Correspondence Search&lt;/li&gt;
&lt;li&gt;Learning To Anticipate Future With Dynamic Context Removal&lt;/li&gt;
&lt;li&gt;GCFSR: A Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors&lt;/li&gt;
&lt;li&gt;On The Integration of Self-Attention and Convolution&lt;/li&gt;
&lt;li&gt;Domain Adaptation on Point Clouds Via Geometry-Aware Implicits&lt;/li&gt;
&lt;li&gt;GroupViT: Semantic Segmentation Emerges From Text Supervision&lt;/li&gt;
&lt;li&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/li&gt;
&lt;li&gt;BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks Via Image Quantization and Contrastive Adversarial Learning&lt;/li&gt;
&lt;li&gt;Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation&lt;/li&gt;
&lt;li&gt;Towards Better Plasticity-Stability Trade-Off in Incremental Learning: A Simple Linear Connector&lt;/li&gt;
&lt;li&gt;Topology-Preserving Shape Reconstruction and Registration Via Neural Diffeomorphic Flow&lt;/li&gt;
&lt;li&gt;Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection&lt;/li&gt;
&lt;li&gt;MAXIM: Multi-Axis MLP for Image Processing&lt;/li&gt;
&lt;li&gt;Learning Part Segmentation Through Unsupervised Domain Adaptation From Synthetic Vehicles&lt;/li&gt;
&lt;li&gt;PSTR: End-to-End One-Step Person Search With Transformers&lt;/li&gt;
&lt;li&gt;NFormer: Robust Person Re-Identification With Neighbor Transformer&lt;/li&gt;
&lt;li&gt;Bridging Global Context Interactions for High-Fidelity Image Completion&lt;/li&gt;
&lt;li&gt;SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning&lt;/li&gt;
&lt;li&gt;Not All Tokens Are Equal: Human-Centric Visual Analysis Via Token Clustering Transformer&lt;/li&gt;
&lt;li&gt;Temporally Efficient Vision Transformer for Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;The Devil Is in The Margin: Margin-Based Label Smoothing for Network Calibration&lt;/li&gt;
&lt;li&gt;NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks&lt;/li&gt;
&lt;li&gt;WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation&lt;/li&gt;
&lt;li&gt;Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding&lt;/li&gt;
&lt;li&gt;E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition&lt;/li&gt;
&lt;li&gt;OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization&lt;/li&gt;
&lt;li&gt;OnePose: One-Shot Object Pose Estimation Without CAD Models&lt;/li&gt;
&lt;li&gt;Rethinking Minimal Sufficient Representation in Contrastive Learning&lt;/li&gt;
&lt;li&gt;Scalable Penalized Regression for Noise Detection in Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;Federated Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Show, Deconfound and Tell: Image Captioning With Causal Inference&lt;/li&gt;
&lt;li&gt;MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image&lt;/li&gt;
&lt;li&gt;Parameter-Free Online Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;SIGMA: Semantic-Complete Graph Matching for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;No Pain, Big Gain: Classify Dynamic Point Cloud Sequences With Static Models By Fitting Feature-Level Space-Time Surfaces&lt;/li&gt;
&lt;li&gt;HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging&lt;/li&gt;
&lt;li&gt;Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space&lt;/li&gt;
&lt;li&gt;Learning To Estimate Robust 3D Human Mesh From In-the-Wild Crowded Scenes&lt;/li&gt;
&lt;li&gt;Detecting Deepfakes With Self-Blended Images&lt;/li&gt;
&lt;li&gt;Implicit Sample Extension for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;Energy-Based Latent Aligner for Incremental Learning&lt;/li&gt;
&lt;li&gt;Towards Semi-Supervised Deep Facial Expression Recognition With An Adaptive Confidence Margin&lt;/li&gt;
&lt;li&gt;Group R-CNN for Weakly Semi-Supervised Object Detection With Points&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Action Transition Learning for Stochastic Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Hybrid Relation Guided Set Matching for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Cross-Patch Dense Contrastive Learning for Semi-Supervised Segmentation of Cellular Nuclei in Histopathologic Images&lt;/li&gt;
&lt;li&gt;Generalized Binary Search Network for Highly-Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation&lt;/li&gt;
&lt;li&gt;FlexIT: Towards Flexible Semantic Image Translation&lt;/li&gt;
&lt;li&gt;CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow&lt;/li&gt;
&lt;li&gt;BoxeR: Box-Attention for 2D and 3D Transformers&lt;/li&gt;
&lt;li&gt;Neural Architecture Search With Representation Mutual Information&lt;/li&gt;
&lt;li&gt;Can Neural Nets Learn The Same Model Twice? Investigating Reproducibility and Double Descent From The Decision Boundary Perspective&lt;/li&gt;
&lt;li&gt;Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction&lt;/li&gt;
&lt;li&gt;Multi-View Transformer for 3D Visual Grounding&lt;/li&gt;
&lt;li&gt;Structured Sparse R-CNN for Direct Scene Graph Generation&lt;/li&gt;
&lt;li&gt;BARC: Learning To Regress 3D Dog Shape From Images By Exploiting Breed Information&lt;/li&gt;
&lt;li&gt;PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models&lt;/li&gt;
&lt;li&gt;Towards Understanding Adversarial Robustness of Optical Flow Networks&lt;/li&gt;
&lt;li&gt;Lifelong Graph Learning&lt;/li&gt;
&lt;li&gt;Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning&lt;/li&gt;
&lt;li&gt;Computing Wasserstein-p Distance Between Images With Linear Cost&lt;/li&gt;
&lt;li&gt;Unsupervised Representation Learning for Binary Networks By Joint Classifier Learning&lt;/li&gt;
&lt;li&gt;Large-Scale Video Panoptic Segmentation in The Wild: A Benchmark&lt;/li&gt;
&lt;li&gt;GrainSpace: A Large-Scale Dataset for Fine-Grained and Domain-Adaptive Recognition of Cereal Grains&lt;/li&gt;
&lt;li&gt;Learning Modal-Invariant and Temporal-Memory for Video-Based Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Oriented RepPoints for Aerial Object Detection&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning&lt;/li&gt;
&lt;li&gt;Low-Resource Adaptation for Personalized Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Task-Specific Inconsistency Alignment for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;MS2DG-Net: Progressive Correspondence Learning Via Multiple Sparse Semantics Dynamic Graph&lt;/li&gt;
&lt;li&gt;Learning To Listen: Modeling Non-Deterministic Dyadic Facial Motion&lt;/li&gt;
&lt;li&gt;Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation From Monocular Video&lt;/li&gt;
&lt;li&gt;MixFormer: End-to-End Tracking With Iterative Mixed Attention&lt;/li&gt;
&lt;li&gt;Plenoxels: Radiance Fields Without Neural Networks&lt;/li&gt;
&lt;li&gt;Selective-Supervised Contrastive Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;SimT: Handling Open-Set Noise for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Frequency-Driven Imperceptible Adversarial Attack on Semantic Similarity&lt;/li&gt;
&lt;li&gt;Video Demoireing With Relation-Based Temporal Consistency&lt;/li&gt;
&lt;li&gt;Industrial Style Transfer With Large-Scale Geometric Warping and Content Preservation&lt;/li&gt;
&lt;li&gt;Modeling Image Composition for Complex Scene Generation&lt;/li&gt;
&lt;li&gt;Decoupling Zero-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions&lt;/li&gt;
&lt;li&gt;Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting The Adversarial Transferability&lt;/li&gt;
&lt;li&gt;IFOR: Iterative Flow Minimization for Robotic Object Rearrangement&lt;/li&gt;
&lt;li&gt;Zero Experience Required: Plug &amp;amp; Play Modular Transfer Learning for Semantic Visual Navigation&lt;/li&gt;
&lt;li&gt;TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation&lt;/li&gt;
&lt;li&gt;The Wanderings of Odysseus in 3D Scenes&lt;/li&gt;
&lt;li&gt;All-in-One Image Restoration for Unknown Corruption&lt;/li&gt;
&lt;li&gt;PUMP: Pyramidal and Uniqueness Matching Priors for Unsupervised Learning of Local Descriptors&lt;/li&gt;
&lt;li&gt;MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video&lt;/li&gt;
&lt;li&gt;RCP: Recurrent Closest Point for Point Cloud&lt;/li&gt;
&lt;li&gt;A Dual Weighting Label Assignment Scheme for Object Detection&lt;/li&gt;
&lt;li&gt;Hyperbolic Vision Transformers: Combining Improvements in Metric Learning&lt;/li&gt;
&lt;li&gt;Instance-Aware Dynamic Neural Network Quantization&lt;/li&gt;
&lt;li&gt;Exploring Effective Data for Surrogate Training Towards Black-Box Attack&lt;/li&gt;
&lt;li&gt;JRDB-Act: A Large-Scale Dataset for Spatio-Temporal Action, Social Group and Activity Detection&lt;/li&gt;
&lt;li&gt;Investigating Top-k White-Box and Transferable Black-Box Attack&lt;/li&gt;
&lt;li&gt;Decoupling and Recoupling Spatiotemporal Representation for RGB-D-Based Motion Recognition&lt;/li&gt;
&lt;li&gt;A Self-Supervised Descriptor for Image Copy Detection&lt;/li&gt;
&lt;li&gt;Negative-Aware Attention Framework for Image-Text Matching&lt;/li&gt;
&lt;li&gt;An Image Patch Is A Wave: Phase-Aware Vision MLP&lt;/li&gt;
&lt;li&gt;Shunted Self-Attention Via Multi-Scale Token Aggregation&lt;/li&gt;
&lt;li&gt;Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression&lt;/li&gt;
&lt;li&gt;Recurrent Variational Network: A Deep Learning Inverse Problem Solver Applied to The Task of Accelerated MRI Reconstruction&lt;/li&gt;
&lt;li&gt;Surpassing The Human Accuracy: Detecting Gallbladder Cancer From USG Images With Curriculum Learning&lt;/li&gt;
&lt;li&gt;Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond&lt;/li&gt;
&lt;li&gt;TrackFormer: Multi-Object Tracking With Transformers&lt;/li&gt;
&lt;li&gt;3D Shape Reconstruction From 2D Images With Disentangled Attribute Flow&lt;/li&gt;
&lt;li&gt;Feature Statistics Mixing Regularization for Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;OpenTAL: Towards Open Set Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection&lt;/li&gt;
&lt;li&gt;Ego4D: Around The World in 3,000 Hours of Egocentric Video&lt;/li&gt;
&lt;li&gt;Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Weakly Supervised Semantic Segmentation Using Out-of-Distribution Data&lt;/li&gt;
&lt;li&gt;DAD-3DHeads: A Large-Scale Dense, Accurate and Diverse Dataset for 3D Head Alignment From A Single Image&lt;/li&gt;
&lt;li&gt;Reconstructing Surfaces for Sparse Point Clouds With On-Surface Priors&lt;/li&gt;
&lt;li&gt;VCLIMB: A Novel Video Class Incremental Learning Benchmark&lt;/li&gt;
&lt;li&gt;Robust Equivariant Imaging: A Fully Unsupervised Framework for Learning To Image From Noisy and Partial Measurements&lt;/li&gt;
&lt;li&gt;ST++: Make Self-Training Work Better for Semi-Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Interacting Attention Graph for Single Image Two-Hand Reconstruction&lt;/li&gt;
&lt;li&gt;Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task&lt;/li&gt;
&lt;li&gt;Cross-Image Relational Knowledge Distillation for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Towards Layer-Wise Image Vectorization&lt;/li&gt;
&lt;li&gt;Scenic: A JAX Library for Computer Vision Research and Beyond&lt;/li&gt;
&lt;li&gt;Real-Time Object Detection for Streaming Perception&lt;/li&gt;
&lt;li&gt;VisualHow: Multimodal Problem Solving&lt;/li&gt;
&lt;li&gt;Spatial Commonsense Graph for Object Localisation in Partial Scenes&lt;/li&gt;
&lt;li&gt;OSSGAN: Open-Set Semi-Supervised Image Generation&lt;/li&gt;
&lt;li&gt;Bi-Level Alignment for Cross-Domain Crowd Counting&lt;/li&gt;
&lt;li&gt;ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation&lt;/li&gt;
&lt;li&gt;Efficient Multi-View Stereo By Iterative Dynamic Cost Volume&lt;/li&gt;
&lt;li&gt;TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing&lt;/li&gt;
&lt;li&gt;Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework&lt;/li&gt;
&lt;li&gt;SGTR: End-to-End Scene Graph Generation With Transformer&lt;/li&gt;
&lt;li&gt;Decoupled Knowledge Distillation&lt;/li&gt;
&lt;li&gt;DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection&lt;/li&gt;
&lt;li&gt;Reusing The Task-Specific Classifier As A Discriminator: Discriminator-Free Adversarial Domain Adaptation&lt;/li&gt;
&lt;li&gt;Show Me What and Tell Me How: Video Synthesis Via Multimodal Conditioning&lt;/li&gt;
&lt;li&gt;SIMBAR: Single Image-Based Scene Relighting for Effective Data Augmentation for Automated Driving Vision Tasks&lt;/li&gt;
&lt;li&gt;Multi-Label Classification With Partial Annotations Using Class-Aware Selective Loss&lt;/li&gt;
&lt;li&gt;CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings&lt;/li&gt;
&lt;li&gt;IntraQ: Learning Synthetic Images With Intra-Class Heterogeneity for Zero-Shot Network Quantization&lt;/li&gt;
&lt;li&gt;I M Avatar: Implicit Morphable Head Avatars From Videos&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Metric Learning With Cross-Module Communications for The Classification of Anterior Chamber Angle Images&lt;/li&gt;
&lt;li&gt;A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Multi-Modal Dynamic Graph Transformer for Visual Grounding&lt;/li&gt;
&lt;li&gt;Geometric Transformer for Fast and Robust Point Cloud Registration&lt;/li&gt;
&lt;li&gt;UMT: Unified Multi-Modal Transformers for Joint Video Moment Retrieval and Highlight Detection&lt;/li&gt;
&lt;li&gt;Demystifying The Neural Tangent Kernel From A Practical Perspective: Can It Be Trusted for Neural Architecture Search Without Training?&lt;/li&gt;
&lt;li&gt;The Devil Is in The Details: Window-Based Attention for Image Compression&lt;/li&gt;
&lt;li&gt;DiLiGenT102: A Photometric Stereo Benchmark Dataset With Controlled Shape and Material Variation&lt;/li&gt;
&lt;li&gt;PolyWorld: Polygonal Building Extraction With Graph Neural Networks in Satellite Images&lt;/li&gt;
&lt;li&gt;Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Spatio-Temporal Relation Modeling for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Multi-Person Extreme Motion Prediction&lt;/li&gt;
&lt;li&gt;B-DARTS: Beta-Decay Regularization for Differentiable Architecture Search&lt;/li&gt;
&lt;li&gt;CMT: Convolutional Neural Networks Meet Vision Transformers&lt;/li&gt;
&lt;li&gt;KNN Local Attention for Image Restoration&lt;/li&gt;
&lt;li&gt;Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered By Pre-Trained Vision-Language Model&lt;/li&gt;
&lt;li&gt;TransMix: Attend To Mix for Vision Transformers&lt;/li&gt;
&lt;li&gt;Inertia-Guided Flow Completion and Style Fusion for Video Inpainting&lt;/li&gt;
&lt;li&gt;Long-Tailed Visual Recognition Via Gaussian Clouded Logit Adjustment&lt;/li&gt;
&lt;li&gt;Image Animation With Perturbed Masks&lt;/li&gt;
&lt;li&gt;Domain Generalization Via Shuffled Style Assembly for Face Anti-Spoofing&lt;/li&gt;
&lt;li&gt;OcclusionFusion: Occlusion-Aware Motion Estimation for Real-Time Dynamic 3D Reconstruction&lt;/li&gt;
&lt;li&gt;MonoScene: Monocular 3D Semantic Scene Completion&lt;/li&gt;
&lt;li&gt;AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition&lt;/li&gt;
&lt;li&gt;Continuous Scene Representations for Embodied AI&lt;/li&gt;
&lt;li&gt;Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds&lt;/li&gt;
&lt;li&gt;Non-Probability Sampling Network for Stochastic Human Trajectory Prediction&lt;/li&gt;
&lt;li&gt;ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning&lt;/li&gt;
&lt;li&gt;Human-Aware Object Placement for Visual Environment Reconstruction&lt;/li&gt;
&lt;li&gt;X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval&lt;/li&gt;
&lt;li&gt;RAMA: A Rapid Multicut Algorithm on GPU&lt;/li&gt;
&lt;li&gt;Adversarial Parametric Pose Prior&lt;/li&gt;
&lt;li&gt;Mask Transfiner for High-Quality Instance Segmentation&lt;/li&gt;
&lt;li&gt;It Is Okay To Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning By Contrastive Data Collection&lt;/li&gt;
&lt;li&gt;DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Event-Based Video Reconstruction Via Potential-Assisted Spiking Neural Network&lt;/li&gt;
&lt;li&gt;YouMVOS: An Actor-Centric Multi-Shot Video Object Segmentation Dataset&lt;/li&gt;
&lt;li&gt;DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification&lt;/li&gt;
&lt;li&gt;Self-Supervised Video Transformer&lt;/li&gt;
&lt;li&gt;AutoRF: Learning 3D Object Radiance Fields From Single View Observations&lt;/li&gt;
&lt;li&gt;Coopernaut: End-to-End Driving With Cooperative Perception for Networked Vehicles&lt;/li&gt;
&lt;li&gt;TubeR: Tubelet Transformer for Video Action Detection&lt;/li&gt;
&lt;li&gt;MUM: Mix Image Tiles and UnMix Feature Tiles for Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;Learning Non-Target Knowledge for Few-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;UKPGAN: A General Self-Supervised Keypoint Detector&lt;/li&gt;
&lt;li&gt;Raw High-Definition Radar for Multi-Task Learning&lt;/li&gt;
&lt;li&gt;Coarse-To-Fine Feature Mining for Video Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Compressing Models With Few Samples: Mimicking Then Replacing&lt;/li&gt;
&lt;li&gt;PokeBNN: A Binary Pursuit of Lightweight Accuracy&lt;/li&gt;
&lt;li&gt;Zoom in and Out: A Mixed-Scale Triplet Network for Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images&lt;/li&gt;
&lt;li&gt;EMScore: Evaluating Video Captioning Via Coarse-Grained and Fine-Grained Embedding Matching&lt;/li&gt;
&lt;li&gt;PoseTriplet: Co-Evolving 3D Human Pose Estimation, Imitation, and Hallucination Under Self-Supervision&lt;/li&gt;
&lt;li&gt;Group Contextualization for Video Recognition&lt;/li&gt;
&lt;li&gt;Single-Domain Generalized Object Detection in Urban Scene Via Cyclic-Disentangled Self-Distillation&lt;/li&gt;
&lt;li&gt;L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Augmented Unpaired Image Dehazing Via Density and Depth Decomposition&lt;/li&gt;
&lt;li&gt;Neural 3D Video Synthesis From Multi-View Video&lt;/li&gt;
&lt;li&gt;SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search&lt;/li&gt;
&lt;li&gt;HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening&lt;/li&gt;
&lt;li&gt;Structure-Aware Flow Generation for Human Body Reshaping&lt;/li&gt;
&lt;li&gt;Learning To Answer Questions in Dynamic Audio-Visual Scenarios&lt;/li&gt;
&lt;li&gt;Synthetic Aperture Imaging With Events and Frames&lt;/li&gt;
&lt;li&gt;MonoGround: Detecting Monocular 3D Objects From The Ground&lt;/li&gt;
&lt;li&gt;Deep Visual Geo-Localization Benchmark&lt;/li&gt;
&lt;li&gt;StyleGAN-V: A Continuous Video Generator With The Price, Image Quality and Perks of StyleGAN2&lt;/li&gt;
&lt;li&gt;LISA: Learning Implicit Shape and Appearance of Hands&lt;/li&gt;
&lt;li&gt;Iterative Deep Homography Estimation&lt;/li&gt;
&lt;li&gt;Learned Queries for Efficient Local Attention&lt;/li&gt;
&lt;li&gt;Colar: Effective and Efficient Online Action Detection By Consulting Exemplars&lt;/li&gt;
&lt;li&gt;SoftGroup for 3D Instance Segmentation on Point Clouds&lt;/li&gt;
&lt;li&gt;MVS2D: Efficient Multi-View Stereo Via Attention-Driven 2D Convolutions&lt;/li&gt;
&lt;li&gt;Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation Via Semantic Knowledge Transfer and Self-Refinement&lt;/li&gt;
&lt;li&gt;Deep Constrained Least Squares for Blind Image Super-Resolution&lt;/li&gt;
&lt;li&gt;EDTER: Edge Detection With Transformer&lt;/li&gt;
&lt;li&gt;AirObject: A Temporally Evolving Graph Embedding for Object Identification&lt;/li&gt;
&lt;li&gt;From Representation to Reasoning: Towards Both Evidence and Commonsense Reasoning for Video Question-Answering&lt;/li&gt;
&lt;li&gt;Semantic-Aware Domain Generalized Segmentation&lt;/li&gt;
&lt;li&gt;DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion&lt;/li&gt;
&lt;li&gt;UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection&lt;/li&gt;
&lt;li&gt;AKB-48: A Real-World Articulated Object Knowledge Base&lt;/li&gt;
&lt;li&gt;Stratified Transformer for 3D Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Aug-NeRF: Training Stronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmentations&lt;/li&gt;
&lt;li&gt;Semantic-Shape Adaptive Feature Modulation for Semantic Image Synthesis&lt;/li&gt;
&lt;li&gt;Day-to-Night Image Synthesis for Training Nighttime Neural ISPs Literature ~Highlight: To address this problem, we propose a method that synthesizes nighttime images from daytime images.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;References 
    &lt;div id=&#34;references&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#references&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&#34; target=&#34;_blank&#34;&gt;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;&lt;br&gt;
Dr Hari Thapliyaal&lt;br&gt;
dasarpai.com&lt;br&gt;
linkedin.com/in/harithapliyal&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Comprehensive Glossary of LLM, Deep Learning, NLP, and CV Terminology</title>
      <link>http://localhost:1313/dsblog/Comprehensive-Glossary-of-LLM/</link>
      <pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Comprehensive-Glossary-of-LLM/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6089-Comprehensive-Glossary-of-LLM.jpg&#34; alt=&#34;Comprehensive Glossary of LLM&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Comprehensive Glossary of LLM 
    &lt;div id=&#34;comprehensive-glossary-of-llm&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#comprehensive-glossary-of-llm&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;I am developing this Glossary slowly at my own pace. Content on this page keep changing. Better definition, better explaination are part of my learing, my evolution and advancement in the field of Deep Learning and Machine Learning. As of Aug&#39;23 the terms are not in any order therefore if you are look for any specific term you can search on the page. When I will have 50+ terms on this page then I will try to sort them on some attribute of these terms.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Machine Learning Metrics</title>
      <link>http://localhost:1313/dsblog/Machine-Learning-Metrics/</link>
      <pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Machine-Learning-Metrics/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6092-Machine-Learning-Metrics.jpg&#34; alt=&#34;Comprehensive Glossary of LLM&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Machine Learning Metrics 
    &lt;div id=&#34;machine-learning-metrics&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#machine-learning-metrics&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In Machine Learning projects whether classical machine learning, deep learning, computer vision, speech processing, NLP, or any other ML project we keep building different models with different datasets. But how to know that for a particular problem model X is the best one? For that, we need to evaluate these models against certain metrics. What metrics we pick, depends upon the problem statement, data imbalance, type of data, etc. In this article, we will explore an exhaustive list of ML Metrics.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</title>
      <link>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg&#34; alt=&#34;Pretrained Language Models for Text Generation&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper Name :- Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt;&lt;br&gt;
Typer of Paper:- Survey Paper&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34; target=&#34;_blank&#34;&gt;Paper URL&lt;/a&gt;&lt;br&gt;
Paper title of the citations mentioned can be found at &lt;a href=&#34;../../dsblog/aip&#34;&gt;AI Papers with Heading&lt;/a&gt;. Use citation code to locate.&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary :- Pretrained Language Models for Text Generation 
    &lt;div id=&#34;paper-summary---pretrained-language-models-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary---pretrained-language-models-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Paper Outcome 
    &lt;div id=&#34;paper-outcome&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-outcome&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;General task deÔ¨Ånition&lt;/li&gt;
&lt;li&gt;Describe the mainstream architectures of PLMs for text generation.&lt;/li&gt;
&lt;li&gt;How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.&lt;/li&gt;
&lt;li&gt;Summarize several important Ô¨Åne-tuning strategies for text generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Ideas from the Paper 
    &lt;div id=&#34;ideas-from-the-paper&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ideas-from-the-paper&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Main Ideas 
    &lt;div id=&#34;main-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#main-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This paper discusses &amp;ldquo;major advances achieved in the topic of PLMs for text generation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;This survey aims to provide &amp;ldquo;text generation researchers a synthesis&amp;rdquo; and pointer to related research.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;General Ideas 
    &lt;div id=&#34;general-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#general-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).&lt;/li&gt;
&lt;li&gt;Neural generation model are deep learning models&lt;/li&gt;
&lt;li&gt;Pretrained language models (PLMs) are neural generation model&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Task Types and Typical Applications 
    &lt;div id=&#34;task-types-and-typical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#task-types-and-typical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )&lt;/li&gt;
&lt;li&gt;If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional
generation task(generate text without any constraint) &lt;a href=&#34;../../dsblog/aip#radford2019&#34;&gt;Radford2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or
attribute-based generation. X plays the role of guiding the text generation. &lt;a href=&#34;../../dsblog/aip#Keskar2019&#34;&gt;Keskar2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation &lt;a href=&#34;../../dsblog/aip#Li2021c&#34;&gt;Li2021c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as image, the task becomes image caption &lt;a href=&#34;../../dsblog/aip#Xia2020&#34;&gt;Xia2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as speech, the task become speech recognition &lt;a href=&#34;../../dsblog/aip#Fan2019&#34;&gt;Fan2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.&lt;/li&gt;
&lt;li&gt;Machine translation aims to translate text from one language into another language automatically &lt;a href=&#34;../../dsblog/aip#Conneau2019&#34;&gt;Conneau2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generating condensed summary of a long document &lt;a href=&#34;../../dsblog/aip#Zhang2019b&#34;&gt;Zhang2019b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dialogue system to converse with humans using natural language. &lt;a href=&#34;../../dsblog/aip#Wolf2019&#34;&gt;Wolf2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Architectures for Text Generation 
    &lt;div id=&#34;architectures-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#architectures-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.
&lt;ul&gt;
&lt;li&gt;MASS &lt;a href=&#34;../../dsblog/aip#song2019&#34;&gt;Song2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T5 &lt;a href=&#34;../../dsblog/aip#raffel2020&#34;&gt;Raffel2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BART &lt;a href=&#34;../../dsblog/aip#lewis2020&#34;&gt;Lewis2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.
&lt;ul&gt;
&lt;li&gt;GPT &lt;a href=&#34;../../dsblog/aip#radfordet2019&#34;&gt;Radfordet2019&lt;/a&gt;; &lt;a href=&#34;../../dsblog/aip#brown2020&#34;&gt;Brown2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CTRL [Keskar2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Modeling Different Data Types from Input 
    &lt;div id=&#34;modeling-different-data-types-from-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#modeling-different-data-types-from-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Unstructured Input 
    &lt;div id=&#34;unstructured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unstructured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]&lt;/li&gt;
&lt;li&gt;Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]&lt;/li&gt;
&lt;li&gt;Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]&lt;/li&gt;
&lt;li&gt;Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Structured Input 
    &lt;div id=&#34;structured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#structured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PLMs are not designed for structured or tabular data but for sequential text/data.&lt;/li&gt;
&lt;li&gt;Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]&lt;/li&gt;
&lt;li&gt;To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]&lt;/li&gt;
&lt;li&gt;Introduced an additional graph encoder to encode the input KG. [Li2021b]&lt;/li&gt;
&lt;li&gt;Template based method to serialize input table into text sequence. [Gong2020]
&lt;ul&gt;
&lt;li&gt;For example, the attribute-value pair ‚Äúname: jack reynolds‚Äù will be serialized as a sentence ‚Äúname is jack reynolds‚Äù. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]&lt;/li&gt;
&lt;li&gt;The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].&lt;/li&gt;
&lt;li&gt;Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Multimedia Input 
    &lt;div id=&#34;multimedia-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multimedia-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]&lt;/li&gt;
&lt;li&gt;Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]&lt;/li&gt;
&lt;li&gt;Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]&lt;/li&gt;
&lt;li&gt;Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020&lt;/li&gt;
&lt;li&gt;Image, video, speech recognition is hungry for human-transcripted supervised data.&lt;/li&gt;
&lt;li&gt;Integrate PLMs for weakly-supervised learning. For example,
&lt;ul&gt;
&lt;li&gt;Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Satisfying Special Properties for Output Text 
    &lt;div id=&#34;satisfying-special-properties-for-output-text&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#satisfying-special-properties-for-output-text&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.&lt;/li&gt;
&lt;li&gt;Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should
also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. - When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] - Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input
representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] - Approach for non-conditioned dialogue [Bao2020].&lt;/li&gt;
&lt;li&gt;Faithfulness. Means the content in generated text should not contradict the facts in input text.
&lt;ul&gt;
&lt;li&gt;PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.&lt;/li&gt;
&lt;li&gt;Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]&lt;/li&gt;
&lt;li&gt;With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.&lt;/li&gt;
&lt;li&gt;Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]&lt;/li&gt;
&lt;li&gt;Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.
&lt;ul&gt;
&lt;li&gt;When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.&lt;/li&gt;
&lt;li&gt;Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]
&lt;ul&gt;
&lt;li&gt;Extracted the word-pair alignment information from the source and target language,&lt;/li&gt;
&lt;li&gt;Aplied the extracted alignment information to enhance order-preserving.&lt;/li&gt;
&lt;li&gt;Translation across multiple languages, called multilingual machine translation [Conneau2019].&lt;/li&gt;
&lt;li&gt;mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]&lt;/li&gt;
&lt;li&gt;Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Summary from Introduction 
    &lt;div id=&#34;summary-from-introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#summary-from-introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].&lt;/li&gt;
&lt;li&gt;Machine translation generates text in a different language based on the source text [Yang2020a];&lt;/li&gt;
&lt;li&gt;Summarization generates an abridged version of the source text to include salient information [Guan2020].&lt;/li&gt;
&lt;li&gt;Text generation tasks based on
&lt;ul&gt;
&lt;li&gt;Recurrent neural networks (RNN) [Li2019],&lt;/li&gt;
&lt;li&gt;Convolutional neural networks (CNN) [Gehring2017],&lt;/li&gt;
&lt;li&gt;Graph neural networks (GNN) [Li2020],&lt;/li&gt;
&lt;li&gt;Attention mechanism [Bahdanau2015].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.&lt;/li&gt;
&lt;li&gt;Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.&lt;/li&gt;
&lt;li&gt;Deep neural networks usually have a large number of parameters to learn, which are likely to overÔ¨Åt on these small datasets and do not generalize well in practice.&lt;/li&gt;
&lt;li&gt;The idea behind PLMs is to Ô¨Årst pretrain the models in large-scale corpus and then Ô¨Ånetune these models in various downstream tasks to achieve
state-of-the-art results.&lt;/li&gt;
&lt;li&gt;PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.&lt;/li&gt;
&lt;li&gt;PLMs are generally beneÔ¨Åcial for downstream tasks and can avoid training a new model from scratch [Brown2020].&lt;/li&gt;
&lt;li&gt;A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Conclusion &amp;amp; Future Recommendations 
    &lt;div id=&#34;conclusion--future-recommendations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#conclusion--future-recommendations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Model Extension.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is LLM</title>
      <link>http://localhost:1313/dsblog/what-is-llm/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/what-is-llm/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6087-What-is-LLM.jpg&#34; alt=&#34;What is LLM&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is Large Language Model 
    &lt;div id=&#34;what-is-large-language-model&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-large-language-model&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;LLM stands for &lt;strong&gt;Large Language Model&lt;/strong&gt;. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>NLP Tasks</title>
      <link>http://localhost:1313/dsblog/nlp-tasks/</link>
      <pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/nlp-tasks/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6085-NLP-Tasks.jpg&#34; alt=&#34;NLP Tasks&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;NLP Tasks 
    &lt;div id=&#34;nlp-tasks&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#nlp-tasks&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Processing words of any language and driving some meaning from these is as old as the human language. Recently, AI momentum is taking on many of these language-processing tasks. Here is the summary of these NLP tasks, this list is continuously growing. Researchers keep creating a dataset for these tasks in different languages. Other researchers keep devising new ways to solve these tasks with better performance. They come up with a new architecture, a new set of hyperparameters, a new pipeline, etc. In summary, as of today, there are around 55 tasks. Hundreds of datasets and research papers exist around these. You can check on &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34;&gt;PaperWithCode&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/&#34; target=&#34;_blank&#34;&gt;Hggingface&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Model Tuning with VertexAI</title>
      <link>http://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6081-Model-Tuning-with-VertexAI.jpg&#34; alt=&#34;Model Tuning with VertexAI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Tuning Large Language Model with VertexAI 
    &lt;div id=&#34;tuning-large-language-model-with-vertexai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#tuning-large-language-model-with-vertexai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Why Model Tuning? 
    &lt;div id=&#34;why-model-tuning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#why-model-tuning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Tuning is required when you want the model to learn something niche or specific that deviates from general language patterns.&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Goal of Tuning 
    &lt;div id=&#34;goal-of-tuning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#goal-of-tuning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Classification 
    &lt;div id=&#34;classification&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#classification&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;p&gt;prompt: &amp;ldquo;Classify the following text into one of the following classes: [business, entertainment].&amp;rdquo;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Product and Services from Google, Azure and AWS</title>
      <link>http://localhost:1313/dsblog/AI-Product-and-Services-from-Google-Azure-and-AWS/</link>
      <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-Product-and-Services-from-Google-Azure-and-AWS/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6078-AI-Product-and-Services-from-Google-Azure-and-AWS.jpg&#34; alt=&#34;AI Product and Services from Google, Azure and AWS&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Product and Services from Google, Azure and AWS 
    &lt;div id=&#34;ai-product-and-services-from-google-azure-and-aws&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-product-and-services-from-google-azure-and-aws&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Sno&lt;/th&gt;
          &lt;th&gt;Azure&lt;/th&gt;
          &lt;th&gt;Google&lt;/th&gt;
          &lt;th&gt;AWS&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/anomaly-detector/&#34; target=&#34;_blank&#34;&gt;Anomaly Detector:&lt;/a&gt; Easily add anomaly detection capabilities to your apps.&lt;/td&gt;
          &lt;td&gt;AutoML: Custom low-code models &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/training/training&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/a2i/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Augmented AI&lt;/a&gt; : Easily implement human review of machine learning predictions&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/bot-services/&#34; target=&#34;_blank&#34;&gt;Azure Bot Service:&lt;/a&gt; Build conversational AI experiences for your customers&lt;/td&gt;
          &lt;td&gt;Cloud TPU: Hardware acceleration for ML &lt;a href=&#34;https://cloud.google.com/tpu/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/tpu/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Bedrock&lt;/a&gt; : The easiest way to build and scale generative AI applications with foundation models (FMs).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/search/&#34; target=&#34;_blank&#34;&gt;Azure Cognitive Search:&lt;/a&gt; Enterprise scale search for app development&lt;/td&gt;
          &lt;td&gt;Cloud Translation: Language detection and translation &lt;a href=&#34;https://cloud.google.com/translate/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/translate/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/codeguru/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon CodeGuru&lt;/a&gt; : Intelligent recommendations for building and running modern applications&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/databricks/&#34; target=&#34;_blank&#34;&gt;Azure Databricks:&lt;/a&gt; Design AI with Apache Spark‚Ñ¢-based analytics&lt;/td&gt;
          &lt;td&gt;Cloud Vision: Image recognition and classification &lt;a href=&#34;https://cloud.google.com/vision/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vision/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/comprehendmedical/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Comprehend Medical&lt;/a&gt; : Amazon Comprehend Medical uses machine learning to extract insights and relationships from medical text.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/machine-learning/&#34; target=&#34;_blank&#34;&gt;Azure Machine Learning:&lt;/a&gt; Enterprise-grade machine learning service to build and deploy models faster&lt;/td&gt;
          &lt;td&gt;Contact Center AI: AI in your contact center &lt;a href=&#34;https://cloud.google.com/solutions/contact-center/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/solutions/contact-center/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/comprehend/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Comprehend&lt;/a&gt; : Analyze Unstructured Text&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/services/open-datasets/&#34; target=&#34;_blank&#34;&gt;Azure Open Datasets:&lt;/a&gt; Cloud platform to host and share curated open datasets to accelerate development of machine learning models&lt;/td&gt;
          &lt;td&gt;Deep Learning Containers: Preconfigured containers for deep learning &lt;a href=&#34;https://cloud.google.com/ai-platform/deep-learning-containers/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/ai-platform/deep-learning-containers/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/deepcomposer/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;AWS DeepComposer&lt;/a&gt; : AWS DeepComposer allows developers of all skill levels to get started with Generative AI.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/&#34; target=&#34;_blank&#34;&gt;Azure Cognitive Services:&lt;/a&gt; Deploy high-quality AI models as APIs&lt;/td&gt;
          &lt;td&gt;Deep Learning VM Images: Preconfigured VMs for deep learning &lt;a href=&#34;https://cloud.google.com/deep-learning-vm/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deep-learning-vm/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/deeplens/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;AWS DeepLens&lt;/a&gt; : Deep Learning Enabled Video Camera&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/services/video-indexer/&#34; target=&#34;_blank&#34;&gt;Azure Video Analyzer for Media:&lt;/a&gt; Unlock video insights&lt;/td&gt;
          &lt;td&gt;Dialogflow: Create conversational interfaces &lt;a href=&#34;https://cloud.google.com/dialogflow-enterprise/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/dialogflow-enterprise/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/deepracer/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;AWS DeepRacer&lt;/a&gt; : Fully autonomous 1/18th scale race car, driven by machine learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/content-safety/&#34; target=&#34;_blank&#34;&gt;Content Moderator GA:&lt;/a&gt; Automated image, text and video moderation&lt;/td&gt;
          &lt;td&gt;Document AI: Analyze, classify, search documents &lt;a href=&#34;https://cloud.google.com/solutions/document-understanding/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/document-understanding/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/devops-guru/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon DevOps Guru&lt;/a&gt; : ML-powered cloud operations service to improve application availability.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/custom-vision-service/&#34; target=&#34;_blank&#34;&gt;Custom Vision:&lt;/a&gt; Easily customise your own state-of-the-art computer vision models for your unique use case&lt;/td&gt;
          &lt;td&gt;Recommendations AI: Create custom recommendations &lt;a href=&#34;https://cloud.google.com/recommendations/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/recommendations-ai/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/forecast/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Forecast&lt;/a&gt; : Amazon Forecast is a fully-managed service for accurate time-series forecasting&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/virtual-machines/data-science-virtual-machines/&#34; target=&#34;_blank&#34;&gt;Data Science Virtual Machines:&lt;/a&gt; Rich pre-configured environment for AI development&lt;/td&gt;
          &lt;td&gt;Speech-To-Text: Convert audio to text &lt;a href=&#34;https://cloud.google.com/speech/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/speech/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/frauddetector/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Fraud Detector&lt;/a&gt; : Detect more online fraud faster using machine learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/form-recognizer/&#34; target=&#34;_blank&#34;&gt;Azure Form Recogniser:&lt;/a&gt; Accelerate information extraction from documents&lt;/td&gt;
          &lt;td&gt;Talent Solutions: Job search with ML &lt;a href=&#34;https://cloud.google.com/job-discovery/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/job-discovery/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/healthlake/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon HealthLake&lt;/a&gt; : Making sense of health data&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/immersive-reader/&#34; target=&#34;_blank&#34;&gt;Azure Immersive Reader:&lt;/a&gt; Empower users of all ages and abilities to read and comprehend text&lt;/td&gt;
          &lt;td&gt;Text-To-Speech: Convert text to audio &lt;a href=&#34;https://cloud.google.com/text-to-speech/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/text-to-speech/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/kendra/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Kendra&lt;/a&gt; : Highly accurate enterprise search service powered by machine learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/kinect-dk/&#34; target=&#34;_blank&#34;&gt;Kinect DK:&lt;/a&gt; Build computer vision and speech models using a developer kit with advanced AI sensors&lt;/td&gt;
          &lt;td&gt;Vertex AI Data Labeling: Data labeling by humans &lt;a href=&#34;https://cloud.google.com/data-labeling/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/lexv2/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Lex&lt;/a&gt; : Build Voice and Text Chatbots&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/conversational-language-understanding/&#34; target=&#34;_blank&#34;&gt;Language Understanding:&lt;/a&gt; Teach your apps to understand commands from your users&lt;/td&gt;
          &lt;td&gt;Vertex AI Edge Manager: Deploy monitor edge inferences &lt;a href=&#34;https://https://cloud.google.com/vertex-ai/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/lookoutequipment/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Lookout for Equipment&lt;/a&gt; : Detect abnormal equipment behavior by analyzing sensor data&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/genomics/&#34; target=&#34;_blank&#34;&gt;Microsoft Genomics:&lt;/a&gt; Power genome sequencing and research insights&lt;/td&gt;
          &lt;td&gt;Vertex AI Feature Store: Managed ML feature repository &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/featurestore&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/featurestore/overview&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/lookoutmetrics/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Lookout for Metrics&lt;/a&gt; : Accurately detect anomalies in your business metrics and quickly understand why&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/personalizer/&#34; target=&#34;_blank&#34;&gt;Personaliser:&lt;/a&gt; An AI service that delivers a personalised user experience&lt;/td&gt;
          &lt;td&gt;Vertex AI Matching Engine: Vector similarity searches &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/matching-engine&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/matching-engine&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/lookoutvision/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Lookout for Vision&lt;/a&gt; : Identify defects using computer vision to automate quality inspection.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/services/project-bonsai/&#34; target=&#34;_blank&#34;&gt;Project Bonsai:&lt;/a&gt; Create intelligent industrial control systems using simulations&lt;/td&gt;
          &lt;td&gt;Vertex AI Model Monitoring: Monitor models for skew/drift &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/model-monitoring&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/model-monitoring/overview&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/monitron/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Monitron&lt;/a&gt; : End-to-end system for equipment monitoring&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/en-in/azure/cognitive-services/QnAMaker/Overview/overview&#34; target=&#34;_blank&#34;&gt;QnA Maker:&lt;/a&gt; Distill information into conversational, easy-to-navigate answers&lt;/td&gt;
          &lt;td&gt;Vertex AI Pipelines: Hosted ML workflows &lt;a href=&#34;https://cloud.google.com/ai-platform/pipelines/&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/omics/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Omics&lt;/a&gt; : Transform omics data into insights.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/speaker-recognition/&#34; target=&#34;_blank&#34;&gt;Speaker Recognition:&lt;/a&gt; A Speech service feature that verifies and identifies speakers&lt;/td&gt;
          &lt;td&gt;Vertex AI Predictions: Autoscaled model serving &lt;a href=&#34;https://cloud.google.com/ai-platform/prediction/docs/overview&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/panorama/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;AWS Panorama&lt;/a&gt; : Enabling computer vision applications at the edge&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/speech-to-text/&#34; target=&#34;_blank&#34;&gt;Speech to Text:&lt;/a&gt; A Speech service feature that accurately converts spoken audio to text&lt;/td&gt;
          &lt;td&gt;Vertex AI Tensorboard: Managed TensorBoard for ML-experiment Visualization &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/experiments&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/personalize/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Personalize&lt;/a&gt; : Amazon Personalize helps you easily add real-time recommendations to your apps&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/speech-translation/&#34; target=&#34;_blank&#34;&gt;Speech Translation:&lt;/a&gt; Easily integrate real-time speech translation to your app&lt;/td&gt;
          &lt;td&gt;Vertex AI Training: Distributed AI training &lt;a href=&#34;https://cloud.google.com/ai-platform/training/docs/overview&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/polly/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Polly&lt;/a&gt; : Turn Text into Lifelike Speech&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/language-service/&#34; target=&#34;_blank&#34;&gt;Cognitive Service for Language:&lt;/a&gt; Add natural language capabilities with a single API call&lt;/td&gt;
          &lt;td&gt;Vertex AI Vizier: black-box hyperparameter tuning &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/vizier/overview&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/vizier&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/rekognition/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Rekognition&lt;/a&gt; : Search and Analyze Images&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/services/cognitive-services/text-to-speech/&#34; target=&#34;_blank&#34;&gt;Text to Speech:&lt;/a&gt; A Speech service feature that converts text to lifelike speech&lt;/td&gt;
          &lt;td&gt;Vertex AI Workbench:Jupyter-based environment for Data Science &lt;a href=&#34;https://cloud.google.com/vertex-ai-workbench&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/workbench&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon SageMaker&lt;/a&gt; : Build, Train, and Deploy Machine Learning Models&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/services/cognitive-services/translator/&#34; target=&#34;_blank&#34;&gt;Translator:&lt;/a&gt; Easily conduct machine translation with a simple REST API call&lt;/td&gt;
          &lt;td&gt;Vertex Explainable AI: Understand ML model predictions &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/explainable-ai/overview&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/explainable-ai&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/textract/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Textract&lt;/a&gt; : Easily extract text and data from virtually any document&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;26.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/metrics-advisor/&#34; target=&#34;_blank&#34;&gt;Azure Metrics Advisor:&lt;/a&gt; An AI service that monitors metrics and diagnoses issues&lt;/td&gt;
          &lt;td&gt;Vertex ML Metadata: Artifact, lineage, and execution tracking &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/ml-metadata&#34; target=&#34;_blank&#34;&gt;Link&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/transcribe/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Transcribe&lt;/a&gt; : Powerful Speech Recognition&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;27.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/bot-services/health-bot/&#34; target=&#34;_blank&#34;&gt;Health Bot:&lt;/a&gt; A managed service purpose-built for development of virtual healthcare assistants&lt;/td&gt;
          &lt;td&gt;Vision Product Search: Visual search for products &lt;a href=&#34;https://cloud.google.com/vision/product-search/docs/&#34; target=&#34;_blank&#34;&gt;Doc&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://us-east-1.console.aws.amazon.com/translate/home?region=us-east-1&#34; target=&#34;_blank&#34;&gt;Amazon Translate&lt;/a&gt; : Powerful Neural Machine Translation&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;28.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://aka.ms/ScalerHomepage&#34; target=&#34;_blank&#34;&gt;Azure Applied AI Services:&lt;/a&gt; Specialised services that enable organisations to accelerate time to value in applying AI to solve common scenarios&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;29.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/openai-service/&#34; target=&#34;_blank&#34;&gt;Azure OpenAI Service:&lt;/a&gt; Apply advanced coding and language models to a variety of use cases&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;30.&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-in/products/cognitive-services/vision-services/&#34; target=&#34;_blank&#34;&gt;Azure Cognitive Services for Vision:&lt;/a&gt; Unlock insights from image and video content with AI&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;&lt;br&gt;
Dr Hari Thapliyaal&lt;br&gt;
dasarpai.com&lt;br&gt;
linkedin.com/in/harithapliyal&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introduction to ML Model Deployment</title>
      <link>http://localhost:1313/dsblog/Introduction-to-ML-Model-deployment/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Introduction-to-ML-Model-deployment/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6077-Introduction-to-ML-Model-deployment.jpg&#34; alt=&#34;Introduction to AI Model Deployement&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Introduction to AI Model deployment 
    &lt;div id=&#34;introduction-to-ai-model-deployment&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-ai-model-deployment&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Big Players 
    &lt;div id=&#34;big-players&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#big-players&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon
&lt;ul&gt;
&lt;li&gt;Amazon has many products and one of their product is &lt;strong&gt;AWS Cloud&lt;/strong&gt;. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt; is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud.&lt;/li&gt;
&lt;li&gt;Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving.&lt;/li&gt;
&lt;li&gt;AWS is oldest cloud service provider in the market.&lt;/li&gt;
&lt;li&gt;AWS Sagemaker was launched in Nov&#39;17.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Google
&lt;ul&gt;
&lt;li&gt;Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called &lt;strong&gt;Google Cloud&lt;/strong&gt;. Under this product they sell IT infrastrcture like Amazon sells under AWS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VertexAI&lt;/strong&gt; is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud.&lt;/li&gt;
&lt;li&gt;VertexAI can be used to train AI Model,host AI model, monitor the model etc.&lt;/li&gt;
&lt;li&gt;VertexAI was launched in Jun&#39;21&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Microsoft
&lt;ul&gt;
&lt;li&gt;Like Amazon&amp;rsquo;s cloud platform which is called AWS Cloud, Microsoft&amp;rsquo;s cloud plateform is called &lt;strong&gt;Azure&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Microsoft&amp;rsquo;s AI product is called &lt;strong&gt;Azure Machine Learning&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Today (Jul&#39;23) Azure Machine Learning has has most of the capabilites than any other player&amp;rsquo;s AI product.&lt;/li&gt;
&lt;li&gt;Azure Machine Learning was launched Feb&#39;14&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is GenAI? 
    &lt;div id=&#34;what-is-genai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-genai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights &amp;amp; biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Embedding with FastText</title>
      <link>http://localhost:1313/dsblog/Embedding-with-FastText/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Embedding-with-FastText/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6073-Embedding-with-FastText.jpg&#34; alt=&#34;Embedding with FastText&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Embedding with FastText 
    &lt;div id=&#34;embedding-with-fasttext&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#embedding-with-fasttext&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;../../dsblog/what-is-nlp#what-is-embedding&#34;&gt;What is Embedding?&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;../../dsblog/what-is-nlp#what-are-different-embedding-types&#34;&gt;What are Different Types of Embedding&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is FastText? 
    &lt;div id=&#34;what-is-fasttext&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-fasttext&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;using word embeddings&lt;/strong&gt;.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Major LLM Developers Shaping the AI Landscape</title>
      <link>http://localhost:1313/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6075-Major-LLM-Developers-Reshaping-NLP-Advancements.jpg&#34; alt=&#34;Major LLM Developers Shaping the AI Landscape&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Major LLM Developers Shaping the AI Landscape 
    &lt;div id=&#34;major-llm-developers-shaping-the-ai-landscape&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#major-llm-developers-shaping-the-ai-landscape&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;From Text to Intelligence: Major LLM Developers Shaping the AI Landscape&lt;/strong&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction: 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Artificial Intelligence (AI) has experienced an exponential growth, fueled by groundbreaking research and the efforts of innovative developers. Among the key players, Large Language Model (LLM) developers have taken center stage, creating powerful language models that have revolutionized natural language processing and understanding. In this article, we delve into the major LLM developers, their key contributions.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is GAN Architecture?</title>
      <link>http://localhost:1313/dsblog/What-is-GAN-Architecture/</link>
      <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-is-GAN-Architecture/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6069-What-is-GAN-Architecture.jpg&#34; alt=&#34;What is GAN Architecture?&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is GAN Architecture? 
    &lt;div id=&#34;what-is-gan-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Generative Adversarial Networks (GANs) are a powerful class of neural networks that are used for unsupervised learning. It was developed and introduced by Ian J. Goodfellow in 2014. It is a type of artificial intelligence (AI) model that consists of two neural networks: a generator and a discriminator. GANs are used for generative tasks, such as creating realistic images, videos, or even audio.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Capabilities of AI Transformers</title>
      <link>http://localhost:1313/dsblog/Capabilities-of-AI-Transformers/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Capabilities-of-AI-Transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6067-Capabilities-of-AI-Transformers.jpg&#34; alt=&#34;Capabilities of AI Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Capabilities of AI Transformers 
    &lt;div id=&#34;capabilities-of-ai-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#capabilities-of-ai-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Model Garden of VertexAI</title>
      <link>http://localhost:1313/dsblog/Model-Garden-of-VertexAI/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Model-Garden-of-VertexAI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6065-Model-Garden-of-VertexAI.jpg&#34; alt=&#34;All Resources to Learn Data Science&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Model Garden of VertexAI: 
    &lt;div id=&#34;model-garden-of-vertexai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#model-garden-of-vertexai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Unlocking the Power of Google&amp;rsquo;s VertexAI: Exploring the World of Pre-Built Models for AI Tasks 
    &lt;div id=&#34;unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction: 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Artificial Intelligence (AI) has transformed numerous industries, from healthcare and finance to e-commerce, logistic, eduction and entertainment. But the complexity of developing machine learning models often poses a challenge. As the demand for AI-powered solutions continues to rise, data scientists seek efficient ways to leverage pre-trained models or build custom models to address specific tasks. In this regard, Google&amp;rsquo;s VertexAI emerges as a robust platform that offers an extensive selection of pre-built models for a wide range of AI tasks. VertexAI platform has revolutionized the landscape by seamlessly leveraging LLM (Large Language Models) and Prompt Engineering techniques to perform complex machine learning tasks effortlessly. With VertexAI, data scientists can harness the power of state-of-the-art language models, such as LLM, to accelerate their ML development process. Additionally, the innovative concept of Prompt Engineering enables users to effectively communicate with the models, guiding them to deliver precise and accurate results. From computer vision and natural language processing to speech processing and structured tabular data analysis, Vertex AI&amp;rsquo;s repertoire includes over 100 models catering to diverse application domains. This article explores how Vertex AI, through its integration of LLM and Prompt Engineering, empowers users to effortlessly tackle intricate machine learning tasks across diverse domains, revolutionizing the AI development experience.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>All Resources to Learn Data Science</title>
      <link>http://localhost:1313/dsblog/all-resources-to-learn-data-science/</link>
      <pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/all-resources-to-learn-data-science/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6064-Resources-to-Learn-Everything-About-AI.jpg&#34; alt=&#34;All Resources to Learn Data Science&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;All Resources to Learn Data Science 
    &lt;div id=&#34;all-resources-to-learn-data-science&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#all-resources-to-learn-data-science&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Welcome to the AI ML Resources category page, where you&amp;rsquo;ll find a wealth of knowledge on various topics related to Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), Natural Language Processing (NLP), Mathematics and statistics required to learn Data Science. Each of the pages mentioned below brings together a wide range of articles, tutorials, and guides that delve into the fascinating world of AI and ML. Whether you&amp;rsquo;re a beginner seeking foundational knowledge or an experienced practitioner looking to expand your skills, these resources offer valuable insights and practical guidance. You need to go through these links one at time. As a master page you can book mark this page.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Demystifying DevOps, MLOps, and DataOps</title>
      <link>http://localhost:1313/dsblog/Demystifying-DevOps-MLOps-and-DataOps/</link>
      <pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Demystifying-DevOps-MLOps-and-DataOps/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6066-Demystifying-DevOps-MLOps-and-DataOps.jpg&#34; alt=&#34;All Resources to Learn Data Science&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Demystifying DevOps, MLOps, and DataOps: 
    &lt;div id=&#34;demystifying-devops-mlops-and-dataops&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#demystifying-devops-mlops-and-dataops&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Bridging the Gap between Software Development, Machine Learning, and Data Managemen&lt;/strong&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is DevOps 
    &lt;div id=&#34;what-is-devops&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-devops&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;DevOps, short for Development and Operations, is a set of practices, principles, and cultural philosophies that aim to improve collaboration and efficiency between software development teams and IT operations teams. It emphasizes the integration of software development and IT operations, breaking down traditional silos and fostering a collaborative approach throughout the entire software delivery lifecycle.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>God Fathers of AI</title>
      <link>http://localhost:1313/dsblog/God-Fathers-of-AI/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/God-Fathers-of-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6058-God-Fathers-of-AI.jpg&#34; alt=&#34;God Fathers of AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;God Fathers of AI 
    &lt;div id=&#34;god-fathers-of-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#god-fathers-of-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In other fields of studies or in religion, there is only one god or only one godfather. But in the field of AI, that is not the case. There are many pioneers or Godfathers who have done significant work in this field. Recently, the resignation of Dr. Geoffrey Hinton from Google raised eyebrows in the business world and in Governments the world over. Technology is good or bad, it depends upon whose hand it is. Geoffrey raised that concern and for that, he wants better controls in place. What will happen, we need to follow the progress and raise our voices around. In this article, I am mentioning some godfathers of AI, their workplaces, and their contributions. I am sure this will inspire many young minds.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Types of Machine Learning</title>
      <link>http://localhost:1313/dsblog/Types-of-Machine-Learning/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Types-of-Machine-Learning/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6056-Types-of-Machine-Learning.jpg&#34; alt=&#34;Types of Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Types of Machine Learning 
    &lt;div id=&#34;types-of-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#types-of-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Cost Functions and Optimizers in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg&#34; alt=&#34;Cost-Functions-and-Optimizers-in-Machine-Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Cost-Functions-and-Optimizers-in-Machine-Learning 
    &lt;div id=&#34;cost-functions-and-optimizers-in-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#cost-functions-and-optimizers-in-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is machine learning? 
    &lt;div id=&#34;what-is-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong&gt;development of algorithms and statistical models&lt;/strong&gt; that enable computers to improve their performance on a specific task through experience.&lt;/p&gt;
&lt;p&gt;In machine learning, the goal is to develop models that can &lt;strong&gt;automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions&lt;/strong&gt;. The models are trained on a large dataset, and the learning process involves &lt;strong&gt;optimizing the parameters of the model to minimize the prediction error&lt;/strong&gt;. For this purpose every algorithms uses some &lt;strong&gt;cost function or loss function&lt;/strong&gt;.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>GPU for Data Science Work</title>
      <link>http://localhost:1313/dsblog/GPU-for-Data-Science-Work/</link>
      <pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/GPU-for-Data-Science-Work/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6042-GPU-for-Data-Science-Work.jpg&#34; alt=&#34;GPU for Data Science Work&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;GPU for Data Science Work 
    &lt;div id=&#34;gpu-for-data-science-work&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#gpu-for-data-science-work&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is the difference between microprocessor (CPU) and GPU? 
    &lt;div id=&#34;what-is-the-difference-between-microprocessor-cpu-and-gpu&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-the-difference-between-microprocessor-cpu-and-gpu&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A microprocessor and a GPU (graphics processing unit) are both types of processors, but they are designed for different purposes and have different architectures.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Usecases in Agriculture Industry</title>
      <link>http://localhost:1313/dsblog/AI-usecases-in-Agriculture-Industry/</link>
      <pubDate>Mon, 23 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-usecases-in-Agriculture-Industry/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6037-AI-usecases-in-Agriculture-Industry.jpg&#34; alt=&#34;AI Usecases in Agriculture Industry&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Usecases in Agriculture Industry 
    &lt;div id=&#34;ai-usecases-in-agriculture-industry&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-usecases-in-agriculture-industry&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In the today world where energy saving, climate change, cost and process optimization, effectiveness is the philosophy of all business activities. With ever-increasing demand of food, the agriculture industry is looking for ways to improve crop yields and optimize farming practices. The use of Artificial Intelligence (AI) in agriculture is proving to be a game-changer, providing farmers with new and innovative tools to improve efficiency and productivity. From precision farming to autonomous tractors, AI is revolutionizing the way we think about farming and agriculture.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AI Use Cases in Food Processing</title>
      <link>http://localhost:1313/dsblog/AI-Use-Cases-in-Food-Processing/</link>
      <pubDate>Sun, 22 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-Use-Cases-in-Food-Processing/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6036-AI-Use-Cases-in-Food-Processing.jpg&#34; alt=&#34;&amp;ldquo;AI Use Cases in Food Processing&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Use Cases in Food Processing 
    &lt;div id=&#34;ai-use-cases-in-food-processing&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-use-cases-in-food-processing&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The food processing industry is a vital sector in the global economy, responsible for providing safe and nutritious food to millions of people around the world. Artificial intelligence (AI) is being increasingly used in the food processing industry to improve efficiency, reduce costs, and enhance the quality and safety of food products. From automated sorting and grading of fruits and vegetables to intelligent vending machines, AI is being used in a wide range of applications across the food processing industry.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introduction to Neural Network</title>
      <link>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg&#34; alt=&#34;Introduction to Neural Network&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Introduction to Neural Network 
    &lt;div id=&#34;introduction-to-neural-network&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-neural-network&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction to a Perceptron 
    &lt;div id=&#34;introduction-to-a-perceptron&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction-to-a-perceptron&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is GAN?</title>
      <link>http://localhost:1313/dsblog/What-is-GAN/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-is-GAN/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6043-gan.jpg&#34; alt=&#34;Partial Dependence Plots&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is GAN? 
    &lt;div id=&#34;what-is-gan&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is GAN (Generative Adversarial Network)? 
    &lt;div id=&#34;what-is-gan-generative-adversarial-network&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gan-generative-adversarial-network&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Generative adversarial networks (GANs) are besing used to generate images, videos, text, audio and music. GAN is a class of machine-learning models introduced by Ian Goodfellow and his colleagues in 2014. The GANs became popular among researchers quickly because of their property to generate new data with the same statistics as the input training set. It can be applied to images, videos, textual data, tabular data and more, proving useful for semi-supervised, fully supervised, and reinforcement learning.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Timeseries Interview Questions</title>
      <link>http://localhost:1313/dsblog/Timeseries-Interview-Questions/</link>
      <pubDate>Sun, 08 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Timeseries-Interview-Questions/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6023-Timeseries-Interview-Questions.jpg&#34; alt=&#34;Timeseries Interview Questions&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Timeseries Interview Questions 
    &lt;div id=&#34;timeseries-interview-questions&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#timeseries-interview-questions&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What are the characterstics of time series data? 
    &lt;div id=&#34;what-are-the-characterstics-of-time-series-data&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-the-characterstics-of-time-series-data&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Time series data is a series of data points collected over time. Some characteristics of time series data include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time dependence: Time series data is typically collected at regular time intervals, and the values of the time series are often dependent on the time at which they were collected.&lt;/li&gt;
&lt;li&gt;Equal duration gap between samples/ records&lt;/li&gt;
&lt;li&gt;No missing record in between&lt;/li&gt;
&lt;li&gt;Trend: Many time series exhibit a long-term trend, either upward or downward. This trend may be influenced by a variety of factors such as economic conditions, population growth, or technological changes.&lt;/li&gt;
&lt;li&gt;Seasonality: Many time series exhibit regular fluctuations due to seasonal factors such as weather, holidays, or other events. For example, retail sales may be higher in the months leading up to Christmas due to holiday shopping.&lt;/li&gt;
&lt;li&gt;Cyclicity: Time series may exhibit cyclical pattern. Like sales is highest in month start, body temprature is high at 6am, traffic is least on weekends etc.&lt;/li&gt;
&lt;li&gt;Noise: Time series data may also be affected by random noise or error, which can make it difficult to accurately forecast future values.&lt;/li&gt;
&lt;li&gt;Autocorrelation: Time series data may exhibit autocorrelation, which is the phenomenon of a value at a particular time being correlated with values at nearby times. This can make it challenging to model the time series, as the value at a given time may depend on the values at nearby times.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Example of time series data 
    &lt;div id=&#34;example-of-time-series-data&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#example-of-time-series-data&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Date&lt;/th&gt;
          &lt;th&gt;Sales (y)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-01-01&lt;/td&gt;
          &lt;td&gt;1000&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-02-01&lt;/td&gt;
          &lt;td&gt;1100&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-03-01&lt;/td&gt;
          &lt;td&gt;1200&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-04-01&lt;/td&gt;
          &lt;td&gt;1100&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-05-01&lt;/td&gt;
          &lt;td&gt;1000&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-06-01&lt;/td&gt;
          &lt;td&gt;900&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-07-01&lt;/td&gt;
          &lt;td&gt;1200&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-08-01&lt;/td&gt;
          &lt;td&gt;1400&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-09-01&lt;/td&gt;
          &lt;td&gt;1300&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2020-10-01&lt;/td&gt;
          &lt;td&gt;1500&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This first column can be date or date or datetime. Interval between two rows must be equal. The unit of date or time or datetime may be milliosecond, or second, or minute, or hour, or day, week, month, quarter, year or dacade. This should be continuous without any gap.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Linear Regression Interview Questions</title>
      <link>http://localhost:1313/dsblog/Linear-Regression-Interview-Questions/</link>
      <pubDate>Sat, 07 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Linear-Regression-Interview-Questions/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6022-Linear-Regression-Interview-Questions.jpg&#34; alt=&#34;Prompt Engineering for GPT4&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Linear Regression Interview Questions and Answers 
    &lt;div id=&#34;linear-regression-interview-questions-and-answers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#linear-regression-interview-questions-and-answers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;In this question-answer article, I will try that the start of every answer from example rather than theory (some unavoidable variation may be possible). I firmly believe if examples are clear, human mind is smart enough in generlization and creating theories.&lt;/p&gt;&lt;/blockquote&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Question 1: What is linear regression? What is the difference between simple linear regression and multiple linear regression? 
    &lt;div id=&#34;question-1-what-is-linear-regression-what-is-the-difference-between-simple-linear-regression-and-multiple-linear-regression&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#question-1-what-is-linear-regression-what-is-the-difference-between-simple-linear-regression-and-multiple-linear-regression&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables. It is used to predict the value of the dependent variable based on the values of the independent variables.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>GPT Usecases</title>
      <link>http://localhost:1313/dsblog/gpt-usecases/</link>
      <pubDate>Thu, 05 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/gpt-usecases/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6020-GPT-Usecases.jpg&#34; alt=&#34;GPT Usecases&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is GPT? 
    &lt;div id=&#34;what-is-gpt&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-gpt&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;GPT is a transformer. Don&amp;rsquo;t confuse it with your electricity transformer! In Artificial Intelligence there are different kinds of neural network architectures to perform various tasks like classification, translation, segmentation, regression, etc. One of those architectures is transformer architecture. The Foundation of this architecture is based on another two architectures called encoder architecture and decoder architecture. There are lots of other technical complexity but for the business readers I am hiding that for that the time being, we will discuss that at some other place. In nutshell, GPT is a Transformer technology developed by OpenAI and it can perform several NLP tasks. NLP stands for natural language preprocessing. NLP tasks mean tasks like sentiment analysis of the text, text classification, topic modeling, translation, named entity recognition, and dozens of other tasks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>ChatGPT Usecases</title>
      <link>http://localhost:1313/dsblog/chatgpt-usecases/</link>
      <pubDate>Wed, 04 Jan 2023 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/chatgpt-usecases/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6019-ChatGPT-Usecases.jpg&#34; alt=&#34;ChatGPT Usecases&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is ChatGPT? 
    &lt;div id=&#34;what-is-chatgpt&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-chatgpt&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;ChatGPT is &lt;strong&gt;general purpose&lt;/strong&gt; - &amp;ldquo;chat model&amp;rdquo; from OpenAI. It is a &lt;strong&gt;language model&lt;/strong&gt;, which means if you type some text then it can understand and respond to you appropriately. At this point in time, it is not accepting voice commands, neither able to process images or videos. A &lt;strong&gt;general-purpose model&lt;/strong&gt; means it can understand the question coming from any domain of life. A domain may be vertical or horizontal. A vertical domain means where a vendor is supplying a product or service for a specific type of customer. A horizontal domain is where a vendor supplies products or services for all types of customer. Healthcare, banking, logistic, insurance, agriculture, philosophy, history, and economics are one kind of verticals whereas
BPO, Quality Management, Software Development, Taxation, HR, IT Security, Accounting, Office Administration, Catering, and Entertainment are other kind of domains. A &lt;strong&gt;general-purpose model&lt;/strong&gt; can understand the questions from all aspects of life whether business vertical or horizontal or normal daily family or conflicts with other group members, family members, etc.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is Computer Vision</title>
      <link>http://localhost:1313/dsblog/what-is-computer-vision/</link>
      <pubDate>Wed, 28 Dec 2022 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/what-is-computer-vision/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6018-What-is-Computer-Vision.jpg&#34; alt=&#34;What is Computer Vision&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What is Computer vision? 
    &lt;div id=&#34;what-is-computer-vision&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-computer-vision&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In the digital world, scientists are working hard to create machines and robots that can interact with humans the way humans interact with each other. You cannot interact with another human being around if you are not aware of the objects and background around you. There are many ways to know the things around us. We can know them through smell; without looking anything around we can tell, here is a rose flower or samosa or sugar factory around. Without looking we can tell whether a train is coming or going, a person is going or coming, this is a song sung by Lata Mangeshkar. Without looking I can tell this is smooth or rough, hard or soft, cold or hot. In all these cases we could identify the objects and things around us without using our eyes.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What is NLP?</title>
      <link>http://localhost:1313/dsblog/what-is-nlp/</link>
      <pubDate>Mon, 19 Dec 2022 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/what-is-nlp/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6016-What-is-NLP.jpg&#34; alt=&#34;What is NLP?&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is NLP? 
    &lt;div id=&#34;what-is-nlp&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-nlp&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Humans interact with their surroundings using different kinds of inputs. Eyes deal with inputs of color, shape, and size. Ear deals with inputs of sound, voice, and noise. Similarly, the other 3 senses also deal with other kinds of inputs. When you write something you may be drawing some art or you may be drawing letters of some language. Language is what we use to speak, for example, English, Hindi, Kannada, Tamil, and French are languages. The script is a tool to write what we speak. There are many kinds of scripts and you can use those scripts to write words of the languages. Some scripts are good for some languages. You cannot write all the words of all the languages of the world using one script (without modifying the original letters of the script). The Roman script is good to write English languages but when you want to write any Indian language using Roman then you will make many mistakes when reading the scripts. Because you won&amp;rsquo;t be able to produce the same sound as the original language was producing.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Domain Knowledge in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Domain-Knowledge-in-Machine-Learning/</link>
      <pubDate>Sat, 15 Oct 2022 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Domain-Knowledge-in-Machine-Learning/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6015-domain-knowledge-in-machine-learning.jpg&#34; alt=&#34;Domain Knowledge in Machine Learning&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Domain Knowledge in Machine Learning 
    &lt;div id=&#34;domain-knowledge-in-machine-learning&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#domain-knowledge-in-machine-learning&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Let‚Äôs say the domain is a restaurant kitchen. A dataset with 3 variables. Two predictors and one predicted. Predictor variables are flour in kilograms and water in liters. A predicted variable is the number of roti/ bread. You know the model will be something like this.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Data Science, AI, ML, eBooks, PDF Books</title>
      <link>http://localhost:1313/dsblog/ds-ai-ml-books/</link>
      <pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/ds-ai-ml-books/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dsresources/dsr120-Data-Science-AI-ML-eBooks-PDF-Books.jpg&#34; alt=&#34;DS, AI, ML, Books Available&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Online Data Science, AI, ML Content 
    &lt;div id=&#34;online-data-science-ai-ml-content&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#online-data-science-ai-ml-content&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;Data Science, AI, Machine Learning, Books/ Guide/ Reports/ Presentations / Jupyter Notebook Available.&lt;/p&gt;
&lt;p&gt;All these books are available in pdf format at &lt;a href=&#34;https://drive.google.com/drive/folders/14wS6JWWDsZ2TEXCD9A7jgLVCEVEd1Mpb?usp=sharing&#34; target=&#34;_blank&#34;&gt; this link&lt;/a&gt;. I update this page less frequently but keep adding books in the repo. The list below may not contain the name you are looking for. Therefore, it is suggested to visit the link mentioned earlier. This link contains excellent presentations Book, (PPT), Handbook, Report, Articles (ARTC), Book, Booklet, eBook, Notebook, Notes, PAPER, GUIDE, TOC, Syllabus, LINKS, Handbook, Chapter, Tool, BROC ‚Äì Broacher on Machine Learning, Deep Learning, NLP, Statistics, Reinforcement Learning, GAN. Approx 800 pdf files which inclues 350+ books.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What Are Transformers in AI</title>
      <link>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg&#34; alt=&#34;What-are-Transformers-in-AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What Are Transformers in AI 
    &lt;div id=&#34;what-are-transformers-in-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-transformers-in-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Transformer Architecture 
    &lt;div id=&#34;transformer-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformer-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/transformer/transformer-arch.jpg&#34; alt=&#34;Transformer&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>How Naive Bayes Classifier Works</title>
      <link>http://localhost:1313/dsblog/How-Naive-Bayes-Classifier-Works/</link>
      <pubDate>Wed, 31 Mar 2021 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/How-Naive-Bayes-Classifier-Works/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6005-How-Naive-Bayes-Work-for-Recommendation.jpg&#34; alt=&#34;Naive Bayes&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;How Naive Bayes Classifier Works? 
    &lt;div id=&#34;how-naive-bayes-classifier-works&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#how-naive-bayes-classifier-works&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Naive Bayes classifier example 
    &lt;div id=&#34;naive-bayes-classifier-example&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#naive-bayes-classifier-example&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In this presentation, I am not going into the depth of the Naive Bayes algorithm. I am assuming you have heard this term many times but are not able to visualize it mentally or struggling to comprehend this. If that is the case, then you are on the right page.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>EDA &amp; Feature Engineering 101</title>
      <link>http://localhost:1313/dsblog/EDA-Feature-Engineering-101/</link>
      <pubDate>Mon, 24 Aug 2020 15:50:00 +0530</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/EDA-Feature-Engineering-101/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6008-EDA101.jpg&#34; alt=&#34;EDA &amp;amp; Feature Engineering&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;What is EDA? 
    &lt;div id=&#34;what-is-eda&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-is-eda&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;EDA means Exploratory Data Analysis. The purpose of data analysis is to explore. Exploration means try to understand what kind of data I have in my hand. Using EDA we try to get the answer to the following questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What kind of data is this? (file format, volume of data, number of columns, metadata data of image/video/audio or some feedback in English or other languages, or tabular data, etc)&lt;/li&gt;
&lt;li&gt;How complex is this data? (How many files are there? primary key? how these files are connected to each other? is nested data in some files? is some field having nested data, etc.)&lt;/li&gt;
&lt;li&gt;Is this data sufficient for meeting our ultimate goal, i.e. Model building?&lt;/li&gt;
&lt;li&gt;Is there any missing data? Data needed but not given by the business or not available at all or costly to get that data etc.&lt;/li&gt;
&lt;li&gt;Are there any missing values? In the given dataset do we have complete information or some values are missing for some records or some columns?&lt;/li&gt;
&lt;li&gt;What are different independent and dependent fields?&lt;/li&gt;
&lt;li&gt;Is there any relationship between different independent variables of the dataset? If yes then how strong is that relationship?&lt;/li&gt;
&lt;li&gt;Are observations independent or tightly coupled like we see in time-series data?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the data scient project lifecycle, EDA is not a sequential, one-time, isolated process. Till the time data is not ready for modeling we keep doing EDA and cleaning the data. So, EDA is followed by a list of decisions taken to clean the dataset, and finally, data cleaning steps are implemented. If the dataset is not in the good shape after the first iteration of EDA we continue EDA in the next cycle. In this article, I am not referring to EDA as just visualizing and understanding the dataset but all the steps required till the dataset is not ready for modeling.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>DS, AI, ML Online Course, Tutorial, Videos</title>
      <link>http://localhost:1313/dsblog/data-science-tutorial-video-resources/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/data-science-tutorial-video-resources/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dsresources/dsr119-DS-AI-ML-Online-Course-Tutorial-Videos.jpg&#34; alt=&#34;DS, AI, ML Online Course, Tutorial, Videos&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;DS, AI, ML Online Course, Tutorial, Videos 
    &lt;div id=&#34;ds-ai-ml-online-course-tutorial-videos&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ds-ai-ml-online-course-tutorial-videos&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Courses 
    &lt;div id=&#34;courses&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#courses&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://class.coursera.org/ml-005&#34; target=&#34;_blank&#34;&gt;Machine Learning ‚Äì Stanford&lt;/a&gt;¬†by Andrew Ng in Coursera (2010-2014)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://work.caltech.edu/lectures.html&#34; target=&#34;_blank&#34;&gt;Machine Learning ‚Äì Caltech&lt;/a&gt;¬†by Yaser Abu-Mostafa (2012-2014)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml&#34; target=&#34;_blank&#34;&gt;Machine Learning ‚Äì Carnegie Mellon&lt;/a&gt;¬†by Tom Mitchell (Spring 2011)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://class.coursera.org/neuralnets-2012-001&#34; target=&#34;_blank&#34;&gt;Neural Networks for Machine Learning&lt;/a&gt;¬†by Geoffrey Hinton in Coursera (2012)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&#34; target=&#34;_blank&#34;&gt;Neural networks class&lt;/a&gt;¬†by Hugo Larochelle from Universit√© de Sherbrooke (2013)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start&#34; target=&#34;_blank&#34;&gt;Deep Learning Course&lt;/a&gt;¬†by CILVR lab @ NYU (2014)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/&#34; target=&#34;_blank&#34;&gt;A.I ‚Äì Berkeley&lt;/a&gt;¬†by Dan Klein and Pieter Abbeel (2013)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/&#34; target=&#34;_blank&#34;&gt;A.I ‚Äì MIT&lt;/a&gt;¬†by Patrick Henry Winston (2010)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html&#34; target=&#34;_blank&#34;&gt;Vision and learning ‚Äì computers and brains&lt;/a&gt;¬†by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vision.stanford.edu/teaching/cs231n/syllabus.html&#34; target=&#34;_blank&#34;&gt;Convolutional Neural Networks for Visual Recognition ‚Äì Stanford&lt;/a&gt;¬†by Fei-Fei Li, Andrej Karpathy (2017)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs224d.stanford.edu/&#34; target=&#34;_blank&#34;&gt;Deep Learning for Natural Language Processing ‚Äì Stanford&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://info.usherbrooke.ca/hlarochelle/neural_networks/content.html&#34; target=&#34;_blank&#34;&gt;Neural Networks ‚Äì usherbrooke&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/&#34; target=&#34;_blank&#34;&gt;Machine Learning ‚Äì Oxford&lt;/a&gt;¬†(2014-2015)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/deep-learning-courses&#34; target=&#34;_blank&#34;&gt;Deep Learning ‚Äì Nvidia&lt;/a&gt;¬†(2015)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA&#34; target=&#34;_blank&#34;&gt;Graduate Summer School: Deep Learning, Feature Learning&lt;/a&gt;¬†by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning--ud730&#34; target=&#34;_blank&#34;&gt;Deep Learning ‚Äì Udacity/Google&lt;/a&gt;¬†by Vincent Vanhoucke and Arpan Chakraborty (2016)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE&#34; target=&#34;_blank&#34;&gt;Deep Learning ‚Äì UWaterloo&lt;/a&gt;¬†by Prof. Ali Ghodsi at University of Waterloo (2015)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=azaLcvuql_g&amp;amp;list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r&#34; target=&#34;_blank&#34;&gt;Statistical Machine Learning ‚Äì CMU&lt;/a&gt;¬†by Prof. Larry Wasserman&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm&#34; target=&#34;_blank&#34;&gt;Deep Learning Course&lt;/a&gt;¬†by Yann LeCun (2016)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm&#34; target=&#34;_blank&#34;&gt;Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://uvadlc.github.io/&#34; target=&#34;_blank&#34;&gt;UVA Deep Learning Course&lt;/a&gt;¬†MSc in Artificial Intelligence for the University of Amsterdam.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://selfdrivingcars.mit.edu/&#34; target=&#34;_blank&#34;&gt;MIT 6.S094: Deep Learning for Self-Driving Cars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://introtodeeplearning.com/&#34; target=&#34;_blank&#34;&gt;MIT 6.S191: Introduction to Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rll.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34;&gt;Berkeley CS 294: Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/livevideo/keras-in-motion&#34; target=&#34;_blank&#34;&gt;Keras in Motion video course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34;&gt;Practical Deep Learning For Coders&lt;/a&gt;¬†by Jeremy Howard ‚Äì Fast.ai&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deeplearning.cs.cmu.edu/&#34; target=&#34;_blank&#34;&gt;Introduction to Deep Learning&lt;/a&gt;¬†by Prof. Bhiksha Raj (2017)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/ai-for-everyone/&#34; target=&#34;_blank&#34;&gt;AI for Everyone&lt;/a&gt;¬†by Andrew Ng (2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://introtodeeplearning.com/&#34; target=&#34;_blank&#34;&gt;MIT Intro to Deep Learning 7 day bootcamp&lt;/a&gt;¬†‚Äì A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mithi.github.io/deep-blueberry&#34; target=&#34;_blank&#34;&gt;Deep Blueberry: Deep Learning&lt;/a&gt;¬†‚Äì A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://spinningup.openai.com/&#34; target=&#34;_blank&#34;&gt;Spinning Up in Deep Reinforcement Learning&lt;/a&gt;¬†‚Äì A free deep reinforcement learning course by OpenAI (2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34; target=&#34;_blank&#34;&gt;Deep Learning Specialization ‚Äì Coursera&lt;/a&gt;¬†‚Äì Breaking into AI with the best course from Andrew NG.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW&#34; target=&#34;_blank&#34;&gt;Deep Learning ‚Äì UC Berkeley STAT-157&lt;/a&gt;¬†by Alex Smola and Mu Li (2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/livevideo/machine-learning-for-mere-mortals&#34; target=&#34;_blank&#34;&gt;Machine Learning for Mere Mortals video course&lt;/a&gt;¬†by Nick Chase&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/&#34; target=&#34;_blank&#34;&gt;Machine Learning Crash Course with TensorFlow APIs&lt;/a&gt;¬†-Google AI&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://course.fast.ai/part2&#34; target=&#34;_blank&#34;&gt;Deep Learning from the Foundations&lt;/a&gt;¬†Jeremy Howard ‚Äì Fast.ai&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893&#34; target=&#34;_blank&#34;&gt;Deep Reinforcement Learning (nanodegree) ‚Äì Udacity&lt;/a&gt;¬†a 3-6 month Udacity nanodegree, spanning multiple courses (2018)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/livevideo/grokking-deep-learning-in-motion&#34; target=&#34;_blank&#34;&gt;Grokking Deep Learning in Motion&lt;/a&gt;¬†by Beau Carnes (2018)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/share/1000gAA0QdcV9aQng=/&#34; target=&#34;_blank&#34;&gt;Face Detection with Computer Vision and Deep Learning&lt;/a&gt;¬†by Hakan Cebeci&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/slides?ranMID=40328&amp;amp;ranEAID=SAyYsTvLiGQ&amp;amp;ranSiteID=SAyYsTvLiGQ-CmOo_hUqOR9Oj8ApcOw0Kg&amp;amp;siteID=SAyYsTvLiGQ-CmOo_hUqOR9Oj8ApcOw0Kg&amp;amp;utm_content=10&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=SAyYsTvLiGQ&#34; target=&#34;_blank&#34;&gt;Presentation skills: Designing Presentation Slides - Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/multivariate-calculus-machine-learning?ranMID=40328&amp;amp;ranEAID=SAyYsTvLiGQ&amp;amp;ranSiteID=SAyYsTvLiGQ-heqdps0Uveezr1XmtoOPDQ&amp;amp;siteID=SAyYsTvLiGQ-heqdps0Uveezr1XmtoOPDQ&amp;amp;utm_content=10&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=SAyYsTvLiGQ&#34; target=&#34;_blank&#34;&gt;Mathematics for Machine Learning: Multivariate Calculus - Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/machine-learning/home/welcome&#34; target=&#34;_blank&#34;&gt;Machine Learning ‚Äì Home Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/multivariate-calculus-machine-learning?ranMID=40328&amp;amp;ranEAID=SAyYsTvLiGQ&amp;amp;ranSiteID=SAyYsTvLiGQ-P3iVNag0daUW2nModtd2GA&amp;amp;siteID=SAyYsTvLiGQ-P3iVNag0daUW2nModtd2GA&amp;amp;utm_content=10&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=SAyYsTvLiGQ&#34; target=&#34;_blank&#34;&gt;Mathematics for Machine Learning: Multivariate Calculus - Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/browse/data-science&#34; target=&#34;_blank&#34;&gt;Data Science Certificates - Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learning.edureka.co/mycourses&#34; target=&#34;_blank&#34;&gt;Edureka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bdlabs.edureka.co:50001/cmf/services/18/status&#34; target=&#34;_blank&#34;&gt;Edureka-Cloudera Manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/&#34; target=&#34;_blank&#34;&gt;Udemy Courses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinereikicourse.com/&#34; target=&#34;_blank&#34;&gt;Courses ‚Äì Online Reiki Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses&#34; target=&#34;_blank&#34;&gt;DataCamp Courses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.byjus.com/video/chapter-videos/44724&#34; target=&#34;_blank&#34;&gt;Byju&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34; target=&#34;_blank&#34;&gt;udacity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://intellipaat.com/blog/what-is-apache-spark/&#34; target=&#34;_blank&#34;&gt;What is Spark ‚Äì A Comparison Between Spark vs. Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://studio.azureml.net/Home/ViewWorkspaceCached/086ca408664942138b618398589b02ff#Workspace/Settings/Name&#34; target=&#34;_blank&#34;&gt;Microsoft Azure Machine Learning Studio (classic)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apache.org/&#34; target=&#34;_blank&#34;&gt;Welcome to The Apache Software Foundation!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://makingindiaemployable.com/&#34; target=&#34;_blank&#34;&gt;Making India Employable - Vivid Vision 10 10 10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ideone.com/&#34; target=&#34;_blank&#34;&gt;GpI8H5 ‚Äì Online Python3 Interpreter &amp;amp; Debugging Tool ‚Äì Ideone.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLOU2XLYxmsILVTiOlMJdo7RQS55jYhsMi&#34; target=&#34;_blank&#34;&gt;Google I/O 2019 ‚Äì All Sessions ‚Äì YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLQY2H8rRoyvy2_vtWvCpQWM9GJXNTa5rV&#34; target=&#34;_blank&#34;&gt;TensorFlow at Google I/O 2019 ‚Äì YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bsc.hcverma.in/course/quantum&#34; target=&#34;_blank&#34;&gt;Quantum Mechanics - BSc Lectures by Prof. H C Verma and Team&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openpathshala.com/&#34; target=&#34;_blank&#34;&gt;Open Pathshala - Your Best Source to Learn Sanskrit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.classcentral.com/&#34; target=&#34;_blank&#34;&gt;Class Central #1 Search Engine for Free Online Courses &amp;amp; MOOCs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.class-central.com/course/coursera-mathematics-for-machine-learning-multivariate-calculus-10452&#34; target=&#34;_blank&#34;&gt;Free Online Course: Mathematics for Machine Learning: Multivariate Calculus from Coursera - Class Central&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://byjus.com/&#34; target=&#34;_blank&#34;&gt;e Learning for Basic Science and Maths&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.skillshare.com/&#34; target=&#34;_blank&#34;&gt;Online Classes by Skillshare - Start for Free Today&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalgarage&#34; target=&#34;_blank&#34;&gt;Learn online marketing with free courses ‚Äì Google Digital Garage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://moz.com/blog&#34; target=&#34;_blank&#34;&gt;Moz Blog ‚Äì SEO and Inbound Marketing Blog ‚Äì Moz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinecourses.nptel.ac.in/m#/lesson/noc19_hs53/8/15&#34; target=&#34;_blank&#34;&gt;NPTEL Online Courses Mobile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/learn/overview&#34; target=&#34;_blank&#34;&gt;Learn Python, Data Viz, Pandas &amp;amp; More - Tutorials - Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.superdatascience.com/training/&#34; target=&#34;_blank&#34;&gt;Data Science Training&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Tutorials 
    &lt;div id=&#34;tutorials&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#tutorials&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial&#34; target=&#34;_blank&#34;&gt;UFLDL Tutorial 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ufldl.stanford.edu/tutorial/supervised/LinearRegression/&#34; target=&#34;_blank&#34;&gt;UFLDL Tutorial 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial&#34; target=&#34;_blank&#34;&gt;Deep Learning for NLP (without Magic)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks&#34; target=&#34;_blank&#34;&gt;A Deep Learning Tutorial: From Perceptrons to Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.metacademy.org/roadmaps/rgrosse/deep_learning&#34; target=&#34;_blank&#34;&gt;Deep Learning from the Bottom up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deeplearning.net/tutorial/deeplearning.pdf&#34; target=&#34;_blank&#34;&gt;Theano Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf&#34; target=&#34;_blank&#34;&gt;Neural Networks for Matlab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/&#34; target=&#34;_blank&#34;&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials&#34; target=&#34;_blank&#34;&gt;Torch7 Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/josephmisiti/machine-learning-module&#34; target=&#34;_blank&#34;&gt;The Best Machine Learning Tutorials On The Web&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html&#34; target=&#34;_blank&#34;&gt;VGG Convolutional Neural Networks Practical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nlintz/TensorFlow-Tutorials&#34; target=&#34;_blank&#34;&gt;TensorFlow tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pkmital/tensorflow_tutorials&#34; target=&#34;_blank&#34;&gt;More TensorFlow tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/aymericdamien/TensorFlow-Examples&#34; target=&#34;_blank&#34;&gt;TensorFlow Python Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Vict0rSch/deep_learning&#34; target=&#34;_blank&#34;&gt;Keras and Lasagne Deep Learning Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition&#34; target=&#34;_blank&#34;&gt;Classification on raw time series in TensorFlow with a LSTM RNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/&#34; target=&#34;_blank&#34;&gt;Using convolutional neural nets to detect facial keypoints tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/astorfi/TensorFlow-World&#34; target=&#34;_blank&#34;&gt;TensorFlow-World&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/deep-learning-with-python&#34; target=&#34;_blank&#34;&gt;Deep Learning with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/grokking-deep-learning&#34; target=&#34;_blank&#34;&gt;Grokking Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/deep-learning-for-search&#34; target=&#34;_blank&#34;&gt;Deep Learning for Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511&#34; target=&#34;_blank&#34;&gt;Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yunjey/pytorch-tutorial&#34; target=&#34;_blank&#34;&gt;Pytorch Tutorial by Yunjey Choi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html&#34; target=&#34;_blank&#34;&gt;Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html&#34; target=&#34;_blank&#34;&gt;Overview and benchmark of traditional and deep learning models in text classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MelAbgrall/HardwareforAI&#34; target=&#34;_blank&#34;&gt;Hardware for AI: Understanding computer hardware &amp;amp; build your own computer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hackr.io/tutorials/learn-artificial-intelligence-ai&#34; target=&#34;_blank&#34;&gt;Programming Community Curated Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://amitness.com/2020/02/illustrated-self-supervised-learning/&#34; target=&#34;_blank&#34;&gt;The Illustrated Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://amitness.com/2020/02/albert-visual-summary/&#34; target=&#34;_blank&#34;&gt;Visual Paper Summary: ALBERT (A Lite BERT)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Videos and Lectures 
    &lt;div id=&#34;videos-and-lectures&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#videos-and-lectures&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RIkxVci-R4k&#34; target=&#34;_blank&#34;&gt;How To Create A Mind&lt;/a&gt;¬†By Ray Kurzweil&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=n1ViNeWhC24&#34; target=&#34;_blank&#34;&gt;Deep Learning, Self-Taught Learning and Unsupervised Feature Learning&lt;/a&gt;¬†By Andrew Ng&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vShMxxqtDDs&amp;amp;index=3&amp;amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT&#34; target=&#34;_blank&#34;&gt;Recent Developments in Deep Learning&lt;/a&gt;¬†By Geoff Hinton&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sc-KbuZqGkI&#34; target=&#34;_blank&#34;&gt;The Unreasonable Effectiveness of Deep Learning&lt;/a&gt;¬†by Yann LeCun&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4xsVFLnHC_0&#34; target=&#34;_blank&#34;&gt;Deep Learning of Representations&lt;/a&gt;¬†by Yoshua bengio&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6ufPpZDmPKA&#34; target=&#34;_blank&#34;&gt;Principles of Hierarchical Temporal Memory&lt;/a&gt;¬†by Jeff Hawkins&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT&#34; target=&#34;_blank&#34;&gt;Machine Learning Discussion Group ‚Äì Deep Learning w/ Stanford AI Lab&lt;/a&gt;¬†by Adam Coates&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vimeo.com/80821560&#34; target=&#34;_blank&#34;&gt;Making Sense of the World with Deep Learning&lt;/a&gt;¬†By Adam Coates&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wZfVBwOO0-k&#34; target=&#34;_blank&#34;&gt;Demystifying Unsupervised Feature Learning&lt;/a&gt;¬†By Adam Coates&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3boKlkPBckA&#34; target=&#34;_blank&#34;&gt;Visual Perception with Deep Learning&lt;/a&gt;¬†By Yann LeCun&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AyzOUbkUf3M&#34; target=&#34;_blank&#34;&gt;The Next Generation of Neural Networks&lt;/a&gt;¬†By Geoffrey Hinton at GoogleTechTalks&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn&#34; target=&#34;_blank&#34;&gt;The wonderful and terrifying implications of computers that can learn&lt;/a&gt;¬†By Jeremy Howard at TEDxBrussels&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs294a/handouts.html&#34; target=&#34;_blank&#34;&gt;Unsupervised Deep Learning ‚Äì Stanford&lt;/a&gt;¬†by Andrew Ng in Stanford (2011)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs224n/handouts/&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt;¬†By Chris Manning in Stanford&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html&#34; target=&#34;_blank&#34;&gt;A beginners Guide to Deep Neural Networks&lt;/a&gt;¬†By Natalie Hammel and Lorraine Yurshansky&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=czLI3oLDe8M&#34; target=&#34;_blank&#34;&gt;Deep Learning: Intelligence from Big Data&lt;/a&gt;¬†by Steve Jurvetson (and panel) at VLAB in Stanford.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FoO8qDB8gUU&#34; target=&#34;_blank&#34;&gt;Introduction to Artificial Neural Networks and Deep Learning&lt;/a&gt;¬†by Leo Isikdogan at Motorola Mobility HQ&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nips.cc/Conferences/2016/Schedule&#34; target=&#34;_blank&#34;&gt;NIPS 2016 lecture and workshop videos&lt;/a&gt;¬†‚Äì NIPS 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oS5fz_mHVz0&amp;amp;list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07&#34; target=&#34;_blank&#34;&gt;Deep Learning Crash Course&lt;/a&gt;: a series of mini-lectures by Leo Isikdogan on YouTube (2018)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/livevideo/deep-learning-crash-course&#34; target=&#34;_blank&#34;&gt;Deep Learning Crash Course&lt;/a&gt;¬†By Oliver Zeigermann&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/livevideo/deep-learning-with-r-in-motion&#34; target=&#34;_blank&#34;&gt;Deep Learning with R in Motion&lt;/a&gt;: a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/f5vUg6i&#34; target=&#34;_blank&#34;&gt;8 Essential Tips for People starting a Career in Data Science&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fMEhi4D&#34; target=&#34;_blank&#34;&gt;Cheatsheet: How to become a data scientist&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fruY2AC&#34; target=&#34;_blank&#34;&gt;The Art of Learning Data Science&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fxReDab&#34; target=&#34;_blank&#34;&gt;The Periodic Table of Data Science&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fXSE-us&#34; target=&#34;_blank&#34;&gt;Aspiring Data Scientists! Start to learn Statistics with these 6 books&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/f8S3Ygd&#34; target=&#34;_blank&#34;&gt;8 Skills You Need to Be a Data Scientist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fKugicE&#34; target=&#34;_blank&#34;&gt;Top 10 Essential Books for the Data Enthusiast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/fTGDkju&#34; target=&#34;_blank&#34;&gt;Aspiring data scientist? Master these fundamentals&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lnkd.in/f_Zhpzf&#34; target=&#34;_blank&#34;&gt;How to Become a Data Scientist ‚Äì On your own.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;GRETL ‚Äì Great Statistical software for Beginners 
    &lt;div id=&#34;gretl--great-statistical-software-for-beginners&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#gretl--great-statistical-software-for-beginners&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Simple Linear Regression¬†&lt;a href=&#34;https://lnkd.in/ecfsV9c&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/ecfsV9c&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Coding Dummy Variables¬†&lt;a href=&#34;https://lnkd.in/ef7Yd7f&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/ef7Yd7f&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Forecasting New Observations¬†&lt;a href=&#34;https://lnkd.in/eNKbxbU&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/eNKbxbU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Forecasting a Large Number of Observations¬†&lt;a href=&#34;https://lnkd.in/eHmibGs&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/eHmibGs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Logistic Regression¬†&lt;a href=&#34;https://lnkd.in/eRfhQ87&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/eRfhQ87&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Forecasting and Confusion Matrix¬†&lt;a href=&#34;https://lnkd.in/eaqrFJr&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/eaqrFJr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Modeling and Forecasting Time Series Data¬†&lt;a href=&#34;https://lnkd.in/e6fqKpF&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/e6fqKpF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Comparing Time Series Trend Models¬†&lt;a href=&#34;https://lnkd.in/eKjEUAE&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/eKjEUAE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Khan Academy is the best online free resource to learn Math for Data Science. (¬†&lt;a href=&#34;https://www.khanacademy.org/math/&#34; target=&#34;_blank&#34;&gt;https://www.khanacademy.org/math/&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Krista King has also done a great job in creating an exceptionally good introductory course. She is too good at designing the course. (¬†&lt;a href=&#34;https://www.udemy.com/user/kristaking/&#34; target=&#34;_blank&#34;&gt;https://www.udemy.com/user/kristaking/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;3Blue1Brown (¬†&lt;a href=&#34;https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/playlists&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/playlists&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Every Intro to Data Science Course on the Internet, Ranked. (&lt;a href=&#34;https://lnkd.in/fQDMiNX&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/fQDMiNX&lt;/a&gt; )&lt;/li&gt;
&lt;li&gt;What would be useful for aspiring data scientists to know? (&lt;a href=&#34;https://lnkd.in/fmcFyN7&#34; target=&#34;_blank&#34;&gt;https://lnkd.in/fmcFyN7&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>Reinforcement Learning Git Repositories</title>
      <link>http://localhost:1313/dsblog/rl-git-repo/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/rl-git-repo/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dsresources/dsr101-Reinforcement-Learning-Git-Repositories.jpg&#34; alt=&#34;Reinforcement Learning Git Repositories&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Reinforcement Learning Git Repositories 
    &lt;div id=&#34;reinforcement-learning-git-repositories&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#reinforcement-learning-git-repositories&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Sno.&lt;/th&gt;
          &lt;th&gt;URL&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;OpenAI Baselines: high-quality implementations of reinforcement learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/hill-a/stable-baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A fork of OpenAI Baselines, implementations of reinforcement learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/spinningup&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/spinningup&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;An educational resource to help anyone learn deep reinforcement learning.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/google/dopamine&#34; target=&#34;_blank&#34;&gt;https://github.com/google/dopamine&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Dopamine is a research framework for fast prototyping of reinforcement learning algorithms.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorflow/agents&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorflow/agents&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TF-Agents is a library for Reinforcement Learning in TensorFlow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/trfl&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/trfl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TensorFlow Reinforcement Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/Horizon&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/Horizon&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A platform for Applied Reinforcement Learning (Applied RL)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ELF&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/ELF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;An End-To-End, Lightweight and Flexible Platform for Game Research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/NervanaSystems/coach&#34; target=&#34;_blank&#34;&gt;https://github.com/NervanaSystems/coach&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Reinforcement Learning Coach by Intel AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ray-project/ray/tree/master/python/ray/rllib&#34; target=&#34;_blank&#34;&gt;https://github.com/ray-project/ray/tree/master/python/ray/rllib&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A fast and simple framework for building and running distributed applications.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/keras-rl/keras-rl&#34; target=&#34;_blank&#34;&gt;https://github.com/keras-rl/keras-rl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Deep Reinforcement Learning for Keras.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&#34; target=&#34;_blank&#34;&gt;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Kaixhin/Rainbow&#34; target=&#34;_blank&#34;&gt;https://github.com/Kaixhin/Rainbow&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Rainbow: Combining Improvements in Deep Reinforcement Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MillionIntegrals/vel&#34; target=&#34;_blank&#34;&gt;https://github.com/MillionIntegrals/vel&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Velocity in deep-learning research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;15&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorforce/tensorforce&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorforce/tensorforce&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorforce: A TensorFlow library for applied reinforcement learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/kengz/SLM-Lab&#34; target=&#34;_blank&#34;&gt;https://github.com/kengz/SLM-Lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Modular Deep Reinforcement Learning framework in PyTorch.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/rlworkgroup/garage&#34; target=&#34;_blank&#34;&gt;https://github.com/rlworkgroup/garage&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A framework for reproducible reinforcement learning research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;18&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34; target=&#34;_blank&#34;&gt;https://github.com/catalyst-team/catalyst&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Reproducible and fast DL &amp;amp; RL.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;19&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/higgsfield/RL-Adventure&#34; target=&#34;_blank&#34;&gt;https://github.com/higgsfield/RL-Adventure&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Pytorch Implementation of DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchical RL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/qfettes/DeepRL-Tutorials&#34; target=&#34;_blank&#34;&gt;https://github.com/qfettes/DeepRL-Tutorials&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Contains high quality implementations of Deep Reinforcement Learning algorithms written in PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;21&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/gym&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A toolkit for developing and comparing reinforcement learning algorithms.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;22&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/lab&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A customisable 3D platform for agent-based AI research&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;23&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Microsoft/malmo&#34; target=&#34;_blank&#34;&gt;https://github.com/Microsoft/malmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Project Malmo is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. We aim to inspire a new generation of research into challenging new problems presented by this unique environment. ‚Äî For installation instruct&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/retro&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Retro Games in Gym&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;25&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/dm_control&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/dm_control&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;The DeepMind Control Suite and Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;26&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/neural-mmo&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/neural-mmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Neural MMO ‚Äì A Massively Multiagent Game Environment&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;27&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/gym&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Gym @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;28&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/lab&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Lab @ DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;29&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Microsoft/malmo&#34; target=&#34;_blank&#34;&gt;https://github.com/Microsoft/malmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Project Malmo @ Microsoft&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;30&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/retro&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Retro @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;31&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/dm_control&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/dm_control&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Control Suite @ DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;32&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/neural-mmo&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/neural-mmo&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Neural MMO @ OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;33&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;34&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34;&gt;https://github.com/hill-a/stable-baselines&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Antonin Raffin, Ashley Hill&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;35&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34; target=&#34;_blank&#34;&gt;https://github.com/catalyst-team/catalyst&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Sergey Kolesnikov&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;36&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ray-project/ray/tree/master/python/ray/rllib&#34; target=&#34;_blank&#34;&gt;https://github.com/ray-project/ray/tree/master/python/ray/rllib&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Ray Team&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;37&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorflow/agents&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorflow/agents&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Google&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;38&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/Horizon&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/Horizon&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Facebook&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;39&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/NervanaSystems/coach&#34; target=&#34;_blank&#34;&gt;https://github.com/NervanaSystems/coach&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Intel&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;40&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/rlworkgroup/garage&#34; target=&#34;_blank&#34;&gt;https://github.com/rlworkgroup/garage&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by community&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;41&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/kengz/SLM-Lab&#34; target=&#34;_blank&#34;&gt;https://github.com/kengz/SLM-Lab&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Wah Loon Keng, Laura Graesser&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;42&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/google/dopamine&#34; target=&#34;_blank&#34;&gt;https://github.com/google/dopamine&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Google&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;43&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/openai/spinningup&#34; target=&#34;_blank&#34;&gt;https://github.com/openai/spinningup&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by OpenAI&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;44&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/trfl&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/trfl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;45&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/deepmind/scalable_agent&#34; target=&#34;_blank&#34;&gt;https://github.com/deepmind/scalable_agent&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by DeepMind&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;46&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ELF&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/ELF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Facebook&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;47&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/keras-rl/keras-rl&#34; target=&#34;_blank&#34;&gt;https://github.com/keras-rl/keras-rl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow Maintained by Matthias Plappert&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;48&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&#34; target=&#34;_blank&#34;&gt;https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Ilya Kostrikov&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;49&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Kaixhin/Rainbow&#34; target=&#34;_blank&#34;&gt;https://github.com/Kaixhin/Rainbow&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Kai Arulkumaran&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;50&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MillionIntegrals/vel&#34; target=&#34;_blank&#34;&gt;https://github.com/MillionIntegrals/vel&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Maintained by Jerry (?)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;51&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Khrylx/PyTorch-RL&#34; target=&#34;_blank&#34;&gt;https://github.com/Khrylx/PyTorch-RL&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;52&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tensorforce/tensorforce&#34; target=&#34;_blank&#34;&gt;https://github.com/tensorforce/tensorforce&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;53&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/higgsfield/RL-Adventure&#34; target=&#34;_blank&#34;&gt;https://github.com/higgsfield/RL-Adventure&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;54&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/qfettes/DeepRL-Tutorials&#34; target=&#34;_blank&#34;&gt;https://github.com/qfettes/DeepRL-Tutorials&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;55&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/SurrealAI/surreal&#34; target=&#34;_blank&#34;&gt;https://github.com/SurrealAI/surreal&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;TorchX&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;56&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/zuoxingdong/lagom&#34; target=&#34;_blank&#34;&gt;https://github.com/zuoxingdong/lagom&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;57&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/dennybritz/reinforcement-learning&#34; target=&#34;_blank&#34;&gt;https://github.com/dennybritz/reinforcement-learning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;58&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/unixpickle/anyrl-py&#34; target=&#34;_blank&#34;&gt;https://github.com/unixpickle/anyrl-py&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;59&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Scitator/rl-course-experiments&#34; target=&#34;_blank&#34;&gt;https://github.com/Scitator/rl-course-experiments&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;60&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/oxwhirl/pymarl&#34; target=&#34;_blank&#34;&gt;https://github.com/oxwhirl/pymarl&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
      
    </item>
    
  </channel>
</rss>
