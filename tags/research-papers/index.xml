<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Papers on </title>
    <link>/tags/research-papers/</link>
    <description>Recent content in Research Papers on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Dr. Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Dr. Hari Thapliyaal)</webMaster>
    <copyright>© 2025 Dr. Hari Thapliyaal</copyright>
    <lastBuildDate>Tue, 22 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/research-papers/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Important AI Paper List</title>
      <link>/dsblog/select-ai-papers/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/select-ai-papers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg&#34; alt=&#34;Important AI Paper List&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Important AI Paper List 
    &lt;div id=&#34;important-ai-paper-list&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#important-ai-paper-list&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduciton 
    &lt;div id=&#34;introduciton&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduciton&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors&amp;rsquo; information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like &amp;ldquo;Vivek Ramaswami, Kartikeyan Karunanidhi&amp;rdquo; it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on &amp;ldquo;google scholar&amp;rdquo;, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper with Code Resources</title>
      <link>/dsblog/paperwithcode-resources/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/paperwithcode-resources/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6091-rps-Paperwithcode-Resources.jpg&#34; alt=&#34;Paper with Code Resources&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper with Code Resources 
    &lt;div id=&#34;paper-with-code-resources&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-with-code-resources&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Papers of 2021 
    &lt;div id=&#34;trending-papers-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-papers-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;ADOP: Approximate Differentiable One-Pixel Point Rendering — Rückert et al — &lt;a href=&#34;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/adop-approximate-differentiable-one-pixel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Bayesian Learning Rule —Khan et al &lt;a href=&#34;https://paperswithcode.com/paper/the-bayesian-learning-rule&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/the-bayesian-learning-rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Program Synthesis with Large Language Models — Austin et al &lt;a href=&#34;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/program-synthesis-with-large-language-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Masked Autoencoders Are Scalable Vision Learners — He et al &lt;a href=&#34;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8-bit Optimizers via Block-wise Quantization — Dettmers et al &lt;a href=&#34;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/8-bit-optimizers-via-block-wise-quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Revisiting ResNets: Improved Training and Scaling Strategies — Bello et al &lt;a href=&#34;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/revisiting-resnets-improved-training-and&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Image Super-Resolution via Iterative Refinement — Saharia et al &lt;a href=&#34;https://paperswithcode.com/paper/image-super-resolution-via-iterative&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/image-super-resolution-via-iterative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs — Jaegle et al &lt;a href=&#34;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do Vision Transformers See Like Convolutional Neural Networks? — Raghu et al &lt;a href=&#34;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/do-vision-transformers-see-like-convolutional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions — Niepert et al &lt;a href=&#34;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Trending Libaries of 2021 
    &lt;div id=&#34;trending-libaries-of-2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#trending-libaries-of-2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PyTorch Image Models — Ross Wightman — &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34; target=&#34;_blank&#34;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Transformers — Hugging Face — &lt;a href=&#34;https://github.com/huggingface/transformers&#34; target=&#34;_blank&#34;&gt;https://github.com/huggingface/transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyTorch-GAN — Erik Linder-Norén — &lt;a href=&#34;https://github.com/eriklindernoren/PyTorch-GAN&#34; target=&#34;_blank&#34;&gt;https://github.com/eriklindernoren/PyTorch-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMDetection — OpenMMLab — &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34; target=&#34;_blank&#34;&gt;https://github.com/open-mmlab/mmdetection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Darknet — AlexeyAB — &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vision Transformer PyTorch — lucidrains — &lt;a href=&#34;https://github.com/lucidrains/vit-pytorch&#34; target=&#34;_blank&#34;&gt;https://github.com/lucidrains/vit-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;InsightFace — DeepInsight — &lt;a href=&#34;https://github.com/deepinsight/insightface&#34; target=&#34;_blank&#34;&gt;https://github.com/deepinsight/insightface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Detectron2 — Meta AI — &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;https://github.com/facebookresearch/detectron2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PaddleOCR — PaddlePaddle — &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34;&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FairSeq — Meta AI — &lt;a href=&#34;https://github.com/pytorch/fairseq&#34; target=&#34;_blank&#34;&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Top Dataset - 2021 
    &lt;div id=&#34;top-dataset---2021&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#top-dataset---2021&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MATH — Hendrycks et al &lt;a href=&#34;https://paperswithcode.com/dataset/math&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UAV-Human — Li et al &lt;a href=&#34;https://paperswithcode.com/dataset/uav-human&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/uav-human&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UPFD (User Preference-aware Fake News Detection) — Dou et al &lt;a href=&#34;https://paperswithcode.com/dataset/upfd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/upfd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OGB-LSC (OGB Large-Scale Challenge) — Hu et al &lt;a href=&#34;https://paperswithcode.com/dataset/ogb-lsc&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/ogb-lsc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CodeXGLUE —Lu et al &lt;a href=&#34;https://paperswithcode.com/dataset/codexglue&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/codexglue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AGORA — Patel et al &lt;a href=&#34;https://paperswithcode.com/dataset/agora&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/agora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEIR (Benchmarking IR) — Thakur et al &lt;a href=&#34;https://paperswithcode.com/dataset/beir&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/beir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WikiGraphs — Wang et al &lt;a href=&#34;https://paperswithcode.com/dataset/wikigraphs&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/wikigraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Few-NERD — Ding et al &lt;a href=&#34;https://paperswithcode.com/dataset/few-nerd&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/few-nerd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PASS (Pictures without humAns for Self-Supervision) —Asano et al &lt;a href=&#34;https://paperswithcode.com/dataset/pass&#34; target=&#34;_blank&#34;&gt;https://paperswithcode.com/dataset/pass&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Papers of 2022 
    &lt;div id=&#34;papers-of-2022&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#papers-of-2022&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Controllable Animation of Fluid Elements in Still Images&lt;/li&gt;
&lt;li&gt;F-SfT: Shape-From-Template With A Physics-Based Deformation Model&lt;/li&gt;
&lt;li&gt;TWIST: Two-Way Inter-Label Self-Training for Semi-Supervised 3D Instance Segmentation&lt;/li&gt;
&lt;li&gt;Do Learned Representations Respect Causal Relationships?&lt;/li&gt;
&lt;li&gt;ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic&lt;/li&gt;
&lt;li&gt;3D Moments From Near-Duplicate Photos&lt;/li&gt;
&lt;li&gt;Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization&lt;/li&gt;
&lt;li&gt;Blind2Unblind: Self-Supervised Image Denoising With Visible Blind Spots&lt;/li&gt;
&lt;li&gt;Balanced and Hierarchical Relation Learning for One-Shot Object Detection&lt;/li&gt;
&lt;li&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/li&gt;
&lt;li&gt;Stochastic Trajectory Prediction Via Motion Indeterminacy Diffusion&lt;/li&gt;
&lt;li&gt;CLRNet: Cross Layer Refinement Network for Lane Detection&lt;/li&gt;
&lt;li&gt;Motion-Aware Contrastive Video Representation Learning Via Foreground-Background Merging&lt;/li&gt;
&lt;li&gt;DINE: Domain Adaptation From Single and Multiple Black-Box Predictors&lt;/li&gt;
&lt;li&gt;FaceFormer: Speech-Driven 3D Facial Animation With Transformers&lt;/li&gt;
&lt;li&gt;Rotationally Equivariant 3D Object Detection&lt;/li&gt;
&lt;li&gt;Accelerating DETR Convergence Via Semantic-Aligned Matching&lt;/li&gt;
&lt;li&gt;Cloning Outfits From Real-World Images to 3D Characters for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;GeoNeRF: Generalizing NeRF With Geometry Priors&lt;/li&gt;
&lt;li&gt;ABPN: Adaptive Blend Pyramid Network for Real-Time Local Retouching of Ultra High-Resolution Photo&lt;/li&gt;
&lt;li&gt;Expanding Low-Density Latent Regions for Open-Set Object Detection&lt;/li&gt;
&lt;li&gt;Uformer: A General U-Shaped Transformer for Image Restoration&lt;/li&gt;
&lt;li&gt;Exploring Dual-Task Correlation for Pose Guided Person Image Generation&lt;/li&gt;
&lt;li&gt;Portrait Eyeglasses and Shadow Removal By Leveraging 3D Synthetic Data&lt;/li&gt;
&lt;li&gt;Modeling 3D Layout for Group Re-Identification&lt;/li&gt;
&lt;li&gt;Toward Fast, Flexible, and Robust Low-Light Image Enhancement&lt;/li&gt;
&lt;li&gt;Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos&lt;/li&gt;
&lt;li&gt;HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network&lt;/li&gt;
&lt;li&gt;Modular Action Concept Grounding in Semantic Video Prediction&lt;/li&gt;
&lt;li&gt;StyleSwin: Transformer-Based GAN for High-Resolution Image Generation&lt;/li&gt;
&lt;li&gt;Discrete Cosine Transform Network for Guided Depth Map Super-Resolution&lt;/li&gt;
&lt;li&gt;Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing&lt;/li&gt;
&lt;li&gt;TransGeo: Transformer Is All You Need for Cross-View Image Geo-Localization&lt;/li&gt;
&lt;li&gt;Contrastive Boundary Learning for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution&lt;/li&gt;
&lt;li&gt;CVNet: Contour Vibration Network for Building Extraction&lt;/li&gt;
&lt;li&gt;Swin Transformer V2: Scaling Up Capacity and Resolution&lt;/li&gt;
&lt;li&gt;Projective Manifold Gradient Layer for Deep Rotation Regression&lt;/li&gt;
&lt;li&gt;HCSC: Hierarchical Contrastive Selective Coding&lt;/li&gt;
&lt;li&gt;TransRank: Self-Supervised Video Representation Learning Via Ranking-Based Transformation Recognition&lt;/li&gt;
&lt;li&gt;DiSparse: Disentangled Sparsification for Multitask Model Compression&lt;/li&gt;
&lt;li&gt;Pushing The Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make A Difference&lt;/li&gt;
&lt;li&gt;Towards Efficient and Scalable Sharpness-Aware Minimization&lt;/li&gt;
&lt;li&gt;OSSO: Obtaining Skeletal Shape From Outside&lt;/li&gt;
&lt;li&gt;A Study on The Distribution of Social Biases in Self-Supervised Learning Visual Models&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes&lt;/li&gt;
&lt;li&gt;Comparing Correspondences: Video Prediction With Correspondence-Wise Losses&lt;/li&gt;
&lt;li&gt;Towards Fewer Annotations: Active Learning Via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding&lt;/li&gt;
&lt;li&gt;Few Shot Generative Model Adaption Via Relaxed Spatial Structural Alignment&lt;/li&gt;
&lt;li&gt;Enhancing Adversarial Training With Second-Order Statistics of Weights&lt;/li&gt;
&lt;li&gt;Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo&lt;/li&gt;
&lt;li&gt;Moving Window Regression: A Novel Approach to Ordinal Regression&lt;/li&gt;
&lt;li&gt;Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection&lt;/li&gt;
&lt;li&gt;Robust Optimization As Data Augmentation for Large-Scale Graphs&lt;/li&gt;
&lt;li&gt;Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients&lt;/li&gt;
&lt;li&gt;Improving The Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input&lt;/li&gt;
&lt;li&gt;ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer&lt;/li&gt;
&lt;li&gt;360MonoDepth: High-Resolution 360deg Monocular Depth Estimation&lt;/li&gt;
&lt;li&gt;POCO: Point Convolution for Surface Reconstruction&lt;/li&gt;
&lt;li&gt;Neural Texture Extraction and Distribution for Controllable Person Image Synthesis&lt;/li&gt;
&lt;li&gt;Classification-Then-Grounding: Reformulating Video Scene Graphs As Temporal Bipartite Graphs&lt;/li&gt;
&lt;li&gt;DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis&lt;/li&gt;
&lt;li&gt;ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes&lt;/li&gt;
&lt;li&gt;UNIST: Unpaired Neural Implicit Shape Translation Network&lt;/li&gt;
&lt;li&gt;APES: Articulated Part Extraction From Sprite Sheets&lt;/li&gt;
&lt;li&gt;SPAct: Self-Supervised Privacy Preservation for Action Recognition&lt;/li&gt;
&lt;li&gt;De-Rendering 3D Objects in The Wild&lt;/li&gt;
&lt;li&gt;Global Sensing and Measurements Reuse for Image Compressed Sensing&lt;/li&gt;
&lt;li&gt;Practical Evaluation of Adversarial Robustness Via Adaptive Auto Attack&lt;/li&gt;
&lt;li&gt;Cross-View Transformers for Real-Time Map-View Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Controllable Dynamic Multi-Task Architectures&lt;/li&gt;
&lt;li&gt;FastDOG: Fast Discrete Optimization on GPU&lt;/li&gt;
&lt;li&gt;Focal and Global Knowledge Distillation for Detectors&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Continual Learning&lt;/li&gt;
&lt;li&gt;Human Mesh Recovery From Multiple Shots&lt;/li&gt;
&lt;li&gt;Convolution of Convolution: Let Kernels Spatially Collaborate&lt;/li&gt;
&lt;li&gt;Make It Move: Controllable Image-to-Video Generation With Text Descriptions&lt;/li&gt;
&lt;li&gt;Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling&lt;/li&gt;
&lt;li&gt;Video-Text Representation Learning Via Differentiable Weak Temporal Alignment&lt;/li&gt;
&lt;li&gt;Bi-Directional Object-Context Prioritization Learning for Saliency Ranking&lt;/li&gt;
&lt;li&gt;Vehicle Trajectory Prediction Works, But Not Everywhere&lt;/li&gt;
&lt;li&gt;MonoDTR: Monocular 3D Object Detection With Depth-Aware Transformer&lt;/li&gt;
&lt;li&gt;Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-Shot Learning&lt;/li&gt;
&lt;li&gt;Generalized Category Discovery&lt;/li&gt;
&lt;li&gt;Contour-Hugging Heatmaps for Landmark Detection&lt;/li&gt;
&lt;li&gt;Voxel Field Fusion for 3D Object Detection&lt;/li&gt;
&lt;li&gt;DisARM: Displacement Aware Relation Module for 3D Detection&lt;/li&gt;
&lt;li&gt;MixFormer: Mixing Features Across Windows and Dimensions&lt;/li&gt;
&lt;li&gt;FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment&lt;/li&gt;
&lt;li&gt;HEAT: Holistic Edge Attention Transformer for Structured Reconstruction&lt;/li&gt;
&lt;li&gt;Mobile-Former: Bridging MobileNet and Transformer&lt;/li&gt;
&lt;li&gt;CycleMix: A Holistic Strategy for Medical Image Segmentation From Scribble Supervision&lt;/li&gt;
&lt;li&gt;VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution&lt;/li&gt;
&lt;li&gt;Towards End-to-End Unified Scene Text Detection and Layout Analysis&lt;/li&gt;
&lt;li&gt;AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation&lt;/li&gt;
&lt;li&gt;ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior&lt;/li&gt;
&lt;li&gt;End-to-End Referring Video Object Segmentation With Multimodal Transformers&lt;/li&gt;
&lt;li&gt;IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds&lt;/li&gt;
&lt;li&gt;Detecting Camouflaged Object in Frequency Domain&lt;/li&gt;
&lt;li&gt;SelfRecon: Self Reconstruction Your Digital Avatar From Monocular Video&lt;/li&gt;
&lt;li&gt;Equivariant Point Cloud Analysis Via Learning Orientations for Message Passing&lt;/li&gt;
&lt;li&gt;Node Representation Learning in Graph Via Node-to-Neighbourhood Mutual Information Maximization&lt;/li&gt;
&lt;li&gt;Semi-Supervised Video Semantic Segmentation With Inter-Frame Feature Reconstruction&lt;/li&gt;
&lt;li&gt;Amodal Segmentation Through Out-of-Task and Out-of-Distribution Generalization With A Bayesian Model&lt;/li&gt;
&lt;li&gt;How Well Do Sparse ImageNet Models Transfer?&lt;/li&gt;
&lt;li&gt;REX: Reasoning-Aware and Grounded Explanation&lt;/li&gt;
&lt;li&gt;Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes&lt;/li&gt;
&lt;li&gt;Object-Aware Video-Language Pre-Training for Retrieval&lt;/li&gt;
&lt;li&gt;MAT: Mask-Aware Transformer for Large Hole Image Inpainting&lt;/li&gt;
&lt;li&gt;Align and Prompt: Video-and-Language Pre-Training With Entity Prompts&lt;/li&gt;
&lt;li&gt;MSG-Transformer: Exchanging Local Spatial Information By Manipulating Messenger Tokens&lt;/li&gt;
&lt;li&gt;Cross Modal Retrieval With Querybank Normalisation&lt;/li&gt;
&lt;li&gt;Ray3D: Ray-Based 3D Human Pose Estimation for Monocular Absolute 3D Localization&lt;/li&gt;
&lt;li&gt;ASM-Loc: Action-Aware Segment Modeling for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs&lt;/li&gt;
&lt;li&gt;End-to-End Multi-Person Pose Estimation With Transformers&lt;/li&gt;
&lt;li&gt;REGTR: End-to-End Point Cloud Correspondences With Transformers&lt;/li&gt;
&lt;li&gt;Neural 3D Scene Reconstruction With The Manhattan-World Assumption&lt;/li&gt;
&lt;li&gt;V2C: Visual Voice Cloning&lt;/li&gt;
&lt;li&gt;Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection&lt;/li&gt;
&lt;li&gt;MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions&lt;/li&gt;
&lt;li&gt;Gait Recognition in The Wild With Dense 3D Representations and A Benchmark&lt;/li&gt;
&lt;li&gt;ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation Via Online Exploration and Synthesis&lt;/li&gt;
&lt;li&gt;QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection&lt;/li&gt;
&lt;li&gt;IDEA-Net: Dynamic 3D Point Cloud Interpolation Via Deep Embedding Alignment&lt;/li&gt;
&lt;li&gt;BEHAVE: Dataset and Method for Tracking Human Object Interactions&lt;/li&gt;
&lt;li&gt;Revisiting Random Channel Pruning for Neural Network Compression&lt;/li&gt;
&lt;li&gt;Generating Diverse and Natural 3D Human Motions From Text&lt;/li&gt;
&lt;li&gt;E-CIR: Event-Enhanced Continuous Intensity Recovery&lt;/li&gt;
&lt;li&gt;Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond&lt;/li&gt;
&lt;li&gt;Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation&lt;/li&gt;
&lt;li&gt;AziNorm: Exploiting The Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception&lt;/li&gt;
&lt;li&gt;Weakly Supervised Rotation-Invariant Aerial Object Detection Network&lt;/li&gt;
&lt;li&gt;Surface Reconstruction From Point Clouds By Learning Predictive Context Priors&lt;/li&gt;
&lt;li&gt;IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes&lt;/li&gt;
&lt;li&gt;DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Action Localization Via Representative Snippet Knowledge Propagation&lt;/li&gt;
&lt;li&gt;E2EC: An End-to-End Contour-Based Method for High-Quality High-Speed Instance Segmentation&lt;/li&gt;
&lt;li&gt;BatchFormer: Learning To Explore Sample Relationships for Robust Representation Learning&lt;/li&gt;
&lt;li&gt;Self-Supervised Image-Specific Prototype Exploration for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Learning Multi-View Aggregation in The Wild for Large-Scale 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition&lt;/li&gt;
&lt;li&gt;Clothes-Changing Person Re-Identification With RGB Modality Only&lt;/li&gt;
&lt;li&gt;Robust Image Forgery Detection Over Online Social Network Shared Images&lt;/li&gt;
&lt;li&gt;Representation Compensation Networks for Continual Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Tracking People By Predicting 3D Appearance, Location and Pose&lt;/li&gt;
&lt;li&gt;Text2Mesh: Text-Driven Neural Stylization for Meshes&lt;/li&gt;
&lt;li&gt;C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image&lt;/li&gt;
&lt;li&gt;Forward Compatible Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Weakly Supervised Object Localization As Domain Adaption&lt;/li&gt;
&lt;li&gt;Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation&lt;/li&gt;
&lt;li&gt;Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching&lt;/li&gt;
&lt;li&gt;Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation&lt;/li&gt;
&lt;li&gt;MatteFormer: Transformer-Based Image Matting Via Prior-Tokens&lt;/li&gt;
&lt;li&gt;Video Shadow Detection Via Spatio-Temporal Interpolation Consistency Training&lt;/li&gt;
&lt;li&gt;Robust and Accurate Superquadric Recovery: A Probabilistic Approach&lt;/li&gt;
&lt;li&gt;Grounding Answers for Visual Questions Asked By Visually Impaired People&lt;/li&gt;
&lt;li&gt;Sparse Instance Activation for Real-Time Instance Segmentation&lt;/li&gt;
&lt;li&gt;VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning&lt;/li&gt;
&lt;li&gt;MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis&lt;/li&gt;
&lt;li&gt;Towards Implicit Text-Guided 3D Shape Generation&lt;/li&gt;
&lt;li&gt;SoftCollage: A Differentiable Probabilistic Tree Generator for Image Collage&lt;/li&gt;
&lt;li&gt;Query and Attention Augmentation for Knowledge-Based Explainable Reasoning&lt;/li&gt;
&lt;li&gt;Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality&lt;/li&gt;
&lt;li&gt;Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection&lt;/li&gt;
&lt;li&gt;Fine-Grained Object Classification Via Self-Supervised Pose Alignment&lt;/li&gt;
&lt;li&gt;Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding&lt;/li&gt;
&lt;li&gt;Fine-Grained Temporal Contrastive Learning for Weakly-Supervised Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Relieving Long-Tailed Instance Segmentation Via Pairwise Class Balance&lt;/li&gt;
&lt;li&gt;Online Convolutional Re-Parameterization&lt;/li&gt;
&lt;li&gt;Mimicking The Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning&lt;/li&gt;
&lt;li&gt;RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition&lt;/li&gt;
&lt;li&gt;Personalized Image Aesthetics Assessment With Rich Attributes&lt;/li&gt;
&lt;li&gt;Part-Based Pseudo Label Refinement for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;HDNet: High-Resolution Dual-Domain Learning for Spectral Compressive Imaging&lt;/li&gt;
&lt;li&gt;OW-DETR: Open-World Detection Transformer&lt;/li&gt;
&lt;li&gt;Learning Deep Implicit Functions for 3D Shapes With Dynamic Code Clouds&lt;/li&gt;
&lt;li&gt;Reversible Vision Transformers&lt;/li&gt;
&lt;li&gt;Amodal Panoptic Segmentation&lt;/li&gt;
&lt;li&gt;Correlation Verification for Image Retrieval&lt;/li&gt;
&lt;li&gt;Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut&lt;/li&gt;
&lt;li&gt;Exploring Structure-Aware Transformer Over Interaction Proposals for Human-Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Decoupled Multi-Task Learning With Cyclical Self-Regulation for Face Parsing&lt;/li&gt;
&lt;li&gt;Glass: Geometric Latent Augmentation for Shape Spaces&lt;/li&gt;
&lt;li&gt;DPICT: Deep Progressive Image Compression Using Trit-Planes&lt;/li&gt;
&lt;li&gt;Text to Image Generation With Semantic-Spatial Aware GAN&lt;/li&gt;
&lt;li&gt;Generalizable Cross-Modality Medical Image Segmentation Via Style Augmentation and Dual Normalization&lt;/li&gt;
&lt;li&gt;Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model&lt;/li&gt;
&lt;li&gt;Interactive Segmentation and Visualization for Tiny Objects in Multi-Megapixel Images&lt;/li&gt;
&lt;li&gt;Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture&lt;/li&gt;
&lt;li&gt;Surface Representation for Point Clouds&lt;/li&gt;
&lt;li&gt;Implicit Motion Handling for Video Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides&lt;/li&gt;
&lt;li&gt;Learning With Twin Noisy Labels for Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;Optical Flow Estimation for Spiking Camera&lt;/li&gt;
&lt;li&gt;GradViT: Gradient Inversion of Vision Transformers&lt;/li&gt;
&lt;li&gt;Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution Via Cycle-Projected Mutual Learning&lt;/li&gt;
&lt;li&gt;Joint Global and Local Hierarchical Priors for Learned Image Compression&lt;/li&gt;
&lt;li&gt;Knowledge Distillation Via The Target-Aware Transformer&lt;/li&gt;
&lt;li&gt;Subspace Adversarial Training&lt;/li&gt;
&lt;li&gt;3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection&lt;/li&gt;
&lt;li&gt;Image Segmentation Using Text and Image Prompts&lt;/li&gt;
&lt;li&gt;AutoMine: An Unmanned Mine Dataset&lt;/li&gt;
&lt;li&gt;Background Activation Suppression for Weakly Supervised Object Localization&lt;/li&gt;
&lt;li&gt;Synthetic Generation of Face Videos With Plethysmograph Physiology&lt;/li&gt;
&lt;li&gt;Hallucinated Neural Radiance Fields in The Wild&lt;/li&gt;
&lt;li&gt;Global Tracking Transformers&lt;/li&gt;
&lt;li&gt;Backdoor Attacks on Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;GMFlow: Learning Optical Flow Via Global Matching&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Explore Spatio-Temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline&lt;/li&gt;
&lt;li&gt;Graph-Based Spatial Transformer With Memory Replay for Multi-Future Pedestrian Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Scanline Homographies for Rolling-Shutter Plane Absolute Pose&lt;/li&gt;
&lt;li&gt;AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-Time Image Enhancement&lt;/li&gt;
&lt;li&gt;Recurrent Glimpse-Based Decoder for Detection With Transformer&lt;/li&gt;
&lt;li&gt;SimMIM: A Simple Framework for Masked Image Modeling&lt;/li&gt;
&lt;li&gt;Label Matching Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;RegionCLIP: Region-Based Language-Image Pretraining&lt;/li&gt;
&lt;li&gt;Video Frame Interpolation Transformer&lt;/li&gt;
&lt;li&gt;BCOT: A Markerless High-Precision 3D Object Tracking Benchmark&lt;/li&gt;
&lt;li&gt;Omni-DETR: Omni-Supervised Object Detection With Transformers&lt;/li&gt;
&lt;li&gt;Transferable Sparse Adversarial Attack&lt;/li&gt;
&lt;li&gt;CREAM: Weakly Supervised Object Localization Via Class RE-Activation Mapping&lt;/li&gt;
&lt;li&gt;VALHALLA: Visual Hallucination for Machine Translation&lt;/li&gt;
&lt;li&gt;HINT: Hierarchical Neuron Concept Explainer&lt;/li&gt;
&lt;li&gt;Neural Face Identification in A 2D Wireframe Projection of A Manifold Object&lt;/li&gt;
&lt;li&gt;Nonuniform-to-Uniform Quantization: Towards Accurate Quantization Via Generalized Straight-Through Estimation&lt;/li&gt;
&lt;li&gt;An Empirical Study of End-to-End Temporal Action Detection&lt;/li&gt;
&lt;li&gt;Object Localization Under Single Coarse Point Supervision&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Accurate Siamese Tracking&lt;/li&gt;
&lt;li&gt;Non-Parametric Depth Distribution Modelling Based Depth Inference for Multi-View Stereo&lt;/li&gt;
&lt;li&gt;Equalized Focal Loss for Dense Long-Tailed Object Detection&lt;/li&gt;
&lt;li&gt;DeepDPM: Deep Clustering With An Unknown Number of Clusters&lt;/li&gt;
&lt;li&gt;ISDNet: Integrating Shallow and Deep Networks for Efficient Ultra-High Resolution Segmentation&lt;/li&gt;
&lt;li&gt;Unsupervised Domain Adaptation for Nighttime Aerial Tracking&lt;/li&gt;
&lt;li&gt;RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs&lt;/li&gt;
&lt;li&gt;Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction&lt;/li&gt;
&lt;li&gt;A Variational Bayesian Method for Similarity Learning in Non-Rigid Image Registration&lt;/li&gt;
&lt;li&gt;Not Just Selection, But Exploration: Online Class-Incremental Continual Learning Via Dual View Consistency&lt;/li&gt;
&lt;li&gt;Coupling Vision and Proprioception for Navigation of Legged Robots&lt;/li&gt;
&lt;li&gt;Exploiting Rigidity Constraints for LiDAR Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;EMOCA: Emotion Driven Monocular Face Capture and Animation&lt;/li&gt;
&lt;li&gt;Quarantine: Sparsity Can Uncover The Trojan Attack Trigger for Free&lt;/li&gt;
&lt;li&gt;AlignQ: Alignment Quantization With ADMM-Based Correlation Preservation&lt;/li&gt;
&lt;li&gt;Interactive Multi-Class Tiny-Object Detection&lt;/li&gt;
&lt;li&gt;Learning From Pixel-Level Noisy Label: A New Perspective for Light Field Saliency Detection&lt;/li&gt;
&lt;li&gt;Multi-View Depth Estimation By Fusing Single-View Depth Probability With Multi-View Geometry&lt;/li&gt;
&lt;li&gt;Slimmable Domain Adaptation&lt;/li&gt;
&lt;li&gt;High-Resolution Image Harmonization Via Collaborative Dual Transformations&lt;/li&gt;
&lt;li&gt;MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Supervised Neural Articulated Shape and Appearance Models&lt;/li&gt;
&lt;li&gt;Topology Preserving Local Road Network Estimation From Single Onboard Camera Image&lt;/li&gt;
&lt;li&gt;Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes&lt;/li&gt;
&lt;li&gt;SwinTextSpotter: Scene Text Spotting Via Better Synergy Between Text Detection and Text Recognition&lt;/li&gt;
&lt;li&gt;Deblur-NeRF: Neural Radiance Fields From Blurry Images&lt;/li&gt;
&lt;li&gt;Whose Track Is It Anyway? Improving Robustness to Tracking Errors With Affinity-Based Trajectory Prediction&lt;/li&gt;
&lt;li&gt;Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation&lt;/li&gt;
&lt;li&gt;Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning&lt;/li&gt;
&lt;li&gt;Blind Image Super-Resolution With Elaborate Degradation Modeling on Noise and Kernel&lt;/li&gt;
&lt;li&gt;Faithful Extreme Rescaling Via Generative Prior Reciprocated Invertible Representations&lt;/li&gt;
&lt;li&gt;Proto2Proto: Can You Recognize The Car, The Way I Do?&lt;/li&gt;
&lt;li&gt;TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing&lt;/li&gt;
&lt;li&gt;Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Habitat-Web: Learning Embodied Object-Search Strategies From Human Demonstrations at Scale&lt;/li&gt;
&lt;li&gt;Simple But Effective: CLIP Embeddings for Embodied AI&lt;/li&gt;
&lt;li&gt;NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition&lt;/li&gt;
&lt;li&gt;Collaborative Transformers for Grounded Situation Recognition&lt;/li&gt;
&lt;li&gt;CPPF: Towards Robust Category-Level 9D Pose Estimation in The Wild&lt;/li&gt;
&lt;li&gt;Continual Test-Time Domain Adaptation&lt;/li&gt;
&lt;li&gt;Dynamic MLP for Fine-Grained Image Classification By Leveraging Geographical and Temporal Information&lt;/li&gt;
&lt;li&gt;MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering&lt;/li&gt;
&lt;li&gt;Fair Contrastive Learning for Facial Attribute Classification&lt;/li&gt;
&lt;li&gt;Directional Self-Supervised Learning for Heavy Image Augmentations&lt;/li&gt;
&lt;li&gt;No-Reference Point Cloud Quality Assessment Via Domain Adaptation&lt;/li&gt;
&lt;li&gt;Comprehending and Ordering Semantics for Image Captioning&lt;/li&gt;
&lt;li&gt;A Large-Scale Comprehensive Dataset and Copy-Overlap Aware Evaluation Protocol for Segment-Level Video Copy Detection&lt;/li&gt;
&lt;li&gt;Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification&lt;/li&gt;
&lt;li&gt;HeadNeRF: A Real-Time NeRF-Based Parametric Head Model&lt;/li&gt;
&lt;li&gt;Occlusion-Robust Face Alignment Using A Viewpoint-Invariant Hierarchical Network Architecture&lt;/li&gt;
&lt;li&gt;IDR: Self-Supervised Image Denoising Via Iterative Data Refinement&lt;/li&gt;
&lt;li&gt;MogFace: Towards A Deeper Appreciation on Face Detection&lt;/li&gt;
&lt;li&gt;Learning Affinity From Attention: End-to-End Weakly-Supervised Semantic Segmentation With Transformers&lt;/li&gt;
&lt;li&gt;CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation&lt;/li&gt;
&lt;li&gt;FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos&lt;/li&gt;
&lt;li&gt;Learning To Detect Mobile Objects From LiDAR Scans Without Labels&lt;/li&gt;
&lt;li&gt;WildNet: Learning Domain Generalized Semantic Segmentation From The Wild&lt;/li&gt;
&lt;li&gt;DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection&lt;/li&gt;
&lt;li&gt;Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Generating Diverse 3D Reconstructions From A Single Occluded Face Image&lt;/li&gt;
&lt;li&gt;Stand-Alone Inter-Frame Attention in Video Models&lt;/li&gt;
&lt;li&gt;Large-Scale Pre-Training for Person Re-Identification With Noisy Labels&lt;/li&gt;
&lt;li&gt;Semantic Segmentation By Early Region Proxy&lt;/li&gt;
&lt;li&gt;LD-ConGR: A Large RGB-D Video Dataset for Long-Distance Continuous Gesture Recognition&lt;/li&gt;
&lt;li&gt;HVH: Learning A Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture&lt;/li&gt;
&lt;li&gt;Rethinking Visual Geo-Localization for Large-Scale Applications&lt;/li&gt;
&lt;li&gt;The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy&lt;/li&gt;
&lt;li&gt;ViM: Out-of-Distribution With Virtual-Logit Matching&lt;/li&gt;
&lt;li&gt;Class-Aware Contrastive Semi-Supervised Learning&lt;/li&gt;
&lt;li&gt;Ditto: Building Digital Twins of Articulated Objects From Interaction&lt;/li&gt;
&lt;li&gt;Adaptive Early-Learning Correction for Segmentation From Noisy Annotations&lt;/li&gt;
&lt;li&gt;Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation&lt;/li&gt;
&lt;li&gt;RSTT: Real-Time Spatial Temporal Transformer for Space-Time Video Super-Resolution&lt;/li&gt;
&lt;li&gt;Partial Class Activation Attention for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Multi-Scale Memory-Based Video Deblurring&lt;/li&gt;
&lt;li&gt;A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching&lt;/li&gt;
&lt;li&gt;Geometric Structure Preserving Warp for Natural Image Stitching&lt;/li&gt;
&lt;li&gt;GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping&lt;/li&gt;
&lt;li&gt;Conditional Prompt Learning for Vision-Language Models&lt;/li&gt;
&lt;li&gt;Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification&lt;/li&gt;
&lt;li&gt;Undoing The Damage of Label Shift for Cross-Domain Semantic Segmentation&lt;/li&gt;
&lt;li&gt;FisherMatch: Semi-Supervised Rotation Regression Via Entropy-Based Filtering&lt;/li&gt;
&lt;li&gt;Affine Medical Image Registration With Coarse-To-Fine Vision Transformer&lt;/li&gt;
&lt;li&gt;A Differentiable Two-Stage Alignment Scheme for Burst Image Reconstruction With Large Shift&lt;/li&gt;
&lt;li&gt;Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes&lt;/li&gt;
&lt;li&gt;Restormer: Efficient Transformer for High-Resolution Image Restoration&lt;/li&gt;
&lt;li&gt;IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation&lt;/li&gt;
&lt;li&gt;Large Loss Matters in Weakly Supervised Multi-Label Classification&lt;/li&gt;
&lt;li&gt;Neural Inertial Localization&lt;/li&gt;
&lt;li&gt;GraftNet: Towards Domain Generalized Stereo Matching With A Broad-Spectrum and Task-Oriented Feature&lt;/li&gt;
&lt;li&gt;VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Catching Both Gray and Black Swans: Open-Set Supervised Anomaly Detection&lt;/li&gt;
&lt;li&gt;MLSLT: Towards Multilingual Sign Language Translation&lt;/li&gt;
&lt;li&gt;Towards An End-to-End Framework for Flow-Guided Video Inpainting&lt;/li&gt;
&lt;li&gt;Contrastive Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;MotionAug: Augmentation With Physical Correction for Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Modeling Indirect Illumination for Inverse Rendering&lt;/li&gt;
&lt;li&gt;TransWeather: Transformer-Based Restoration of Images Degraded By Adverse Weather Conditions&lt;/li&gt;
&lt;li&gt;H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection&lt;/li&gt;
&lt;li&gt;P3Depth: Monocular Depth Estimation With A Piecewise Planarity Prior&lt;/li&gt;
&lt;li&gt;GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection&lt;/li&gt;
&lt;li&gt;Simple Multi-Dataset Detection&lt;/li&gt;
&lt;li&gt;Proactive Image Manipulation Detection&lt;/li&gt;
&lt;li&gt;StyTr2: Image Style Transfer With Transformers&lt;/li&gt;
&lt;li&gt;Global Matching With Overlapping Attention for Optical Flow Estimation&lt;/li&gt;
&lt;li&gt;Language As Queries for Referring Video Object Segmentation&lt;/li&gt;
&lt;li&gt;MViTv2: Improved Multiscale Vision Transformers for Classification and Detection&lt;/li&gt;
&lt;li&gt;Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language&lt;/li&gt;
&lt;li&gt;Rethinking Efficient Lane Detection Via Curve Modeling&lt;/li&gt;
&lt;li&gt;Self-Supervised Arbitrary-Scale Point Clouds Upsampling Via Implicit Neural Representation&lt;/li&gt;
&lt;li&gt;Co-Advise: Cross Inductive Bias Distillation&lt;/li&gt;
&lt;li&gt;AdaMixer: A Fast-Converging Query-Based Object Detector&lt;/li&gt;
&lt;li&gt;DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification&lt;/li&gt;
&lt;li&gt;BEVT: BERT Pretraining of Video Transformers&lt;/li&gt;
&lt;li&gt;Deep Generalized Unfolding Networks for Image Restoration&lt;/li&gt;
&lt;li&gt;VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;Deep Unlearning Via Randomized Conditionally Independent Hessians&lt;/li&gt;
&lt;li&gt;Revisiting Skeleton-Based Action Recognition&lt;/li&gt;
&lt;li&gt;Stereo Depth From Events Cameras: Concentrate and Focus on The Future&lt;/li&gt;
&lt;li&gt;A Simple Data Mixing Prior for Improving Self-Supervised Learning&lt;/li&gt;
&lt;li&gt;Knowledge Distillation As Efficient Pre-Training: Faster Convergence, Higher Data-Efficiency, and Better Transferability&lt;/li&gt;
&lt;li&gt;BigDL 2.0: Seamless Scaling of AI Pipelines From Laptops to Distributed Cluster&lt;/li&gt;
&lt;li&gt;Attentive Fine-Grained Structured Sparsity for Image Restoration&lt;/li&gt;
&lt;li&gt;Learning Fair Classifiers With Partially Annotated Group Labels&lt;/li&gt;
&lt;li&gt;NightLab: A Dual-Level Architecture With Hardness Detection for Segmentation at Night&lt;/li&gt;
&lt;li&gt;Constrained Few-Shot Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Threshold Matters in WSSS: Manipulating The Activation for The Robust and Accurate Segmentation Model Against Thresholds&lt;/li&gt;
&lt;li&gt;TransMVSNet: Global Context-Aware Multi-View Stereo Network With Transformers&lt;/li&gt;
&lt;li&gt;DPGEN: Differentially Private Generative Energy-Guided Network for Natural Image Synthesis&lt;/li&gt;
&lt;li&gt;The Majority Can Help The Minority: Context-Rich Minority Oversampling for Long-Tailed Classification&lt;/li&gt;
&lt;li&gt;IntentVizor: Towards Generic Query Guided Interactive Video Summarization&lt;/li&gt;
&lt;li&gt;Shape-Invariant 3D Adversarial Point Clouds&lt;/li&gt;
&lt;li&gt;Bootstrapping ViTs: Towards Liberating Vision Transformers From Pre-Training&lt;/li&gt;
&lt;li&gt;PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents&lt;/li&gt;
&lt;li&gt;Meta-Attention for ViT-Backed Continual Learning&lt;/li&gt;
&lt;li&gt;DST: Dynamic Substitute Training for Data-Free Black-Box Attack&lt;/li&gt;
&lt;li&gt;Unified Contrastive Learning in Image-Text-Label Space&lt;/li&gt;
&lt;li&gt;Unsupervised Pre-Training for Temporal Action Localization Tasks&lt;/li&gt;
&lt;li&gt;Look Outside The Room: Synthesizing A Consistent Long-Term 3D Scene Video From A Single Image&lt;/li&gt;
&lt;li&gt;High-Fidelity Human Avatars From A Single RGB Camera&lt;/li&gt;
&lt;li&gt;Multiview Transformers for Video Recognition&lt;/li&gt;
&lt;li&gt;How Good Is Aesthetic Ability of A Fashion Model?&lt;/li&gt;
&lt;li&gt;Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds&lt;/li&gt;
&lt;li&gt;Sequential Voting With Relational Box Fields for Active Object Detection&lt;/li&gt;
&lt;li&gt;Semantic-Aware Auto-Encoders for Self-Supervised Representation Learning&lt;/li&gt;
&lt;li&gt;Consistency Learning Via Decoding Path Augmentation for Transformers in Human Object Interaction Detection&lt;/li&gt;
&lt;li&gt;Consistent Explanations By Contrastive Learning&lt;/li&gt;
&lt;li&gt;Hierarchical Modular Network for Video Captioning&lt;/li&gt;
&lt;li&gt;Depth Estimation By Combining Binocular Stereo and Monocular Structured-Light&lt;/li&gt;
&lt;li&gt;Salient-to-Broad Transition for Video Person Re-Identification&lt;/li&gt;
&lt;li&gt;DeeCap: Dynamic Early Exiting for Efficient Image Captioning&lt;/li&gt;
&lt;li&gt;RepMLPNet: Hierarchical Vision MLP With Re-Parameterized Locality&lt;/li&gt;
&lt;li&gt;DR.VIC: Decomposition and Reasoning for Video Individual Counting&lt;/li&gt;
&lt;li&gt;ARCS: Accurate Rotation and Correspondence Search&lt;/li&gt;
&lt;li&gt;Learning To Anticipate Future With Dynamic Context Removal&lt;/li&gt;
&lt;li&gt;GCFSR: A Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors&lt;/li&gt;
&lt;li&gt;On The Integration of Self-Attention and Convolution&lt;/li&gt;
&lt;li&gt;Domain Adaptation on Point Clouds Via Geometry-Aware Implicits&lt;/li&gt;
&lt;li&gt;GroupViT: Semantic Segmentation Emerges From Text Supervision&lt;/li&gt;
&lt;li&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/li&gt;
&lt;li&gt;BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks Via Image Quantization and Contrastive Adversarial Learning&lt;/li&gt;
&lt;li&gt;Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation&lt;/li&gt;
&lt;li&gt;Towards Better Plasticity-Stability Trade-Off in Incremental Learning: A Simple Linear Connector&lt;/li&gt;
&lt;li&gt;Topology-Preserving Shape Reconstruction and Registration Via Neural Diffeomorphic Flow&lt;/li&gt;
&lt;li&gt;Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection&lt;/li&gt;
&lt;li&gt;MAXIM: Multi-Axis MLP for Image Processing&lt;/li&gt;
&lt;li&gt;Learning Part Segmentation Through Unsupervised Domain Adaptation From Synthetic Vehicles&lt;/li&gt;
&lt;li&gt;PSTR: End-to-End One-Step Person Search With Transformers&lt;/li&gt;
&lt;li&gt;NFormer: Robust Person Re-Identification With Neighbor Transformer&lt;/li&gt;
&lt;li&gt;Bridging Global Context Interactions for High-Fidelity Image Completion&lt;/li&gt;
&lt;li&gt;SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning&lt;/li&gt;
&lt;li&gt;Not All Tokens Are Equal: Human-Centric Visual Analysis Via Token Clustering Transformer&lt;/li&gt;
&lt;li&gt;Temporally Efficient Vision Transformer for Video Instance Segmentation&lt;/li&gt;
&lt;li&gt;The Devil Is in The Margin: Margin-Based Label Smoothing for Network Calibration&lt;/li&gt;
&lt;li&gt;NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks&lt;/li&gt;
&lt;li&gt;WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation&lt;/li&gt;
&lt;li&gt;Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding&lt;/li&gt;
&lt;li&gt;E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition&lt;/li&gt;
&lt;li&gt;OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization&lt;/li&gt;
&lt;li&gt;OnePose: One-Shot Object Pose Estimation Without CAD Models&lt;/li&gt;
&lt;li&gt;Rethinking Minimal Sufficient Representation in Contrastive Learning&lt;/li&gt;
&lt;li&gt;Scalable Penalized Regression for Noise Detection in Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;Federated Class-Incremental Learning&lt;/li&gt;
&lt;li&gt;Show, Deconfound and Tell: Image Captioning With Causal Inference&lt;/li&gt;
&lt;li&gt;MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image&lt;/li&gt;
&lt;li&gt;Parameter-Free Online Test-Time Adaptation&lt;/li&gt;
&lt;li&gt;SIGMA: Semantic-Complete Graph Matching for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;No Pain, Big Gain: Classify Dynamic Point Cloud Sequences With Static Models By Fitting Feature-Level Space-Time Surfaces&lt;/li&gt;
&lt;li&gt;HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging&lt;/li&gt;
&lt;li&gt;Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space&lt;/li&gt;
&lt;li&gt;Learning To Estimate Robust 3D Human Mesh From In-the-Wild Crowded Scenes&lt;/li&gt;
&lt;li&gt;Detecting Deepfakes With Self-Blended Images&lt;/li&gt;
&lt;li&gt;Implicit Sample Extension for Unsupervised Person Re-Identification&lt;/li&gt;
&lt;li&gt;Energy-Based Latent Aligner for Incremental Learning&lt;/li&gt;
&lt;li&gt;Towards Semi-Supervised Deep Facial Expression Recognition With An Adaptive Confidence Margin&lt;/li&gt;
&lt;li&gt;Group R-CNN for Weakly Semi-Supervised Object Detection With Points&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Action Transition Learning for Stochastic Human Motion Prediction&lt;/li&gt;
&lt;li&gt;Hybrid Relation Guided Set Matching for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Cross-Patch Dense Contrastive Learning for Semi-Supervised Segmentation of Cellular Nuclei in Histopathologic Images&lt;/li&gt;
&lt;li&gt;Generalized Binary Search Network for Highly-Efficient Multi-View Stereo&lt;/li&gt;
&lt;li&gt;SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation&lt;/li&gt;
&lt;li&gt;FlexIT: Towards Flexible Semantic Image Translation&lt;/li&gt;
&lt;li&gt;CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow&lt;/li&gt;
&lt;li&gt;BoxeR: Box-Attention for 2D and 3D Transformers&lt;/li&gt;
&lt;li&gt;Neural Architecture Search With Representation Mutual Information&lt;/li&gt;
&lt;li&gt;Can Neural Nets Learn The Same Model Twice? Investigating Reproducibility and Double Descent From The Decision Boundary Perspective&lt;/li&gt;
&lt;li&gt;Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction&lt;/li&gt;
&lt;li&gt;Multi-View Transformer for 3D Visual Grounding&lt;/li&gt;
&lt;li&gt;Structured Sparse R-CNN for Direct Scene Graph Generation&lt;/li&gt;
&lt;li&gt;BARC: Learning To Regress 3D Dog Shape From Images By Exploiting Breed Information&lt;/li&gt;
&lt;li&gt;PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models&lt;/li&gt;
&lt;li&gt;Towards Understanding Adversarial Robustness of Optical Flow Networks&lt;/li&gt;
&lt;li&gt;Lifelong Graph Learning&lt;/li&gt;
&lt;li&gt;Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning&lt;/li&gt;
&lt;li&gt;Computing Wasserstein-p Distance Between Images With Linear Cost&lt;/li&gt;
&lt;li&gt;Unsupervised Representation Learning for Binary Networks By Joint Classifier Learning&lt;/li&gt;
&lt;li&gt;Large-Scale Video Panoptic Segmentation in The Wild: A Benchmark&lt;/li&gt;
&lt;li&gt;GrainSpace: A Large-Scale Dataset for Fine-Grained and Domain-Adaptive Recognition of Cereal Grains&lt;/li&gt;
&lt;li&gt;Learning Modal-Invariant and Temporal-Memory for Video-Based Visible-Infrared Person Re-Identification&lt;/li&gt;
&lt;li&gt;MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning&lt;/li&gt;
&lt;li&gt;Oriented RepPoints for Aerial Object Detection&lt;/li&gt;
&lt;li&gt;Weakly Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning&lt;/li&gt;
&lt;li&gt;Low-Resource Adaptation for Personalized Co-Speech Gesture Generation&lt;/li&gt;
&lt;li&gt;Task-Specific Inconsistency Alignment for Domain Adaptive Object Detection&lt;/li&gt;
&lt;li&gt;MS2DG-Net: Progressive Correspondence Learning Via Multiple Sparse Semantics Dynamic Graph&lt;/li&gt;
&lt;li&gt;Learning To Listen: Modeling Non-Deterministic Dyadic Facial Motion&lt;/li&gt;
&lt;li&gt;Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation From Monocular Video&lt;/li&gt;
&lt;li&gt;MixFormer: End-to-End Tracking With Iterative Mixed Attention&lt;/li&gt;
&lt;li&gt;Plenoxels: Radiance Fields Without Neural Networks&lt;/li&gt;
&lt;li&gt;Selective-Supervised Contrastive Learning With Noisy Labels&lt;/li&gt;
&lt;li&gt;SimT: Handling Open-Set Noise for Domain Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Frequency-Driven Imperceptible Adversarial Attack on Semantic Similarity&lt;/li&gt;
&lt;li&gt;Video Demoireing With Relation-Based Temporal Consistency&lt;/li&gt;
&lt;li&gt;Industrial Style Transfer With Large-Scale Geometric Warping and Content Preservation&lt;/li&gt;
&lt;li&gt;Modeling Image Composition for Complex Scene Generation&lt;/li&gt;
&lt;li&gt;Decoupling Zero-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions&lt;/li&gt;
&lt;li&gt;Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting The Adversarial Transferability&lt;/li&gt;
&lt;li&gt;IFOR: Iterative Flow Minimization for Robotic Object Rearrangement&lt;/li&gt;
&lt;li&gt;Zero Experience Required: Plug &amp;amp; Play Modular Transfer Learning for Semantic Visual Navigation&lt;/li&gt;
&lt;li&gt;TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation&lt;/li&gt;
&lt;li&gt;The Wanderings of Odysseus in 3D Scenes&lt;/li&gt;
&lt;li&gt;All-in-One Image Restoration for Unknown Corruption&lt;/li&gt;
&lt;li&gt;PUMP: Pyramidal and Uniqueness Matching Priors for Unsupervised Learning of Local Descriptors&lt;/li&gt;
&lt;li&gt;MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video&lt;/li&gt;
&lt;li&gt;RCP: Recurrent Closest Point for Point Cloud&lt;/li&gt;
&lt;li&gt;A Dual Weighting Label Assignment Scheme for Object Detection&lt;/li&gt;
&lt;li&gt;Hyperbolic Vision Transformers: Combining Improvements in Metric Learning&lt;/li&gt;
&lt;li&gt;Instance-Aware Dynamic Neural Network Quantization&lt;/li&gt;
&lt;li&gt;Exploring Effective Data for Surrogate Training Towards Black-Box Attack&lt;/li&gt;
&lt;li&gt;JRDB-Act: A Large-Scale Dataset for Spatio-Temporal Action, Social Group and Activity Detection&lt;/li&gt;
&lt;li&gt;Investigating Top-k White-Box and Transferable Black-Box Attack&lt;/li&gt;
&lt;li&gt;Decoupling and Recoupling Spatiotemporal Representation for RGB-D-Based Motion Recognition&lt;/li&gt;
&lt;li&gt;A Self-Supervised Descriptor for Image Copy Detection&lt;/li&gt;
&lt;li&gt;Negative-Aware Attention Framework for Image-Text Matching&lt;/li&gt;
&lt;li&gt;An Image Patch Is A Wave: Phase-Aware Vision MLP&lt;/li&gt;
&lt;li&gt;Shunted Self-Attention Via Multi-Scale Token Aggregation&lt;/li&gt;
&lt;li&gt;Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression&lt;/li&gt;
&lt;li&gt;Recurrent Variational Network: A Deep Learning Inverse Problem Solver Applied to The Task of Accelerated MRI Reconstruction&lt;/li&gt;
&lt;li&gt;Surpassing The Human Accuracy: Detecting Gallbladder Cancer From USG Images With Curriculum Learning&lt;/li&gt;
&lt;li&gt;Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond&lt;/li&gt;
&lt;li&gt;TrackFormer: Multi-Object Tracking With Transformers&lt;/li&gt;
&lt;li&gt;3D Shape Reconstruction From 2D Images With Disentangled Attribute Flow&lt;/li&gt;
&lt;li&gt;Feature Statistics Mixing Regularization for Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;OpenTAL: Towards Open Set Temporal Action Localization&lt;/li&gt;
&lt;li&gt;Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection&lt;/li&gt;
&lt;li&gt;Ego4D: Around The World in 3,000 Hours of Egocentric Video&lt;/li&gt;
&lt;li&gt;Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Weakly Supervised Semantic Segmentation Using Out-of-Distribution Data&lt;/li&gt;
&lt;li&gt;DAD-3DHeads: A Large-Scale Dense, Accurate and Diverse Dataset for 3D Head Alignment From A Single Image&lt;/li&gt;
&lt;li&gt;Reconstructing Surfaces for Sparse Point Clouds With On-Surface Priors&lt;/li&gt;
&lt;li&gt;VCLIMB: A Novel Video Class Incremental Learning Benchmark&lt;/li&gt;
&lt;li&gt;Robust Equivariant Imaging: A Fully Unsupervised Framework for Learning To Image From Noisy and Partial Measurements&lt;/li&gt;
&lt;li&gt;ST++: Make Self-Training Work Better for Semi-Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Interacting Attention Graph for Single Image Two-Hand Reconstruction&lt;/li&gt;
&lt;li&gt;Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task&lt;/li&gt;
&lt;li&gt;Cross-Image Relational Knowledge Distillation for Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Towards Layer-Wise Image Vectorization&lt;/li&gt;
&lt;li&gt;Scenic: A JAX Library for Computer Vision Research and Beyond&lt;/li&gt;
&lt;li&gt;Real-Time Object Detection for Streaming Perception&lt;/li&gt;
&lt;li&gt;VisualHow: Multimodal Problem Solving&lt;/li&gt;
&lt;li&gt;Spatial Commonsense Graph for Object Localisation in Partial Scenes&lt;/li&gt;
&lt;li&gt;OSSGAN: Open-Set Semi-Supervised Image Generation&lt;/li&gt;
&lt;li&gt;Bi-Level Alignment for Cross-Domain Crowd Counting&lt;/li&gt;
&lt;li&gt;ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation&lt;/li&gt;
&lt;li&gt;Efficient Multi-View Stereo By Iterative Dynamic Cost Volume&lt;/li&gt;
&lt;li&gt;TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing&lt;/li&gt;
&lt;li&gt;Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework&lt;/li&gt;
&lt;li&gt;SGTR: End-to-End Scene Graph Generation With Transformer&lt;/li&gt;
&lt;li&gt;Decoupled Knowledge Distillation&lt;/li&gt;
&lt;li&gt;DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection&lt;/li&gt;
&lt;li&gt;Reusing The Task-Specific Classifier As A Discriminator: Discriminator-Free Adversarial Domain Adaptation&lt;/li&gt;
&lt;li&gt;Show Me What and Tell Me How: Video Synthesis Via Multimodal Conditioning&lt;/li&gt;
&lt;li&gt;SIMBAR: Single Image-Based Scene Relighting for Effective Data Augmentation for Automated Driving Vision Tasks&lt;/li&gt;
&lt;li&gt;Multi-Label Classification With Partial Annotations Using Class-Aware Selective Loss&lt;/li&gt;
&lt;li&gt;CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings&lt;/li&gt;
&lt;li&gt;IntraQ: Learning Synthetic Images With Intra-Class Heterogeneity for Zero-Shot Network Quantization&lt;/li&gt;
&lt;li&gt;I M Avatar: Implicit Morphable Head Avatars From Videos&lt;/li&gt;
&lt;li&gt;Weakly-Supervised Metric Learning With Cross-Module Communications for The Classification of Anterior Chamber Angle Images&lt;/li&gt;
&lt;li&gt;A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-Resolution&lt;/li&gt;
&lt;li&gt;Multi-Modal Dynamic Graph Transformer for Visual Grounding&lt;/li&gt;
&lt;li&gt;Geometric Transformer for Fast and Robust Point Cloud Registration&lt;/li&gt;
&lt;li&gt;UMT: Unified Multi-Modal Transformers for Joint Video Moment Retrieval and Highlight Detection&lt;/li&gt;
&lt;li&gt;Demystifying The Neural Tangent Kernel From A Practical Perspective: Can It Be Trusted for Neural Architecture Search Without Training?&lt;/li&gt;
&lt;li&gt;The Devil Is in The Details: Window-Based Attention for Image Compression&lt;/li&gt;
&lt;li&gt;DiLiGenT102: A Photometric Stereo Benchmark Dataset With Controlled Shape and Material Variation&lt;/li&gt;
&lt;li&gt;PolyWorld: Polygonal Building Extraction With Graph Neural Networks in Satellite Images&lt;/li&gt;
&lt;li&gt;Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation&lt;/li&gt;
&lt;li&gt;Spatio-Temporal Relation Modeling for Few-Shot Action Recognition&lt;/li&gt;
&lt;li&gt;Multi-Person Extreme Motion Prediction&lt;/li&gt;
&lt;li&gt;B-DARTS: Beta-Decay Regularization for Differentiable Architecture Search&lt;/li&gt;
&lt;li&gt;CMT: Convolutional Neural Networks Meet Vision Transformers&lt;/li&gt;
&lt;li&gt;KNN Local Attention for Image Restoration&lt;/li&gt;
&lt;li&gt;Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered By Pre-Trained Vision-Language Model&lt;/li&gt;
&lt;li&gt;TransMix: Attend To Mix for Vision Transformers&lt;/li&gt;
&lt;li&gt;Inertia-Guided Flow Completion and Style Fusion for Video Inpainting&lt;/li&gt;
&lt;li&gt;Long-Tailed Visual Recognition Via Gaussian Clouded Logit Adjustment&lt;/li&gt;
&lt;li&gt;Image Animation With Perturbed Masks&lt;/li&gt;
&lt;li&gt;Domain Generalization Via Shuffled Style Assembly for Face Anti-Spoofing&lt;/li&gt;
&lt;li&gt;OcclusionFusion: Occlusion-Aware Motion Estimation for Real-Time Dynamic 3D Reconstruction&lt;/li&gt;
&lt;li&gt;MonoScene: Monocular 3D Semantic Scene Completion&lt;/li&gt;
&lt;li&gt;AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition&lt;/li&gt;
&lt;li&gt;Continuous Scene Representations for Embodied AI&lt;/li&gt;
&lt;li&gt;Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds&lt;/li&gt;
&lt;li&gt;Non-Probability Sampling Network for Stochastic Human Trajectory Prediction&lt;/li&gt;
&lt;li&gt;ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning&lt;/li&gt;
&lt;li&gt;Human-Aware Object Placement for Visual Environment Reconstruction&lt;/li&gt;
&lt;li&gt;X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval&lt;/li&gt;
&lt;li&gt;RAMA: A Rapid Multicut Algorithm on GPU&lt;/li&gt;
&lt;li&gt;Adversarial Parametric Pose Prior&lt;/li&gt;
&lt;li&gt;Mask Transfiner for High-Quality Instance Segmentation&lt;/li&gt;
&lt;li&gt;It Is Okay To Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning By Contrastive Data Collection&lt;/li&gt;
&lt;li&gt;DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis&lt;/li&gt;
&lt;li&gt;Event-Based Video Reconstruction Via Potential-Assisted Spiking Neural Network&lt;/li&gt;
&lt;li&gt;YouMVOS: An Actor-Centric Multi-Shot Video Object Segmentation Dataset&lt;/li&gt;
&lt;li&gt;DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification&lt;/li&gt;
&lt;li&gt;Self-Supervised Video Transformer&lt;/li&gt;
&lt;li&gt;AutoRF: Learning 3D Object Radiance Fields From Single View Observations&lt;/li&gt;
&lt;li&gt;Coopernaut: End-to-End Driving With Cooperative Perception for Networked Vehicles&lt;/li&gt;
&lt;li&gt;TubeR: Tubelet Transformer for Video Action Detection&lt;/li&gt;
&lt;li&gt;MUM: Mix Image Tiles and UnMix Feature Tiles for Semi-Supervised Object Detection&lt;/li&gt;
&lt;li&gt;Learning Non-Target Knowledge for Few-Shot Semantic Segmentation&lt;/li&gt;
&lt;li&gt;UKPGAN: A General Self-Supervised Keypoint Detector&lt;/li&gt;
&lt;li&gt;Raw High-Definition Radar for Multi-Task Learning&lt;/li&gt;
&lt;li&gt;Coarse-To-Fine Feature Mining for Video Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Compressing Models With Few Samples: Mimicking Then Replacing&lt;/li&gt;
&lt;li&gt;PokeBNN: A Binary Pursuit of Lightweight Accuracy&lt;/li&gt;
&lt;li&gt;Zoom in and Out: A Mixed-Scale Triplet Network for Camouflaged Object Detection&lt;/li&gt;
&lt;li&gt;SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images&lt;/li&gt;
&lt;li&gt;EMScore: Evaluating Video Captioning Via Coarse-Grained and Fine-Grained Embedding Matching&lt;/li&gt;
&lt;li&gt;PoseTriplet: Co-Evolving 3D Human Pose Estimation, Imitation, and Hallucination Under Self-Supervision&lt;/li&gt;
&lt;li&gt;Group Contextualization for Video Recognition&lt;/li&gt;
&lt;li&gt;Single-Domain Generalized Object Detection in Urban Scene Via Cyclic-Disentangled Self-Distillation&lt;/li&gt;
&lt;li&gt;L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation&lt;/li&gt;
&lt;li&gt;Self-Augmented Unpaired Image Dehazing Via Density and Depth Decomposition&lt;/li&gt;
&lt;li&gt;Neural 3D Video Synthesis From Multi-View Video&lt;/li&gt;
&lt;li&gt;SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search&lt;/li&gt;
&lt;li&gt;HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening&lt;/li&gt;
&lt;li&gt;Structure-Aware Flow Generation for Human Body Reshaping&lt;/li&gt;
&lt;li&gt;Learning To Answer Questions in Dynamic Audio-Visual Scenarios&lt;/li&gt;
&lt;li&gt;Synthetic Aperture Imaging With Events and Frames&lt;/li&gt;
&lt;li&gt;MonoGround: Detecting Monocular 3D Objects From The Ground&lt;/li&gt;
&lt;li&gt;Deep Visual Geo-Localization Benchmark&lt;/li&gt;
&lt;li&gt;StyleGAN-V: A Continuous Video Generator With The Price, Image Quality and Perks of StyleGAN2&lt;/li&gt;
&lt;li&gt;LISA: Learning Implicit Shape and Appearance of Hands&lt;/li&gt;
&lt;li&gt;Iterative Deep Homography Estimation&lt;/li&gt;
&lt;li&gt;Learned Queries for Efficient Local Attention&lt;/li&gt;
&lt;li&gt;Colar: Effective and Efficient Online Action Detection By Consulting Exemplars&lt;/li&gt;
&lt;li&gt;SoftGroup for 3D Instance Segmentation on Point Clouds&lt;/li&gt;
&lt;li&gt;MVS2D: Efficient Multi-View Stereo Via Attention-Driven 2D Convolutions&lt;/li&gt;
&lt;li&gt;Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation Via Semantic Knowledge Transfer and Self-Refinement&lt;/li&gt;
&lt;li&gt;Deep Constrained Least Squares for Blind Image Super-Resolution&lt;/li&gt;
&lt;li&gt;EDTER: Edge Detection With Transformer&lt;/li&gt;
&lt;li&gt;AirObject: A Temporally Evolving Graph Embedding for Object Identification&lt;/li&gt;
&lt;li&gt;From Representation to Reasoning: Towards Both Evidence and Commonsense Reasoning for Video Question-Answering&lt;/li&gt;
&lt;li&gt;Semantic-Aware Domain Generalized Segmentation&lt;/li&gt;
&lt;li&gt;DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion&lt;/li&gt;
&lt;li&gt;UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection&lt;/li&gt;
&lt;li&gt;AKB-48: A Real-World Articulated Object Knowledge Base&lt;/li&gt;
&lt;li&gt;Stratified Transformer for 3D Point Cloud Segmentation&lt;/li&gt;
&lt;li&gt;Aug-NeRF: Training Stronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmentations&lt;/li&gt;
&lt;li&gt;Semantic-Shape Adaptive Feature Modulation for Semantic Image Synthesis&lt;/li&gt;
&lt;li&gt;Day-to-Night Image Synthesis for Training Nighttime Neural ISPs Literature ~Highlight: To address this problem, we propose a method that synthesizes nighttime images from daytime images.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2 class=&#34;relative group&#34;&gt;References 
    &lt;div id=&#34;references&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#references&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&#34; target=&#34;_blank&#34;&gt;https://www.paperdigest.org/2022/06/cvpr-2022-papers-with-code-data/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;&lt;br&gt;
Dr Hari Thapliyaal&lt;br&gt;
dasarpai.com&lt;br&gt;
linkedin.com/in/harithapliyal&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</title>
      <link>/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg&#34; alt=&#34;Pretrained Language Models for Text Generation&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper Name :- Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt;&lt;br&gt;
Typer of Paper:- Survey Paper&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34; target=&#34;_blank&#34;&gt;Paper URL&lt;/a&gt;&lt;br&gt;
Paper title of the citations mentioned can be found at &lt;a href=&#34;/dsblog/aip&#34;&gt;AI Papers with Heading&lt;/a&gt;. Use citation code to locate.&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary :- Pretrained Language Models for Text Generation 
    &lt;div id=&#34;paper-summary---pretrained-language-models-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary---pretrained-language-models-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Paper Outcome 
    &lt;div id=&#34;paper-outcome&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-outcome&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;General task deﬁnition&lt;/li&gt;
&lt;li&gt;Describe the mainstream architectures of PLMs for text generation.&lt;/li&gt;
&lt;li&gt;How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.&lt;/li&gt;
&lt;li&gt;Summarize several important ﬁne-tuning strategies for text generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Ideas from the Paper 
    &lt;div id=&#34;ideas-from-the-paper&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ideas-from-the-paper&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Main Ideas 
    &lt;div id=&#34;main-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#main-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This paper discusses &amp;ldquo;major advances achieved in the topic of PLMs for text generation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;This survey aims to provide &amp;ldquo;text generation researchers a synthesis&amp;rdquo; and pointer to related research.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;General Ideas 
    &lt;div id=&#34;general-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#general-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).&lt;/li&gt;
&lt;li&gt;Neural generation model are deep learning models&lt;/li&gt;
&lt;li&gt;Pretrained language models (PLMs) are neural generation model&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Task Types and Typical Applications 
    &lt;div id=&#34;task-types-and-typical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#task-types-and-typical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )&lt;/li&gt;
&lt;li&gt;If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional
generation task(generate text without any constraint) &lt;a href=&#34;/dsblog/aip#radford2019&#34;&gt;Radford2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or
attribute-based generation. X plays the role of guiding the text generation. &lt;a href=&#34;/dsblog/aip#Keskar2019&#34;&gt;Keskar2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation &lt;a href=&#34;/dsblog/aip#Li2021c&#34;&gt;Li2021c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as image, the task becomes image caption &lt;a href=&#34;/dsblog/aip#Xia2020&#34;&gt;Xia2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as speech, the task become speech recognition &lt;a href=&#34;/dsblog/aip#Fan2019&#34;&gt;Fan2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.&lt;/li&gt;
&lt;li&gt;Machine translation aims to translate text from one language into another language automatically &lt;a href=&#34;/dsblog/aip#Conneau2019&#34;&gt;Conneau2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generating condensed summary of a long document &lt;a href=&#34;/dsblog/aip#Zhang2019b&#34;&gt;Zhang2019b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dialogue system to converse with humans using natural language. &lt;a href=&#34;/dsblog/aip#Wolf2019&#34;&gt;Wolf2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Architectures for Text Generation 
    &lt;div id=&#34;architectures-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#architectures-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.
&lt;ul&gt;
&lt;li&gt;MASS &lt;a href=&#34;/dsblog/aip#song2019&#34;&gt;Song2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T5 &lt;a href=&#34;/dsblog/aip#raffel2020&#34;&gt;Raffel2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BART &lt;a href=&#34;/dsblog/aip#lewis2020&#34;&gt;Lewis2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.
&lt;ul&gt;
&lt;li&gt;GPT &lt;a href=&#34;/dsblog/aip#radfordet2019&#34;&gt;Radfordet2019&lt;/a&gt;; &lt;a href=&#34;/dsblog/aip#brown2020&#34;&gt;Brown2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CTRL [Keskar2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Modeling Different Data Types from Input 
    &lt;div id=&#34;modeling-different-data-types-from-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#modeling-different-data-types-from-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Unstructured Input 
    &lt;div id=&#34;unstructured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unstructured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]&lt;/li&gt;
&lt;li&gt;Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]&lt;/li&gt;
&lt;li&gt;Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]&lt;/li&gt;
&lt;li&gt;Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Structured Input 
    &lt;div id=&#34;structured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#structured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PLMs are not designed for structured or tabular data but for sequential text/data.&lt;/li&gt;
&lt;li&gt;Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]&lt;/li&gt;
&lt;li&gt;To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]&lt;/li&gt;
&lt;li&gt;Introduced an additional graph encoder to encode the input KG. [Li2021b]&lt;/li&gt;
&lt;li&gt;Template based method to serialize input table into text sequence. [Gong2020]
&lt;ul&gt;
&lt;li&gt;For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]&lt;/li&gt;
&lt;li&gt;The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].&lt;/li&gt;
&lt;li&gt;Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Multimedia Input 
    &lt;div id=&#34;multimedia-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multimedia-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]&lt;/li&gt;
&lt;li&gt;Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]&lt;/li&gt;
&lt;li&gt;Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]&lt;/li&gt;
&lt;li&gt;Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020&lt;/li&gt;
&lt;li&gt;Image, video, speech recognition is hungry for human-transcripted supervised data.&lt;/li&gt;
&lt;li&gt;Integrate PLMs for weakly-supervised learning. For example,
&lt;ul&gt;
&lt;li&gt;Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Satisfying Special Properties for Output Text 
    &lt;div id=&#34;satisfying-special-properties-for-output-text&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#satisfying-special-properties-for-output-text&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.&lt;/li&gt;
&lt;li&gt;Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should
also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. - When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] - Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input
representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] - Approach for non-conditioned dialogue [Bao2020].&lt;/li&gt;
&lt;li&gt;Faithfulness. Means the content in generated text should not contradict the facts in input text.
&lt;ul&gt;
&lt;li&gt;PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.&lt;/li&gt;
&lt;li&gt;Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]&lt;/li&gt;
&lt;li&gt;With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.&lt;/li&gt;
&lt;li&gt;Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]&lt;/li&gt;
&lt;li&gt;Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.
&lt;ul&gt;
&lt;li&gt;When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.&lt;/li&gt;
&lt;li&gt;Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]
&lt;ul&gt;
&lt;li&gt;Extracted the word-pair alignment information from the source and target language,&lt;/li&gt;
&lt;li&gt;Aplied the extracted alignment information to enhance order-preserving.&lt;/li&gt;
&lt;li&gt;Translation across multiple languages, called multilingual machine translation [Conneau2019].&lt;/li&gt;
&lt;li&gt;mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]&lt;/li&gt;
&lt;li&gt;Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Summary from Introduction 
    &lt;div id=&#34;summary-from-introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#summary-from-introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].&lt;/li&gt;
&lt;li&gt;Machine translation generates text in a different language based on the source text [Yang2020a];&lt;/li&gt;
&lt;li&gt;Summarization generates an abridged version of the source text to include salient information [Guan2020].&lt;/li&gt;
&lt;li&gt;Text generation tasks based on
&lt;ul&gt;
&lt;li&gt;Recurrent neural networks (RNN) [Li2019],&lt;/li&gt;
&lt;li&gt;Convolutional neural networks (CNN) [Gehring2017],&lt;/li&gt;
&lt;li&gt;Graph neural networks (GNN) [Li2020],&lt;/li&gt;
&lt;li&gt;Attention mechanism [Bahdanau2015].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.&lt;/li&gt;
&lt;li&gt;Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.&lt;/li&gt;
&lt;li&gt;Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice.&lt;/li&gt;
&lt;li&gt;The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve
state-of-the-art results.&lt;/li&gt;
&lt;li&gt;PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.&lt;/li&gt;
&lt;li&gt;PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020].&lt;/li&gt;
&lt;li&gt;A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Conclusion &amp;amp; Future Recommendations 
    &lt;div id=&#34;conclusion--future-recommendations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#conclusion--future-recommendations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Model Extension.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>How to do Literature Review</title>
      <link>/dsblog/How-To-Do-Literature-Review/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>/dsblog/How-To-Do-Literature-Review/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;/assets/images/dspost/dsp6086-How-To-Do-Literature-Review.jpg&#34; alt=&#34;How to do Literature Review&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;How to Conduct Literature Review? 
    &lt;div id=&#34;how-to-conduct-literature-review&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#how-to-conduct-literature-review&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Literature Review (LR) or Literature Survey (LS) is a process that helps you to browse the libraries, literature, articles, books, conference proceedings, etc. The objective of this process is to study the work of other researchers in the field and around the topic you are interested in. This is one of the heaviest work in any research work. If you are not on track or this process is taking unusually longer time, then it means you don&amp;rsquo;t have any guiding process or best practices in place. As per my experience during my MS in Data Science and Ph.D./DBA in AI-Natural Language Processing, I am writing my thoughts in this article. I am sure it will help you, especially if you are entering in the field of research.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
