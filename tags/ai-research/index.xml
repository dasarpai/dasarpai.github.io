<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Research on </title>
    <link>http://localhost:1313/tags/ai-research/</link>
    <description>Recent content in AI Research on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Dr. Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Dr. Hari Thapliyaal)</webMaster>
    <copyright>© 2025 Dr. Hari Thapliyaal</copyright>
    <lastBuildDate>Wed, 09 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ai-research/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AI Benchmarks for 2025</title>
      <link>http://localhost:1313/dsblog/ai-benchmarks-2025/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/ai-benchmarks-2025/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6256-aibenchmarks-for-2025.jpg&#34; alt=&#34;AI Benchmark for 2025&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI Benchmarks for 2025 
    &lt;div id=&#34;ai-benchmarks-for-2025&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-benchmarks-for-2025&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;A term &lt;strong&gt;&amp;ldquo;AI benchmark&amp;rdquo;&lt;/strong&gt; is thrown around a lot and can be confusing because it’s used in slightly different ways depending on the context. In this artcile we will try to understand what are the different meaning of this term and what are the latest AI benchmarks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>My Journey from Master to PhD in Data Science and AI</title>
      <link>http://localhost:1313/dsblog/journey-from-master-to-phd/</link>
      <pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/journey-from-master-to-phd/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6100-Journey-from-MS-to-Phd.jpg&#34; alt=&#34;My Journey from Master to PhD in Data Science and AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;My Journey from Master to PhD in Data Science and AI 
    &lt;div id=&#34;my-journey-from-master-to-phd-in-data-science-and-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#my-journey-from-master-to-phd-in-data-science-and-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;I have been in software development between 1993 to 2009. Some of these years were in senior leadership roles in delivery management, project management, CMMI, ISO, ISMS, PMO, etc. In 2010 I moved into project management training and consulting. In 2018, I was considering going back to technology but this time I wanted to pick up a completely new stack of technology. I decided I would move into Data Science and AI. I knew AI as much as any typical software development person in senior management knew about this. It means I was highly confident that I could pick this up quickly. On top of that lots of content is available on the internet, many YouTube channels, and many courses are available. I thought it should be a cakewalk for me. I started learning this technology in my own way in my committed free time. Within 5-6 months after lots of study I started realizing this was the way to get the knowledge but it could not help me get confident to solve the problem. The more I learned, the more I felt that I did not know how much I needed to learn or how far I needed to go.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Important AI Paper List</title>
      <link>http://localhost:1313/dsblog/select-ai-papers/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/select-ai-papers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg&#34; alt=&#34;Important AI Paper List&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Important AI Paper List 
    &lt;div id=&#34;important-ai-paper-list&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#important-ai-paper-list&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduciton 
    &lt;div id=&#34;introduciton&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduciton&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors&amp;rsquo; information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like &amp;ldquo;Vivek Ramaswami, Kartikeyan Karunanidhi&amp;rdquo; it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on &amp;ldquo;google scholar&amp;rdquo;, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</title>
      <link>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg&#34; alt=&#34;Pretrained Language Models for Text Generation&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper Name :- Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt;&lt;br&gt;
Typer of Paper:- Survey Paper&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34; target=&#34;_blank&#34;&gt;Paper URL&lt;/a&gt;&lt;br&gt;
Paper title of the citations mentioned can be found at &lt;a href=&#34;../../dsblog/aip&#34;&gt;AI Papers with Heading&lt;/a&gt;. Use citation code to locate.&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary :- Pretrained Language Models for Text Generation 
    &lt;div id=&#34;paper-summary---pretrained-language-models-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary---pretrained-language-models-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Paper Outcome 
    &lt;div id=&#34;paper-outcome&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-outcome&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;General task deﬁnition&lt;/li&gt;
&lt;li&gt;Describe the mainstream architectures of PLMs for text generation.&lt;/li&gt;
&lt;li&gt;How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.&lt;/li&gt;
&lt;li&gt;Summarize several important ﬁne-tuning strategies for text generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Ideas from the Paper 
    &lt;div id=&#34;ideas-from-the-paper&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ideas-from-the-paper&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Main Ideas 
    &lt;div id=&#34;main-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#main-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This paper discusses &amp;ldquo;major advances achieved in the topic of PLMs for text generation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;This survey aims to provide &amp;ldquo;text generation researchers a synthesis&amp;rdquo; and pointer to related research.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;General Ideas 
    &lt;div id=&#34;general-ideas&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#general-ideas&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).&lt;/li&gt;
&lt;li&gt;Neural generation model are deep learning models&lt;/li&gt;
&lt;li&gt;Pretrained language models (PLMs) are neural generation model&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Task Types and Typical Applications 
    &lt;div id=&#34;task-types-and-typical-applications&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#task-types-and-typical-applications&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )&lt;/li&gt;
&lt;li&gt;If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional
generation task(generate text without any constraint) &lt;a href=&#34;../../dsblog/aip#radford2019&#34;&gt;Radford2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or
attribute-based generation. X plays the role of guiding the text generation. &lt;a href=&#34;../../dsblog/aip#Keskar2019&#34;&gt;Keskar2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation &lt;a href=&#34;../../dsblog/aip#Li2021c&#34;&gt;Li2021c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as image, the task becomes image caption &lt;a href=&#34;../../dsblog/aip#Xia2020&#34;&gt;Xia2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If X is multimedia input such as speech, the task become speech recognition &lt;a href=&#34;../../dsblog/aip#Fan2019&#34;&gt;Fan2019&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.&lt;/li&gt;
&lt;li&gt;Machine translation aims to translate text from one language into another language automatically &lt;a href=&#34;../../dsblog/aip#Conneau2019&#34;&gt;Conneau2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generating condensed summary of a long document &lt;a href=&#34;../../dsblog/aip#Zhang2019b&#34;&gt;Zhang2019b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dialogue system to converse with humans using natural language. &lt;a href=&#34;../../dsblog/aip#Wolf2019&#34;&gt;Wolf2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Architectures for Text Generation 
    &lt;div id=&#34;architectures-for-text-generation&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#architectures-for-text-generation&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.
&lt;ul&gt;
&lt;li&gt;MASS &lt;a href=&#34;../../dsblog/aip#song2019&#34;&gt;Song2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T5 &lt;a href=&#34;../../dsblog/aip#raffel2020&#34;&gt;Raffel2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BART &lt;a href=&#34;../../dsblog/aip#lewis2020&#34;&gt;Lewis2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.
&lt;ul&gt;
&lt;li&gt;GPT &lt;a href=&#34;../../dsblog/aip#radfordet2019&#34;&gt;Radfordet2019&lt;/a&gt;; &lt;a href=&#34;../../dsblog/aip#brown2020&#34;&gt;Brown2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CTRL [Keskar2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Modeling Different Data Types from Input 
    &lt;div id=&#34;modeling-different-data-types-from-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#modeling-different-data-types-from-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Unstructured Input 
    &lt;div id=&#34;unstructured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#unstructured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]&lt;/li&gt;
&lt;li&gt;Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]&lt;/li&gt;
&lt;li&gt;Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]&lt;/li&gt;
&lt;li&gt;Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Structured Input 
    &lt;div id=&#34;structured-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#structured-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PLMs are not designed for structured or tabular data but for sequential text/data.&lt;/li&gt;
&lt;li&gt;Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]&lt;/li&gt;
&lt;li&gt;To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]&lt;/li&gt;
&lt;li&gt;Introduced an additional graph encoder to encode the input KG. [Li2021b]&lt;/li&gt;
&lt;li&gt;Template based method to serialize input table into text sequence. [Gong2020]
&lt;ul&gt;
&lt;li&gt;For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]&lt;/li&gt;
&lt;li&gt;The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].&lt;/li&gt;
&lt;li&gt;Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;Multimedia Input 
    &lt;div id=&#34;multimedia-input&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#multimedia-input&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]&lt;/li&gt;
&lt;li&gt;Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]&lt;/li&gt;
&lt;li&gt;Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]&lt;/li&gt;
&lt;li&gt;Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020&lt;/li&gt;
&lt;li&gt;Image, video, speech recognition is hungry for human-transcripted supervised data.&lt;/li&gt;
&lt;li&gt;Integrate PLMs for weakly-supervised learning. For example,
&lt;ul&gt;
&lt;li&gt;Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Satisfying Special Properties for Output Text 
    &lt;div id=&#34;satisfying-special-properties-for-output-text&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#satisfying-special-properties-for-output-text&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.&lt;/li&gt;
&lt;li&gt;Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should
also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. - When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] - Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input
representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] - Approach for non-conditioned dialogue [Bao2020].&lt;/li&gt;
&lt;li&gt;Faithfulness. Means the content in generated text should not contradict the facts in input text.
&lt;ul&gt;
&lt;li&gt;PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.&lt;/li&gt;
&lt;li&gt;Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]&lt;/li&gt;
&lt;li&gt;With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.&lt;/li&gt;
&lt;li&gt;Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]&lt;/li&gt;
&lt;li&gt;Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.
&lt;ul&gt;
&lt;li&gt;When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.&lt;/li&gt;
&lt;li&gt;Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]
&lt;ul&gt;
&lt;li&gt;Extracted the word-pair alignment information from the source and target language,&lt;/li&gt;
&lt;li&gt;Aplied the extracted alignment information to enhance order-preserving.&lt;/li&gt;
&lt;li&gt;Translation across multiple languages, called multilingual machine translation [Conneau2019].&lt;/li&gt;
&lt;li&gt;mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]&lt;/li&gt;
&lt;li&gt;Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Summary from Introduction 
    &lt;div id=&#34;summary-from-introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#summary-from-introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].&lt;/li&gt;
&lt;li&gt;Machine translation generates text in a different language based on the source text [Yang2020a];&lt;/li&gt;
&lt;li&gt;Summarization generates an abridged version of the source text to include salient information [Guan2020].&lt;/li&gt;
&lt;li&gt;Text generation tasks based on
&lt;ul&gt;
&lt;li&gt;Recurrent neural networks (RNN) [Li2019],&lt;/li&gt;
&lt;li&gt;Convolutional neural networks (CNN) [Gehring2017],&lt;/li&gt;
&lt;li&gt;Graph neural networks (GNN) [Li2020],&lt;/li&gt;
&lt;li&gt;Attention mechanism [Bahdanau2015].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.&lt;/li&gt;
&lt;li&gt;Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.&lt;/li&gt;
&lt;li&gt;Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice.&lt;/li&gt;
&lt;li&gt;The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve
state-of-the-art results.&lt;/li&gt;
&lt;li&gt;PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.&lt;/li&gt;
&lt;li&gt;PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020].&lt;/li&gt;
&lt;li&gt;A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]&lt;/li&gt;
&lt;/ul&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Conclusion &amp;amp; Future Recommendations 
    &lt;div id=&#34;conclusion--future-recommendations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#conclusion--future-recommendations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Model Extension.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Major LLM Developers Shaping the AI Landscape</title>
      <link>http://localhost:1313/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6075-Major-LLM-Developers-Reshaping-NLP-Advancements.jpg&#34; alt=&#34;Major LLM Developers Shaping the AI Landscape&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Major LLM Developers Shaping the AI Landscape 
    &lt;div id=&#34;major-llm-developers-shaping-the-ai-landscape&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#major-llm-developers-shaping-the-ai-landscape&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;From Text to Intelligence: Major LLM Developers Shaping the AI Landscape&lt;/strong&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Introduction: 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Artificial Intelligence (AI) has experienced an exponential growth, fueled by groundbreaking research and the efforts of innovative developers. Among the key players, Large Language Model (LLM) developers have taken center stage, creating powerful language models that have revolutionized natural language processing and understanding. In this article, we delve into the major LLM developers, their key contributions.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>God Fathers of AI</title>
      <link>http://localhost:1313/dsblog/God-Fathers-of-AI/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/God-Fathers-of-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6058-God-Fathers-of-AI.jpg&#34; alt=&#34;God Fathers of AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;God Fathers of AI 
    &lt;div id=&#34;god-fathers-of-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#god-fathers-of-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;In other fields of studies or in religion, there is only one god or only one godfather. But in the field of AI, that is not the case. There are many pioneers or Godfathers who have done significant work in this field. Recently, the resignation of Dr. Geoffrey Hinton from Google raised eyebrows in the business world and in Governments the world over. Technology is good or bad, it depends upon whose hand it is. Geoffrey raised that concern and for that, he wants better controls in place. What will happen, we need to follow the progress and raise our voices around. In this article, I am mentioning some godfathers of AI, their workplaces, and their contributions. I am sure this will inspire many young minds.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>What Are Transformers in AI</title>
      <link>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg&#34; alt=&#34;What-are-Transformers-in-AI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;What Are Transformers in AI 
    &lt;div id=&#34;what-are-transformers-in-ai&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#what-are-transformers-in-ai&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Transformer Architecture 
    &lt;div id=&#34;transformer-architecture&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#transformer-architecture&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dspost/transformer/transformer-arch.jpg&#34; alt=&#34;Transformer&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;Background 
    &lt;div id=&#34;background&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#background&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Data Scientists and AI, ML Researchers</title>
      <link>http://localhost:1313/dsblog/ds-ai-ml-researchers/</link>
      <pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/ds-ai-ml-researchers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dsresources/dsr117-Data-Scientists-and-AI-ML-Researchers.jpg&#34; alt=&#34;Data Scientists and AI, ML Researchers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Data Scientists and AI, ML Researchers 
    &lt;div id=&#34;data-scientists-and-ai-ml-researchers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#data-scientists-and-ai-ml-researchers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;For the initial list, I have taken content from &lt;a href=&#34;https://github.com/datasciencescoop/awesome-deep-learning&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;. I have expanded this and will keep updating this in the future. If you know any, please feel free to put the name in the comment box.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://aaroncourville.wordpress.com/&#34; target=&#34;_blank&#34;&gt;Aaron Courville&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~asamir/&#34; target=&#34;_blank&#34;&gt;Abdel-rahman Mohamed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/~acoates/&#34; target=&#34;_blank&#34;&gt;Adam Coates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.microsoft.com/en-us/people/alexac/&#34; target=&#34;_blank&#34;&gt;Alex Acero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.utoronto.ca/~kriz/index.html&#34; target=&#34;_blank&#34;&gt;Alex Krizhevsky&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://users.ics.aalto.fi/alexilin/&#34; target=&#34;_blank&#34;&gt;Alexander Ilin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/amos/&#34; target=&#34;_blank&#34;&gt;Amos Storkey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/~karpathy/&#34; target=&#34;_blank&#34;&gt;Andrej Karpathy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://andrewnc.github.io&#34; target=&#34;_blank&#34;&gt;Andrew Carr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stanford.edu/~asaxe/&#34; target=&#34;_blank&#34;&gt;Andrew M. Saxe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.stanford.edu/people/ang/&#34; target=&#34;_blank&#34;&gt;Andrew Ng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google.com/pubs/author37792.html&#34; target=&#34;_blank&#34;&gt;Andrew W. Senior&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.gatsby.ucl.ac.uk/~amnih/&#34; target=&#34;_blank&#34;&gt;Andriy Mnih&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.nyu.edu/~naz/&#34; target=&#34;_blank&#34;&gt;Ayse Naz Erkan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://reslab.elis.ugent.be/benjamin&#34; target=&#34;_blank&#34;&gt;Benjamin Schrauwen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cisuc.uc.pt/people/show/2020&#34; target=&#34;_blank&#34;&gt;Bernardete Ribeiro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vision.caltech.edu/~bchen3/Site/Bo_David_Chen.html&#34; target=&#34;_blank&#34;&gt;Bo David Chen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.nyu.edu/~ylan/&#34; target=&#34;_blank&#34;&gt;Boureau Y-Lan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://researcher.watson.ibm.com/researcher/view.php?person=us-bedk&#34; target=&#34;_blank&#34;&gt;Brian Kingsbury&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chrisalbon.com&#34; target=&#34;_blank&#34;&gt;Chris Albon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/~manning/&#34; target=&#34;_blank&#34;&gt;Christopher Manning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datasciencemasters.org&#34; target=&#34;_blank&#34;&gt;Clare Corthell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.clement.farabet.net/&#34; target=&#34;_blank&#34;&gt;Clement Farabet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colah.github.io&#34; target=&#34;_blank&#34;&gt;Colah’s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.idsia.ch/~ciresan/&#34; target=&#34;_blank&#34;&gt;Dan Claudiu Cireșan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://serre-lab.clps.brown.edu/person/david-reichert/&#34; target=&#34;_blank&#34;&gt;David Reichert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mil.engr.utk.edu/nmil/member/5.html&#34; target=&#34;_blank&#34;&gt;Derek Rose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.microsoft.com/en-us/people/dongyu/default.aspx&#34; target=&#34;_blank&#34;&gt;Dong Yu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.seas.upenn.edu/~wulsin/&#34; target=&#34;_blank&#34;&gt;Drausin Wulsin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drewconway.com&#34; target=&#34;_blank&#34;&gt;Drew Conway&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://music.ece.drexel.edu/people/eschmidt&#34; target=&#34;_blank&#34;&gt;Erik M. Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333&#34; target=&#34;_blank&#34;&gt;Eugenio Culurciello&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vision.stanford.edu/feifeili&#34; target=&#34;_blank&#34;&gt;Fei-Fei Li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.microsoft.com/en-us/people/fseide/&#34; target=&#34;_blank&#34;&gt;Frank Seide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~galen/&#34; target=&#34;_blank&#34;&gt;Galen Andrew&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~hinton/&#34; target=&#34;_blank&#34;&gt;Geoffrey Hinton&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~gdahl/&#34; target=&#34;_blank&#34;&gt;George Dahl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uoguelph.ca/~gwtaylor/&#34; target=&#34;_blank&#34;&gt;Graham Taylor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gregoire.montavon.name/&#34; target=&#34;_blank&#34;&gt;Grégoire Montavon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://personal-homepages.mis.mpg.de/montufar/&#34; target=&#34;_blank&#34;&gt;Guido Francisco Montúfar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://brainlogging.wordpress.com/&#34; target=&#34;_blank&#34;&gt;Guillaume Desjardins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ais.uni-bonn.de/~schulz/&#34; target=&#34;_blank&#34;&gt;Hannes Schulz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lri.fr/~hpaugam/&#34; target=&#34;_blank&#34;&gt;Hélène Paugam-Moisy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hilarymason.com&#34; target=&#34;_blank&#34;&gt;Hilary Mason&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hilaryparker.com&#34; target=&#34;_blank&#34;&gt;Hilary Parker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.eecs.umich.edu/~honglak/&#34; target=&#34;_blank&#34;&gt;Honglak Lee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dmi.usherb.ca/~larocheh/index_en.html&#34; target=&#34;_blank&#34;&gt;Hugo Larochelle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google.com/pubs/105214.html&#34; target=&#34;_blank&#34;&gt;Ian Goodfellow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~ilya/&#34; target=&#34;_blank&#34;&gt;Ilya Sutskever&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mil.engr.utk.edu/nmil/member/2.html&#34; target=&#34;_blank&#34;&gt;Itamar Arel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~jmartens/&#34; target=&#34;_blank&#34;&gt;James Martens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jasonmorton.com/&#34; target=&#34;_blank&#34;&gt;Jason Morton&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thespermwhale.com/jaseweston/&#34; target=&#34;_blank&#34;&gt;Jason Weston&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google.com/pubs/jeff.html&#34; target=&#34;_blank&#34;&gt;Jeff Dean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/~jngiam/&#34; target=&#34;_blank&#34;&gt;Jiquan Mgiam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-etud.iro.umontreal.ca/~turian/&#34; target=&#34;_blank&#34;&gt;Joseph Turian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclab.ca/users/josh/index.html&#34; target=&#34;_blank&#34;&gt;Joshua Matthew Susskind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.idsia.ch/~juergen/&#34; target=&#34;_blank&#34;&gt;Jürgen Schmidhuber&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/blancousna/&#34; target=&#34;_blank&#34;&gt;Justin A. Blanco&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kldavenport.com&#34; target=&#34;_blank&#34;&gt;Kevin Davenport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://koray.kavukcuoglu.org/&#34; target=&#34;_blank&#34;&gt;Koray Kavukcuoglu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://users.ics.aalto.fi/kcho/&#34; target=&#34;_blank&#34;&gt;KyungHyun Cho&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.microsoft.com/en-us/people/deng/&#34; target=&#34;_blank&#34;&gt;Li Deng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html&#34; target=&#34;_blank&#34;&gt;Lucas Theis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ludovicarnold.altervista.org/home/&#34; target=&#34;_blank&#34;&gt;Ludovic Arnold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.nyu.edu/~ranzato/&#34; target=&#34;_blank&#34;&gt;Marc’Aurelio Ranzato&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aass.oru.se/~mlt/&#34; target=&#34;_blank&#34;&gt;Martin Längkvist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hairysun.com&#34; target=&#34;_blank&#34;&gt;Matt Harrison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://miningthesocialweb.com&#34; target=&#34;_blank&#34;&gt;Matthew Russell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mdenil.com/&#34; target=&#34;_blank&#34;&gt;Misha Denil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~norouzi/&#34; target=&#34;_blank&#34;&gt;Mohammad Norouzi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~nando/&#34; target=&#34;_blank&#34;&gt;Nando de Freitas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.utoronto.ca/~ndjaitly/&#34; target=&#34;_blank&#34;&gt;Navdeep Jaitly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nicolas.le-roux.name/&#34; target=&#34;_blank&#34;&gt;Nicolas Le Roux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~nitish/&#34; target=&#34;_blank&#34;&gt;Nitish Srivastava&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://complexdiagrams.com&#34; target=&#34;_blank&#34;&gt;Noah Iliinsky&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cisuc.uc.pt/people/show/2028&#34; target=&#34;_blank&#34;&gt;Noel Lopes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.berkeley.edu/~vinyals/&#34; target=&#34;_blank&#34;&gt;Oriol Vinyals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.iro.umontreal.ca/~vincentp&#34; target=&#34;_blank&#34;&gt;Pascal Vincent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/drpngx/&#34; target=&#34;_blank&#34;&gt;Patrick Nguyen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudofdata.com&#34; target=&#34;_blank&#34;&gt;Paul Miller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~pedrod/&#34; target=&#34;_blank&#34;&gt;Pedro Domingos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/pseries/&#34; target=&#34;_blank&#34;&gt;Peggy Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.nyu.edu/~sermanet&#34; target=&#34;_blank&#34;&gt;Pierre Sermanet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.nyu.edu/~mirowski/&#34; target=&#34;_blank&#34;&gt;Piotr Mirowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.stanford.edu/~quocle/&#34; target=&#34;_blank&#34;&gt;Quoc V. Le&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bci.tugraz.at/scherer/&#34; target=&#34;_blank&#34;&gt;Reinhold Scherer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.socher.org/&#34; target=&#34;_blank&#34;&gt;Richard Socher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php&#34; target=&#34;_blank&#34;&gt;Rob Fergus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mil.engr.utk.edu/nmil/member/19.html&#34; target=&#34;_blank&#34;&gt;Robert Coop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~rcg/&#34; target=&#34;_blank&#34;&gt;Robert Gens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.site.uottawa.ca/~laganier/&#34; target=&#34;_blank&#34;&gt;Robert Laganière&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.csail.mit.edu/rgrosse/&#34; target=&#34;_blank&#34;&gt;Roger Grosse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ronan.collobert.com/&#34; target=&#34;_blank&#34;&gt;Ronan Collobert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.utstat.toronto.edu/~rsalakhu/&#34; target=&#34;_blank&#34;&gt;Ruslan Salakhutdinov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://seanjtaylor.com&#34; target=&#34;_blank&#34;&gt;Sean J. Taylor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html&#34; target=&#34;_blank&#34;&gt;Sebastian Gerwinn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sebastianruder.com&#34; target=&#34;_blank&#34;&gt;Sebastian’s&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openresearch.wordpress.com&#34; target=&#34;_blank&#34;&gt;Siah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cmap.polytechnique.fr/~mallat/&#34; target=&#34;_blank&#34;&gt;Stéphane Mallat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ais.uni-bonn.de/behnke/&#34; target=&#34;_blank&#34;&gt;Sven Behnke&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://users.ics.aalto.fi/praiko/&#34; target=&#34;_blank&#34;&gt;Tapani Raiko&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/tsainath/&#34; target=&#34;_blank&#34;&gt;Tara Sainath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://magnus-notitia.blogspot.com.tr&#34; target=&#34;_blank&#34;&gt;Tevfik Kosar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/&#34; target=&#34;_blank&#34;&gt;Tijmen Tieleman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mil.engr.utk.edu/nmil/member/36.html&#34; target=&#34;_blank&#34;&gt;Tom Karnowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.facebook.com/tomas-mikolov&#34; target=&#34;_blank&#34;&gt;Tomáš Mikolov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.idsia.ch/~meier/&#34; target=&#34;_blank&#34;&gt;Ueli Meier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vincent.vanhoucke.com/&#34; target=&#34;_blank&#34;&gt;Vincent Vanhoucke&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~vmnih/&#34; target=&#34;_blank&#34;&gt;Volodymyr Mnih&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wesmckinney.com&#34; target=&#34;_blank&#34;&gt;Wes McKinney&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yann.lecun.com/&#34; target=&#34;_blank&#34;&gt;Yann LeCun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~tang/&#34; target=&#34;_blank&#34;&gt;Yichuan Tang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html&#34; target=&#34;_blank&#34;&gt;Yoshua Bengio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yota.ro/&#34; target=&#34;_blank&#34;&gt;Yotaro Kubo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.stanford.edu/~wzou&#34; target=&#34;_blank&#34;&gt;Youzhi (Will) Zou&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
      
    </item>
    
    <item>
      <title>AI, ML, Deep Learning, NLP Conferences &amp; Journals</title>
      <link>http://localhost:1313/dsblog/ai-ml-dl-nlp-conferences/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Dr. Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/ai-ml-dl-nlp-conferences/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;../../assets/images/dsresources/dsr108-AI-ML-Deep-Learning-NLP-Conferences-Journals.jpg&#34; alt=&#34;AI, ML, Deep Learning, NLP Conferences &amp;amp; Journals&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;AI, ML, Deep Learning, NLP Conferences &amp;amp; Journals 
    &lt;div id=&#34;ai-ml-deep-learning-nlp-conferences--journals&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#ai-ml-deep-learning-nlp-conferences--journals&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Content on this page keeps changing. These links are taken from my chrome browser favorites.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A list of 30+ AI conferences and journals related to AI, NLP, Deep Learning. Almost all major happening in the world of AI/ML is presented in these conferences and published in these magazines.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
