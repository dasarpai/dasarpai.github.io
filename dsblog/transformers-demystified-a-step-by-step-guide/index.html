<!DOCTYPE html>
<html lang="en" dir="ltr" class="scroll-smooth" data-default-appearance="dark"
  data-auto-appearance="true"><head>
  <meta charset="utf-8" />
  
    <meta http-equiv="content-language" content="en" />
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />

  
  <title>Transformers Demystified A Step-by-Step Guide &middot; </title>
    <meta name="title" content="Transformers Demystified A Step-by-Step Guide &middot; " />

  
  <meta name="description" content="Exploring AI with Consciousness" />

  <meta name="keywords" content="Transformer Models, Deep Learning (DL), Natural Language Processing (NLP), Neural Networks, Machine Learning (ML), AI Architecture, Transformer Architecture, " />

  

  

  <link rel="canonical" href="/dsblog/transformers-demystified-a-step-by-step-guide/" />
  
  
  
  

  
  
  
  
  
    
  
 
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.992f1ed2a700f2e3c354557698d56ee0d5187b3459ea3c095ee28646144bd437b6483b666f8e95b8ab291da0087ea2c2c328b9190ebb2bb54742e3180351c6db.css"
    integrity="sha512-mS8e0qcA8uPDVFV2mNVu4NUYezRZ6jwJXuKGRhRL1De2SDtmb46VuKspHaAIfqLCwyi5GQ67K7VHQuMYA1HG2w==" />
  
  
 
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
    
    
    
  
 
  
    
    
  
 
  
 
  
  
 
  
    
    <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.cfdc882a8d2ac2af2f62adaf5e62be2fb299823d11e1e8dd03dadbc766bac71d0ec56bbd6ada5577eaea8e5d6158e390b1dd2ab33cc2916d154a79e11c520851.js"
      integrity="sha512-z9yIKo0qwq8vYq2vXmK&#43;L7KZgj0R4ejdA9rbx2a6xx0OxWu9atpVd&#43;rqjl1hWOOQsd0qszzCkW0VSnnhHFIIUQ==" data-copy="" data-copied=""></script>
  
 
  
    
    <script src="/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js" integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj&#43;Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script>
  
 
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/%20site.webmanifest" />
  
  
  
  
    <meta name="google-site-verification" content="google926354b0a3e2593e.html" />
  
  
  
  
  
  
  
  
  
  

  
  <meta property="og:url" content="/dsblog/transformers-demystified-a-step-by-step-guide/">
  <meta property="og:title" content="Transformers Demystified A Step-by-Step Guide">
  <meta property="og:description" content="Transformers Demystified A Step-by-Step Guide # All modern Transformers are based on a paper “Attention is all you need”
Introduction # This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2024-07-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-07-25T00:00:00+00:00">
    <meta property="article:tag" content="Transformer Models">
    <meta property="article:tag" content="Deep Learning (DL)">
    <meta property="article:tag" content="Natural Language Processing (NLP)">
    <meta property="article:tag" content="Neural Networks">
    <meta property="article:tag" content="Machine Learning (ML)">
    <meta property="article:tag" content="AI Architecture">

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Transformers Demystified A Step-by-Step Guide">
  <meta name="twitter:description" content="Transformers Demystified A Step-by-Step Guide # All modern Transformers are based on a paper “Attention is all you need”
Introduction # This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.">


   
   
      
      <meta property="og:image" content="/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg" />
      <meta property="og:image:alt" content="Transformers Demystified A Step-by-Step Guide" />
      <meta name="twitter:image" content="/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg" />
      <meta name="twitter:image:alt" content="Transformers Demystified A Step-by-Step Guide" />
   

  
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$']],     
        displayMath: [['$$', '$$']]   
      }
    };
</script>  
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>




  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Data Science Blog",
    "name": "Transformers Demystified A Step-by-Step Guide",
    "headline": "Transformers Demystified A Step-by-Step Guide",
    
    "abstract": "\u003cp\u003e\n    \u003cfigure\u003e\n      \u003cimg class=\u0022my-0 rounded-md\u0022 loading=\u0022lazy\u0022 src=\u0022\/assets\/images\/dspost\/dsp6113-transformers-demystified-a-step-by-step-guide.jpg\u0022 alt=\u0022Transformers Demystified A Step-by-Step Guide\u0022 \/\u003e\n      \n    \u003c\/figure\u003e\n\u003c\/p\u003e\n\n\n\u003ch1 class=\u0022relative group\u0022\u003eTransformers Demystified A Step-by-Step Guide \n    \u003cdiv id=\u0022transformers-demystified-a-step-by-step-guide\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#transformers-demystified-a-step-by-step-guide\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h1\u003e\n\u003cp\u003eAll modern Transformers are based on a paper \u0026ldquo;Attention is all you need\u0026rdquo;\u003c\/p\u003e\n\n\n\u003ch2 class=\u0022relative group\u0022\u003eIntroduction \n    \u003cdiv id=\u0022introduction\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#introduction\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h2\u003e\n\u003cp\u003eThis was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning \/ Google in 2017. We will discuss following and anything whatever question\/observation\/idea I have.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "\/dsblog\/transformers-demystified-a-step-by-step-guide\/",
    "author" : {
      "@type": "Person",
      "name": "Dr. Hari Thapliyaal"
    },
    "copyrightYear": "2024",
    "dateCreated": "2024-07-25T00:00:00\u002b00:00",
    "datePublished": "2024-07-25T00:00:00\u002b00:00",
    
    "dateModified": "2024-07-25T00:00:00\u002b00:00",
    
    "keywords": ["Transformer Architecture","Neural Networks","Attention Mechanism","Deep Learning Models","NLP Architecture","Machine Learning Models","AI Model Design","Language Processing"],
    
    "mainEntityOfPage": "true",
    "wordCount": "7314"
  }]
  </script>


  
  
    <meta name="author" content="Dr. Hari Thapliyaal" />
  
  
  
    
      
        
          <link href="https://twitter.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://github.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://linkedin.com/in/harithapliyal" rel="me" />
        
      
    
      
        
          <link href="https://instagram.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://facebook.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://youtube.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://discord.com/channels/1194908486153285712" rel="me" />
        
      
    
      
        
          <link href="https://orcid.org/0000-0001-7907-865X" rel="me" />
        
      
    
      
        
      
    
  
 
  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>






















  
  


   <script async src="https://www.googletagmanager.com/gtag/js?id=G-1R4BY557PZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1R4BY557PZ');
</script>



  
  
  
  <meta name="theme-color" />
  
  
    
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js"></script>


      <script>
         const firebaseConfig = {
           apiKey: "AIzaSyCagquMLI341aJrl9SjyvA5OtDC0nwUy7s",
           authDomain: "dasarpai-firebase-project1.firebaseapp.com",
           projectId: "dasarpai-firebase-project1",
           storageBucket: "dasarpai-firebase-project1.firebasestorage.app",
           messagingSenderId: "1099195695524",
           appId: "1:1099195695524:web:dbe9a07f509e7db9f54131",
           
         };
       
         var app = firebase.initializeApp(firebaseConfig);
         var db = firebase.firestore();
         var auth = firebase.auth();
       </script>
       

    
  

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

  


   

</head><body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a>
  </div>
  
  
  <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0" style="z-index:100">
  <div id="menu-blur" class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="container mx-auto px-4">  
    <div class="relative">
      <style>
   .site-title {
       display: block;
       font-weight: bold;
       color: inherit;
       text-decoration: none;
   }
   .site-subtitle {
       display: block;
       font-size: 0.75rem;
       opacity: 0.8;
       margin-top: 0.25rem;
   }
</style>

<div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3">
    
      
      
      <div class="flex items-center gap-4">  
         <a href="/" class="flex-shrink-0">  
               <span class="sr-only"></span>
               
               <img src="/assets/images/site-logo.png" 
                  class="logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom rounded-lg" 
                  alt="" />
               
         </a>

         <div class="flex flex-col">  
               <a class="site-title" href="/">
                  dasarpAI
                  <span class="site-subtitle">Exploring AI with Consciousness</span>
               </a>
         </div>
      </div>
      

    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900"></a>
            

        </nav>
        <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">

            
            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Home
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/aboutme"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            About Me
          </p>
        </a>
        
        <a href="/clients"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Clients
          </p>
        </a>
        
        <a href="/mycertifications"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            My Certifications
          </p>
        </a>
        
        <a href="/testimonials"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Testimonial
          </p>
        </a>
        
        <a href="/pmlogy-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMLOGY Home
          </p>
        </a>
        
        <a href="/wia-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            WIA Home
          </p>
        </a>
        
        <a href="/samskrutyatra-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            SamskrutYatra Home
          </p>
        </a>
        
        <a href="/publications-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Publications
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Services
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dscourses"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Courses/Services
          </p>
        </a>
        
        <a href="/projects/project-index-page"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project/Work Catalog
          </p>
        </a>
        
        <a href="/projects/summary-of-al-ml-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork by Business Domain
          </p>
        </a>
        
        <a href="/projects/summary-of-my-technology-stacks"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork by Tech Stack
          </p>
        </a>
        
        <a href="/projects/summary-of-management-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork in Project Management
          </p>
        </a>
        
        <a href="/management"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Management Courses/Services
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      My Blogs
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Blog
          </p>
        </a>
        
        <a href="/wiaposts"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Blog
          </p>
        </a>
        
        <a href="/quotations"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom Quotes
          </p>
        </a>
        
        <a href="/samskrutyatra"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Blog
          </p>
        </a>
        
        <a href="/categories/mychanting"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            My Chantings
          </p>
        </a>
        
        <a href="/gk"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Blog
          </p>
        </a>
        
        <a href="/booksummary"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Books/Interviews Blog
          </p>
        </a>
        
        <a href="/news"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            AI and Business News
          </p>
        </a>
        
        <a href="/pmblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMLOGY Blog
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      My gallery
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/gallery/slider-online-sessions1"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 1
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions2"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 2
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions3"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 3
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions4"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 4
          </p>
        </a>
        
        <a href="/gallery/slider-pm-selected-photos"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Management Classes
          </p>
        </a>
        
        <a href="/gallery/slider-pm-workshops"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PM &amp; DS Workshop
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Tags
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Tags
          </p>
        </a>
        
        <a href="/wiaposts/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Tags
          </p>
        </a>
        
        <a href="/samskrutyatra/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Yatra Tags
          </p>
        </a>
        
        <a href="/pmblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project Management Tags
          </p>
        </a>
        
        <a href="/pmbok6/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMBOK6 Tags
          </p>
        </a>
        
        <a href="/pmbok6hi/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMBOK6hi Tags
          </p>
        </a>
        
        <a href="/booksummary/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Booksummary Tags
          </p>
        </a>
        
        <a href="/gk/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Tags
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Topics
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Categories
          </p>
        </a>
        
        <a href="/wiaposts/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Categories
          </p>
        </a>
        
        <a href="/samskrutyatra/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Yatra Categories
          </p>
        </a>
        
        <a href="/pmblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project Management Categories
          </p>
        </a>
        
        <a href="/booksummary/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Booksummary Categories
          </p>
        </a>
        
        <a href="/gk/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Categories
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class=" flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 md:hidden">

        <label id="menu-button" class="block">
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li id="menu-close-button">
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Home
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/aboutme"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            About Me
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/clients"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Clients
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/mycertifications"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            My Certifications
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/testimonials"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Testimonial
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmlogy-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMLOGY Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wia-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            WIA Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            SamskrutYatra Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/publications-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Publications
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Services
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dscourses"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Courses/Services
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/project-index-page"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project/Work Catalog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-al-ml-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork by Business Domain
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-my-technology-stacks"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork by Tech Stack
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-management-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork in Project Management
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/management"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Management Courses/Services
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            My Blogs
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/quotations"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom Quotes
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/categories/mychanting"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            My Chantings
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Books/Interviews Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/news"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            AI and Business News
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMLOGY Blog
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            My gallery
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions1"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 1
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions2"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 2
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions3"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 3
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions4"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 4
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-pm-selected-photos"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Management Classes
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-pm-workshops"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PM &amp; DS Workshop
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Tags
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Yatra Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project Management Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmbok6/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMBOK6 Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmbok6hi/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMBOK6hi Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Booksummary Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Tags
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Topics
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Yatra Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project Management Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Booksummary Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Categories
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>




<script>
    (function () {
        var $mainmenu = $('.main-menu');
        var path = window.location.pathname;
        $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
            $(e).children('p').addClass('active');
        });
    })();
</script>


    </div>
  </div>
</div>
<script>
  window.addEventListener('scroll', function (e) {
    var scroll = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop || 0;
    var background_blur = document.getElementById('menu-blur');
    background_blur.style.opacity = (scroll / 300);
  });
</script>

  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      










  




<style>
  .article-content img {
    width: 100%;
    max-width: 100vw;
    height: auto;
    object-fit: contain;
    margin-left: 50%;
    transform: translateX(-50%);
  }
  @media (max-width: 768px) {
    .article-content img {
      width: 100%;
      margin-left: 0;
      transform: none;
    }
  }
</style>

<article>
  

  <header id="single_header" class="mt-5">
    
      <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/"
      >dasarpAI</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/dsblog/"
      >Data Science Blog</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/dsblog/transformers-demystified-a-step-by-step-guide/"
      >Transformers Demystified A Step-by-Step Guide</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


    

    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      Transformers Demystified A Step-by-Step Guide
    </h1>

    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  





  



  





  



  





  



<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2024-07-25T00:00:00&#43;00:00">July 25, 2024</time><span class="px-2 text-primary-500">&middot;</span><span>7314 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">35 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2024-07-25-6113-Transformers_Demystified_A_Step-by-Step_Guide.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span><span class="px-2 text-primary-500">&middot;</span><span>
    <button id="button_likes"
        class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400"
        onclick="process_article()">
        <span id="button_likes_heart" style="display:none" class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

 </span>
        <span id="button_likes_emtpty_heart" class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z"/></svg>
  </span>

</span>
        <span id="button_likes_text">&nbsp;Like</span>
    </button>
</span><span class="px-2 text-primary-500">&middot;</span>


<script type="text/javascript" src="/js/zen-mode.min.eea5245cf9244ecbdf2c150d1c8833226c1541cadf6e98f63a7c9192b1a3676df2c3ec603b14f4cfaaa53971fd9d8955640c0f405bf3de2b43ee7a5fb29ae721.js" integrity="sha512-7qUkXPkkTsvfLBUNHIgzImwVQcrfbpj2OnyRkrGjZ23yw&#43;xgOxT0z6qlOXH9nYlVZAwPQFvz3itD7npfsprnIQ=="></script>

<span class="mb-[2px]">
    <span id="zen-mode-button"
          class="text-lg hover:text-primary-500"
          title="Enable zen mode"
          data-title-i18n-disable="Enable zen mode"
          data-title-i18n-enable="Disable zen mode">
        <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50px" height="50px">
    <path fill="currentColor" d="M 12.980469 4 C 9.1204688 4 5.9804688 7.14 5.9804688 11 L 6 26 L 9.9804688 26 L 9.9804688 11 C 9.9804688 9.35 11.320469 8 12.980469 8 L 40.019531 8 C 41.679531 8 43.019531 9.35 43.019531 11 L 43.019531 39 C 43.019531 40.65 41.679531 42 40.019531 42 L 29 42 C 29 43.54 28.420938 44.94 27.460938 46 L 40.019531 46 C 43.879531 46 47.019531 42.86 47.019531 39 L 47.019531 11 C 47.019531 7.14 43.879531 4 40.019531 4 L 12.980469 4 z M 7 28 C 4.794 28 3 29.794 3 32 L 3 42 C 3 44.206 4.794 46 7 46 L 23 46 C 25.206 46 27 44.206 27 42 L 27 32 C 27 29.794 25.206 28 23 28 L 7 28 z M 7 32 L 23 32 L 23.001953 42 L 7 42 L 7 32 z"/>
</svg>
  </span>

</span>
    </span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/ai/ml-models/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI/ML Models
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/artificial-intelligence-ai/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Artificial Intelligence (AI)
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/transformer-models/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer Models
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/deep-learning-dl/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Deep Learning (DL)
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/natural-language-processing-nlp/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Natural Language Processing (NLP)
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/neural-networks/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Neural Networks
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/machine-learning-ml/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Machine Learning (ML)
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/ai-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI Architecture
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/transformer-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformer Architecture
  </span>
</span>
  </span>
  
  
  
  
</div>




    </div>

    

    
    

    

    
      

      

      
    
  </header>

  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
    
    

    
      <div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8">
        <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]">
          
            <h4>On This Page</h4>
<details open id="TOCView"
  class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#what-was-need-of-this-work">What was need of this work?</a>
      <ul>
        <li><a href="#limitations-of-previous-models">Limitations of Previous Models</a></li>
      </ul>
    </li>
    <li><a href="#nlp-tasks">NLP Tasks</a></li>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#metrics">Metrics</a></li>
        <li><a href="#benchmarks">Benchmarks</a></li>
      </ul>
    </li>
    <li><a href="#key-terms">Key terms</a></li>
    <li><a href="#with-encoder-only-architecture-we-can-do">With &ldquo;Encoder Only&rdquo; Architecture we can do.</a></li>
    <li><a href="#with-decoder-only-architecture-we-can-do">With &ldquo;Decoder only&rdquo; Architecture we can do.</a></li>
    <li><a href="#with-encoder-decoder-architecture-we-can-do-following-task">With Encoder-Decoder Architecture we can do following task.</a></li>
    <li><a href="#why-do-we-need-encoder-decoder-architectures">Why do we need encoder-decoder architectures?</a>
      <ul>
        <li><a href="#resource-considerations">Resource Considerations</a></li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
    <li><a href="#popular-transformer-architectures">Popular Transformer Architectures</a></li>
    <li><a href="#why-multi-headed-attention">Why Multi headed attention?</a>
      <ul>
        <li><a href="#syntactic-relationships">Syntactic Relationships:</a></li>
        <li><a href="#semantic-relationships">Semantic Relationships:</a></li>
        <li><a href="#coreference-resolution">Coreference Resolution:</a></li>
        <li><a href="#long-range-dependencies">Long-Range Dependencies:</a></li>
        <li><a href="#positional-information">Positional Information:</a></li>
        <li><a href="#named-entity-recognition">Named Entity Recognition:</a></li>
        <li><a href="#polarity-and-sentiment">Polarity and Sentiment:</a></li>
      </ul>
    </li>
    <li><a href="#how-attention-works">How attention works?</a></li>
    <li><a href="#how-multihead-attention-works">How multihead attention works.</a></li>
    <li><a href="#how-q-k-v-calculated">How Q, K, V Calculated?</a>
      <ul>
        <li><a href="#steps-for-multi-head-attention">Steps for Multi-Head Attention:</a></li>
        <li><a href="#scaled-dot-product-attention-per-head">Scaled Dot-Product Attention (Per Head):</a></li>
        <li><a href="#concatenation-and-final-linear-layer">Concatenation and Final Linear Layer:</a></li>
      </ul>
    </li>
    <li><a href="#floating-point-value-range">Floating Point Value Range</a>
      <ul>
        <li><a href="#common-precision-formats-for-embeddings">Common Precision Formats for Embeddings</a></li>
        <li><a href="#in-summary">In Summary</a></li>
      </ul>
    </li>
    <li><a href="#illustration-of-working-of-transformer">Illustration of Working of Transformer</a>
      <ul>
        <li><a href="#step-1-tokenization-and-embedding">Step 1: Tokenization and Embedding</a></li>
        <li><a href="#step-2-input-to-the-transformer-model">Step 2: Input to the Transformer Model</a></li>
        <li><a href="#step-3-self-attention">Step 3: <a href="#self-attention-mechanism">Self Attention</a></a></li>
        <li><a href="#step-4-processing-through-transformer-layers">Step 4. <strong>Processing Through Transformer Layers</strong></a></li>
        <li><a href="#step-5-output-logits">Step 5. <strong>Output Logits</strong></a></li>
        <li><a href="#step-6-softmax-function">Step 6. <strong>Softmax Function</strong></a></li>
        <li><a href="#step-7-token-selection">Step 7. <strong>Token Selection</strong></a></li>
        <li><a href="#step-8-generating-the-next-token">Step 8. <strong>Generating the Next Token</strong></a></li>
        <li><a href="#generating-text">Generating Text</a></li>
      </ul>
    </li>
    <li><a href="#what-different-parameters-are-learned-during-transformer-training">What different parameters are learned during transformer training?</a>
      <ul>
        <li><a href="#1-weights-and-biases-in-transformer-architecture">1. <strong>Weights and Biases in Transformer Architecture</strong></a>
          <ul>
            <li><a href="#attention-mechanism-weights"><strong>Attention Mechanism Weights</strong></a></li>
            <li><a href="#feed-forward-network-weights"><strong>Feed-Forward Network Weights</strong></a></li>
            <li><a href="#layer-normalization-weights"><strong>Layer Normalization Weights</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-overall-model-weights-and-biases">2. <strong>Overall Model Weights and Biases</strong></a>
          <ul>
            <li><a href="#embedding-weights"><strong>Embedding Weights</strong></a></li>
            <li><a href="#layer-weights"><strong>Layer Weights</strong></a></li>
          </ul>
        </li>
        <li><a href="#summary-1">Summary</a></li>
      </ul>
    </li>
    <li><a href="#self-attention-mechanism">Self Attention Mechanism</a>
      <ul>
        <li><a href="#compute-q-k-v-matrices-for-multi-head-attention-2-heads">Compute Q, K, V Matrices for Multi-Head Attention (2 heads)</a></li>
        <li><a href="#compute-q-k-v-for-head-1">Compute Q, K, V for Head 1</a>
          <ul>
            <li><a href="#compute-q-k-v-for-each-token-for-head-1">Compute Q, K, V for each token for Head 1:</a></li>
          </ul>
        </li>
        <li><a href="#step-3-self-attention-calculation">Step 3: Self-Attention Calculation</a>
          <ul>
            <li><a href="#compute-attention-scores">Compute attention scores:</a></li>
            <li><a href="#apply-softmax-to-obtain-attention-weights">Apply softmax to obtain attention weights:</a></li>
            <li><a href="#compute-the-weighted-sum-of-the-values">Compute the weighted sum of the values:</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#position-embedding-mechanism">Position Embedding Mechanism</a>
      <ul>
        <li><a href="#formulas-for-position-embedding">Formulas for Position Embedding</a></li>
        <li><a href="#explanation">Explanation</a></li>
        <li><a href="#example">Example</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#what-was-need-of-this-work">What was need of this work?</a>
      <ul>
        <li><a href="#limitations-of-previous-models">Limitations of Previous Models</a></li>
      </ul>
    </li>
    <li><a href="#nlp-tasks">NLP Tasks</a></li>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#metrics">Metrics</a></li>
        <li><a href="#benchmarks">Benchmarks</a></li>
      </ul>
    </li>
    <li><a href="#key-terms">Key terms</a></li>
    <li><a href="#with-encoder-only-architecture-we-can-do">With &ldquo;Encoder Only&rdquo; Architecture we can do.</a></li>
    <li><a href="#with-decoder-only-architecture-we-can-do">With &ldquo;Decoder only&rdquo; Architecture we can do.</a></li>
    <li><a href="#with-encoder-decoder-architecture-we-can-do-following-task">With Encoder-Decoder Architecture we can do following task.</a></li>
    <li><a href="#why-do-we-need-encoder-decoder-architectures">Why do we need encoder-decoder architectures?</a>
      <ul>
        <li><a href="#resource-considerations">Resource Considerations</a></li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
    <li><a href="#popular-transformer-architectures">Popular Transformer Architectures</a></li>
    <li><a href="#why-multi-headed-attention">Why Multi headed attention?</a>
      <ul>
        <li><a href="#syntactic-relationships">Syntactic Relationships:</a></li>
        <li><a href="#semantic-relationships">Semantic Relationships:</a></li>
        <li><a href="#coreference-resolution">Coreference Resolution:</a></li>
        <li><a href="#long-range-dependencies">Long-Range Dependencies:</a></li>
        <li><a href="#positional-information">Positional Information:</a></li>
        <li><a href="#named-entity-recognition">Named Entity Recognition:</a></li>
        <li><a href="#polarity-and-sentiment">Polarity and Sentiment:</a></li>
      </ul>
    </li>
    <li><a href="#how-attention-works">How attention works?</a></li>
    <li><a href="#how-multihead-attention-works">How multihead attention works.</a></li>
    <li><a href="#how-q-k-v-calculated">How Q, K, V Calculated?</a>
      <ul>
        <li><a href="#steps-for-multi-head-attention">Steps for Multi-Head Attention:</a></li>
        <li><a href="#scaled-dot-product-attention-per-head">Scaled Dot-Product Attention (Per Head):</a></li>
        <li><a href="#concatenation-and-final-linear-layer">Concatenation and Final Linear Layer:</a></li>
      </ul>
    </li>
    <li><a href="#floating-point-value-range">Floating Point Value Range</a>
      <ul>
        <li><a href="#common-precision-formats-for-embeddings">Common Precision Formats for Embeddings</a></li>
        <li><a href="#in-summary">In Summary</a></li>
      </ul>
    </li>
    <li><a href="#illustration-of-working-of-transformer">Illustration of Working of Transformer</a>
      <ul>
        <li><a href="#step-1-tokenization-and-embedding">Step 1: Tokenization and Embedding</a></li>
        <li><a href="#step-2-input-to-the-transformer-model">Step 2: Input to the Transformer Model</a></li>
        <li><a href="#step-3-self-attention">Step 3: <a href="#self-attention-mechanism">Self Attention</a></a></li>
        <li><a href="#step-4-processing-through-transformer-layers">Step 4. <strong>Processing Through Transformer Layers</strong></a></li>
        <li><a href="#step-5-output-logits">Step 5. <strong>Output Logits</strong></a></li>
        <li><a href="#step-6-softmax-function">Step 6. <strong>Softmax Function</strong></a></li>
        <li><a href="#step-7-token-selection">Step 7. <strong>Token Selection</strong></a></li>
        <li><a href="#step-8-generating-the-next-token">Step 8. <strong>Generating the Next Token</strong></a></li>
        <li><a href="#generating-text">Generating Text</a></li>
      </ul>
    </li>
    <li><a href="#what-different-parameters-are-learned-during-transformer-training">What different parameters are learned during transformer training?</a>
      <ul>
        <li><a href="#1-weights-and-biases-in-transformer-architecture">1. <strong>Weights and Biases in Transformer Architecture</strong></a>
          <ul>
            <li><a href="#attention-mechanism-weights"><strong>Attention Mechanism Weights</strong></a></li>
            <li><a href="#feed-forward-network-weights"><strong>Feed-Forward Network Weights</strong></a></li>
            <li><a href="#layer-normalization-weights"><strong>Layer Normalization Weights</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-overall-model-weights-and-biases">2. <strong>Overall Model Weights and Biases</strong></a>
          <ul>
            <li><a href="#embedding-weights"><strong>Embedding Weights</strong></a></li>
            <li><a href="#layer-weights"><strong>Layer Weights</strong></a></li>
          </ul>
        </li>
        <li><a href="#summary-1">Summary</a></li>
      </ul>
    </li>
    <li><a href="#self-attention-mechanism">Self Attention Mechanism</a>
      <ul>
        <li><a href="#compute-q-k-v-matrices-for-multi-head-attention-2-heads">Compute Q, K, V Matrices for Multi-Head Attention (2 heads)</a></li>
        <li><a href="#compute-q-k-v-for-head-1">Compute Q, K, V for Head 1</a>
          <ul>
            <li><a href="#compute-q-k-v-for-each-token-for-head-1">Compute Q, K, V for each token for Head 1:</a></li>
          </ul>
        </li>
        <li><a href="#step-3-self-attention-calculation">Step 3: Self-Attention Calculation</a>
          <ul>
            <li><a href="#compute-attention-scores">Compute attention scores:</a></li>
            <li><a href="#apply-softmax-to-obtain-attention-weights">Apply softmax to obtain attention weights:</a></li>
            <li><a href="#compute-the-weighted-sum-of-the-values">Compute the weighted sum of the values:</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#position-embedding-mechanism">Position Embedding Mechanism</a>
      <ul>
        <li><a href="#formulas-for-position-embedding">Formulas for Position Embedding</a></li>
        <li><a href="#explanation">Explanation</a></li>
        <li><a href="#example">Example</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>

<script>

  var margin = 200;
  var marginError = 50;

  (function () {
    var $window = $(window);
    var $toc = $('#TOCView');
    var tocHeight = $toc.height();

    function onResize() {
      var windowAndMarginHeight = $window.height() - margin;
      if(tocHeight >= windowAndMarginHeight) {
        $toc.css("overflow-y", "scroll")
        $toc.css("max-height", (windowAndMarginHeight + marginError) + "px")
      } else {
        $toc.css("overflow-y", "hidden")
        $toc.css("max-height", "9999999px")
      }
    }

    $window.on('resize', onResize);
    $(document).ready(onResize);
  })();



  (function () {
    var $toc = $('#TableOfContents');
    if ($toc.length > 0) {
      var $window = $(window);

      function onScroll() {
        var currentScroll = $window.scrollTop();
        var h = $('.anchor');
        var id = "";
        h.each(function (i, e) {
          e = $(e);
          if (e.offset().top - $(window).height()/3 <= currentScroll) {
            id = decodeURIComponent(e.attr('id'));
          }
        });
        var active = $toc.find('a.active');      
        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

        active.each(function (i, e) {
          
            $(e).removeClass('active');
          
        });
        $toc.find('a[href="#' + id + '"]').addClass('active')
        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
          $(e).children('a').parents('ul').show();          
        });
      }

      $window.on('scroll', onScroll);
      $(document).ready(function () {
        
        onScroll();
      });
    }
  })();


</script>

          
          
          
        </div>
      </div>
    

    <div class="min-w-0 min-h-0 max-w-fit">
      
      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    <b>Share with :</b> 
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share on LinkedIn"
      aria-label="Share on LinkedIn"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://twitter.com/intent/tweet/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;text=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Tweet on Twitter"
      aria-label="Tweet on Twitter"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://api.whatsapp.com/send?text=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share via WhatsApp"
      aria-label="Share via WhatsApp"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://t.me/share/url?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share via Telegram"
      aria-label="Share via Telegram"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://pinterest.com/pin/create/bookmarklet/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;description=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Pin on Pinterest"
      aria-label="Pin on Pinterest"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M496 256c0 137-111 248-248 248-25.6 0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8 0 128.7-68.8 128.7-154.3 0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1 0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6 0-54.7 41.4-107.6 112-107.6 60.9 0 103.6 41.5 103.6 100.9 0 67.1-33.9 113.6-78 113.6-24.3 0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6 0-19-10.2-34.9-31.4-34.9-24.9 0-44.9 25.7-44.9 60.2 0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9 0 361.1 0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.facebook.com/sharer/sharer.php?u=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;quote=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share on Facebook"
      aria-label="Share on Facebook"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="mailto:?body=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;subject=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Send via email"
      aria-label="Send via email"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://bsky.app/intent/compose?text=Transformers%20Demystified%20A%20Step-by-Step%20Guide&#43;/dsblog/transformers-demystified-a-step-by-step-guide/"
      title="Post on Bluesky"
      aria-label="Post on Bluesky"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://reddit.com/submit/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Submit to Reddit"
      aria-label="Submit to Reddit"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>


    </a>
      
    
  </section>


      <div class="article-content mb-20">
        <p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg" alt="Transformers Demystified A Step-by-Step Guide" />
      
    </figure>
</p>


<h1 class="relative group">Transformers Demystified A Step-by-Step Guide 
    <div id="transformers-demystified-a-step-by-step-guide" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#transformers-demystified-a-step-by-step-guide" aria-label="Anchor">#</a>
    </span>        
    
</h1>
<p>All modern Transformers are based on a paper &ldquo;Attention is all you need&rdquo;</p>


<h2 class="relative group">Introduction 
    <div id="introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#introduction" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.</p>
<ul>
<li>The need
Why this paper was needed? What problem it solved?</li>
<li>What is transformer? What is encoder transformer? What is decoder transformer? What is encoder-decoder transformer?</li>
<li>What is embedding? What is need for embedding? What are different types of embedding? What embeddingg is proposed in this work</li>
<li>What benchmark dataset was used, what metrics were used and what was the performance of this model?</li>
<li>Finally we will looks all the calculations with one illustration.</li>
</ul>
<p>Encourage all to read this <a href="https://arxiv.org/abs/1706.03762" target="_blank">original paper</a>.</p>


<h2 class="relative group">What was need of this work? 
    <div id="what-was-need-of-this-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#what-was-need-of-this-work" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>This paper addressed several limitations of previous sequence-to-sequence models used for tasks such as machine translation, text summarization, and other natural language processing (NLP) applications. The need for this paper arose from various challenges and limitations in existing models:</p>


<h3 class="relative group">Limitations of Previous Models 
    <div id="limitations-of-previous-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#limitations-of-previous-models" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p><strong>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks</strong>:</p>
<ul>
<li><strong>Sequential Computation</strong>: RNNs and LSTMs process sequences step-by-step, which makes it difficult to parallelize computations and slows down training and inference.</li>
<li><strong>Long-Range Dependencies</strong>: RNNs and LSTMs struggle to capture dependencies in long sequences, leading to difficulties in understanding context over long distances.</li>
<li><strong>Gradient Issues</strong>: These models can suffer from vanishing or exploding gradient problems, particularly with long sequences.</li>
</ul>
</li>
<li>
<p><strong>Convolutional Neural Networks (CNNs)</strong>:</p>
<ul>
<li><strong>Fixed Context Size</strong>: CNNs have a fixed receptive field, which can limit their ability to capture long-range dependencies effectively.</li>
<li><strong>Complexity</strong>: Extending CNNs to capture longer contexts can significantly increase the model&rsquo;s complexity and computational cost.</li>
</ul>
</li>
</ol>
<p>The introduction of the Transformer architecture had a profound impact on the field of NLP and beyond. It paved the way for the development of large-scale pre-trained language models like BERT, GPT, and others, which have since become the foundation for many state-of-the-art AI applications. The principles of the Transformer architecture have also been adapted for other domains, such as image processing and reinforcement learning.</p>


<h2 class="relative group">NLP Tasks 
    <div id="nlp-tasks" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#nlp-tasks" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<ol>
<li>
<p><strong>Text Classification</strong>:</p>
<ul>
<li><strong>Spam Detection</strong>: Classifying messages as spam or non-spam.</li>
<li><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</li>
<li><strong>Sarcasm Detection</strong></li>
<li><strong>Offensive Language Detection</strong></li>
</ul>
</li>
<li>
<p><strong>Sentiment and Emotion Analysis</strong>: Determining the sentiment or emotional tone expressed in a text.</p>
</li>
<li>
<p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., people, organizations, locations) within a text.</p>
</li>
<li>
<p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in a text.</p>
</li>
<li>
<p><strong>Machine Translation</strong>: Translating text from one language to another (e.g., English to French).</p>
</li>
<li>
<p><strong>Language Modeling</strong>: Predicting the next word or character in a sequence, often used in generating text or improving other NLP tasks.</p>
</li>
<li>
<p><strong>Text Summarization</strong>:</p>
<ul>
<li><strong>Extractive Summarization</strong>: Extracting key sentences from a text to create a summary.</li>
<li><strong>Abstractive Summarization</strong>: Generating a concise summary that captures the main ideas of the text.</li>
</ul>
</li>
<li>
<p><strong>Question Answering</strong>: Providing answers to questions based on a given text or dataset.</p>
</li>
<li>
<p><strong>Text Generation</strong>: Generating coherent and contextually relevant text, such as in chatbots or story generation.</p>
</li>
<li>
<p><strong>Text Similarity</strong>: Measuring how similar two pieces of text are, which can be used in tasks like duplicate detection or paraphrase identification.</p>
</li>
<li>
<p><strong>Coreference Resolution</strong>: Identifying when different expressions in a text refer to the same entity.</p>
</li>
<li>
<p><strong>Speech Recognition</strong>: Converting spoken language into text.</p>
</li>
<li>
<p><strong>Speech Synthesis (Text-to-Speech)</strong>: Converting text into spoken language.</p>
</li>
<li>
<p><strong>Dialogue Systems</strong>:</p>
<ul>
<li><strong>Chatbots</strong>: Engaging in conversation with users.</li>
<li><strong>Virtual Assistants</strong>: Assisting users with tasks through natural language interactions.</li>
</ul>
</li>
<li>
<p><strong>Information Retrieval</strong>: Finding relevant information within large datasets or the web, such as search engines.</p>
</li>
<li>
<p><strong>Dependency Parsing</strong>: Analyzing the grammatical structure of a sentence to establish relationships between words.</p>
</li>
<li>
<p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p>
</li>
<li>
<p><strong>Textual Entailment</strong>: Determining if one sentence logically follows from another.</p>
</li>
<li>
<p><strong>Word Sense Disambiguation</strong>: Identifying which sense of a word is used in a given context.</p>
</li>
<li>
<p><strong>Natural Language Inference (NLI)</strong>: Determining if a premise logically entails a hypothesis.</p>
</li>
</ol>
<p>Each of these tasks involves different techniques and models, often leveraging machine learning and deep learning to achieve state-of-the-art performance.</p>


<h2 class="relative group">Background 
    <div id="background" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#background" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Google Translate was launched on April 28, 2006. Initially, it was a statistical machine translation service that used United Nations and European Parliament transcripts to gather linguistic data.</p>
<p>It used a statistical machine translation (SMT) approach. This method relied on statistical models to translate text based on patterns found in large volumes of bilingual text corpora. The SMT system analyzed these patterns to make educated guesses about the most likely translations.</p>
<p>In 2016, Google Translate transitioned to using a neural machine translation (NMT) system, specifically the Google Neural Machine Translation (GNMT) system. This system uses deep learning techniques and neural networks to provide more accurate and natural translations by considering the entire sentence as a whole, rather than translating piece by piece.</p>


<h3 class="relative group">Metrics 
    <div id="metrics" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#metrics" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>BLEU (Bilingual Evaluation Understudy) score, which measures the quality of machine-translated text against reference translations.</p>
<p>Google reported that the new NMT system achieved improvements ranging from 55% to 85% across various language pairs in terms of BLEU scores compared to their previous SMT system. This was a substantial leap in translation quality.</p>
<ul>
<li>For Chinese to English translations, the BLEU score improvement was reported to be around 24.2, a significant increase from the previous SMT system.</li>
<li>For English to French, the BLEU score improvement was noted as being around 5-8 points higher than the SMT system.</li>
</ul>


<h3 class="relative group">Benchmarks 
    <div id="benchmarks" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#benchmarks" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p><strong>WMT 2014 English-to-German (En-De) Translation Task</strong>:</p>
<ul>
<li><strong>Dataset</strong>: The dataset consisted of 4.5 million sentence pairs.</li>
<li><strong>Performance</strong>: The Transformer model achieved a BLEU score of 28.4, which was a significant improvement over the previous state-of-the-art models.</li>
</ul>
</li>
<li>
<p><strong>WMT 2014 English-to-French (En-Fr) Translation Task</strong>:</p>
<ul>
<li><strong>Dataset</strong>: The dataset consisted of 36 million sentence pairs.</li>
<li><strong>Performance</strong>: The Transformer model achieved a BLEU score of 41.8, which also outperformed previous models.</li>
</ul>
</li>
</ol>


<h2 class="relative group">Key terms 
    <div id="key-terms" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#key-terms" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<ol>
<li>
<p><strong>Transformer Architecture</strong>: The paper introduces the Transformer, a novel architecture solely based on attention mechanisms, dispensing with recurrence and convolutions entirely.</p>
</li>
<li>
<p><strong>Attention Mechanism</strong>: The core idea is the use of self-attention, allowing the model to weigh the importance of different words in a sentence when forming a representation of each word.</p>
</li>
<li>
<p><strong>Self-Attention</strong>: Self-attention allows each word to focus on different parts of the input sentence, making it easier for the model to understand context and relationships between words.</p>
</li>
<li>
<p><strong>Multi-Head Attention</strong>: Instead of performing a single attention function, the Transformer employs multiple attention heads, each focusing on different parts of the sentence, capturing diverse aspects of the information.</p>
</li>
<li>
<p><strong>Positional Encoding</strong>: Since the Transformer does not have recurrence, it uses positional encodings to give the model information about the position of each word in the sentence.</p>
</li>
<li>
<p><strong>Layer Normalization and Residual Connections</strong>: Each sub-layer (multi-head attention and feed-forward) is followed by layer normalization and residual connections, aiding in training deep networks.</p>
</li>
<li>
<p><strong>Encoder-Decoder Structure</strong>: The Transformer is composed of an encoder (which processes the input) and a decoder (which generates the output). Each consists of multiple layers of self-attention and feed-forward networks.</p>
</li>
<li>
<p><strong>Performance</strong>: The Transformer achieves state-of-the-art performance on several NLP tasks while being more parallelizable and faster to train than recurrent architectures like LSTMs and GRUs.</p>
</li>
<li>
<p><strong>Scalability</strong>: Due to its architecture, the Transformer scales well with the amount of available data and computational power, making it suitable for large-scale tasks.</p>
</li>
</ol>


<h2 class="relative group">With &ldquo;Encoder Only&rdquo; Architecture we can do. 
    <div id="with-encoder-only-architecture-we-can-do" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#with-encoder-only-architecture-we-can-do" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>An encoder-only architecture, such as BERT (Bidirectional Encoder Representations from Transformers), is typically used for tasks that require understanding and representing input sequences without generating sequences. Here are some common NLP tasks that can be effectively handled using an encoder-only architecture:</p>
<ol>
<li>
<p><strong>Text Classification</strong>:</p>
<ul>
<li><strong>Sentiment Analysis</strong>: Determining the sentiment (positive, negative, neutral) expressed in a piece of text.</li>
<li><strong>Spam Detection</strong>: Classifying messages as spam or not spam.</li>
<li><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</li>
</ul>
</li>
<li>
<p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., persons, organizations, locations) within the text.</p>
</li>
<li>
<p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in the text.</p>
</li>
<li>
<p><strong>Question Answering (Extractive)</strong>: Extracting an answer from a given context based on a query. For instance, answering questions from a passage of text.</p>
</li>
<li>
<p><strong>Textual Entailment</strong>: Determining whether a given hypothesis logically follows from a premise (also known as Natural Language Inference, NLI).</p>
</li>
<li>
<p><strong>Sentence Pair Classification</strong>:</p>
<ul>
<li><strong>Paraphrase Detection</strong>: Identifying whether two sentences are paraphrases of each other.</li>
<li><strong>Semantic Similarity</strong>: Measuring how similar two sentences or phrases are in meaning.</li>
</ul>
</li>
<li>
<p><strong>Coreference Resolution</strong>: Determining which expressions in a text refer to the same entity.</p>
</li>
<li>
<p><strong>Text Summarization (Extractive)</strong>: Selecting the most important sentences from a document to create a summary.</p>
</li>
<li>
<p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p>
</li>
<li>
<p><strong>Information Retrieval</strong>: Ranking and retrieving relevant documents based on a query.</p>
</li>
<li>
<p><strong>Document Classification</strong>: Categorizing entire documents into classes or categories.</p>
</li>
<li>
<p><strong>Named Entity Disambiguation</strong>: Resolving ambiguities in named entities to identify the correct entity among potential candidates.</p>
</li>
<li>
<p><strong>Feature Extraction for Downstream Tasks</strong>: Generating contextualized embeddings from text to be used as features in other machine learning models.</p>
</li>
</ol>
<p>Encoder-only models are particularly effective in tasks that benefit from understanding the context and semantics of the input text, as they are designed to capture rich, contextual representations of the input data.</p>


<h2 class="relative group">With &ldquo;Decoder only&rdquo; Architecture we can do. 
    <div id="with-decoder-only-architecture-we-can-do" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#with-decoder-only-architecture-we-can-do" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Decoder-only architectures, such as GPT (Generative Pre-trained Transformer), are primarily designed for tasks involving text generation. These models are well-suited for autoregressive tasks where the goal is to predict or generate text based on a given context. Here are some common NLP tasks that can be effectively handled using a decoder-only architecture:</p>
<ol>
<li>
<p><strong>Text Generation</strong>:</p>
<ul>
<li><strong>Creative Writing</strong>: Generating coherent and contextually relevant creative content, such as stories, poems, or dialogues.</li>
<li><strong>Content Generation</strong>: Creating blog posts, articles, or other types of written content.</li>
</ul>
</li>
<li>
<p><strong>Language Modeling</strong>:</p>
<ul>
<li><strong>Next Word Prediction</strong>: Predicting the next word or token in a sequence given the preceding context.</li>
<li><strong>Completion</strong>: Providing completion for a partially written sentence or text.</li>
</ul>
</li>
<li>
<p><strong>Conversational AI</strong>:</p>
<ul>
<li><strong>Chatbots</strong>: Engaging in conversation with users, generating responses to user inputs.</li>
<li><strong>Virtual Assistants</strong>: Assisting with tasks through natural language interactions.</li>
</ul>
</li>
<li>
<p><strong>Text Summarization (Abstractive)</strong>: Generating a concise summary of a text that captures the main ideas, potentially rephrasing and reorganizing information.</p>
</li>
<li>
<p><strong>Machine Translation</strong>: Translating text from one language to another in an autoregressive manner, generating translated sentences token by token.</p>
</li>
<li>
<p><strong>Dialogue Generation</strong>:</p>
<ul>
<li><strong>Interactive Fiction</strong>: Generating dialogues in interactive fiction or role-playing scenarios.</li>
<li><strong>Conversational Agents</strong>: Generating responses in a conversation based on the context of the dialogue.</li>
</ul>
</li>
<li>
<p><strong>Storytelling</strong>: Creating narratives or expanding on a given prompt to generate a complete story or narrative.</p>
</li>
<li>
<p><strong>Autoregressive Text Editing</strong>: Modifying or editing text based on a given context, such as rewriting or expanding text.</p>
</li>
<li>
<p><strong>Text-based Games</strong>: Generating responses and interactions in text-based games or interactive storytelling environments.</p>
</li>
</ol>
<p>Decoder-only architectures excel in generating text sequences and modeling complex language patterns due to their autoregressive nature. They predict the next token in a sequence based on the previous tokens, which makes them ideal for tasks that involve creating or completing text.</p>


<h2 class="relative group">With Encoder-Decoder Architecture we can do following task. 
    <div id="with-encoder-decoder-architecture-we-can-do-following-task" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#with-encoder-decoder-architecture-we-can-do-following-task" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Encoder-decoder architectures, like those used in the original Transformer model and its derivatives (e.g., T5, BART), are particularly well-suited for tasks that involve transforming one sequence into another. These models leverage the encoder to process and understand the input sequence and the decoder to generate the output sequence. Here are some key tasks that benefit from an encoder-decoder architecture:</p>
<ol>
<li>
<p><strong>Machine Translation</strong>: Translating text from one language to another. The encoder processes the source language text, while the decoder generates the translated text in the target language.</p>
</li>
<li>
<p><strong>Text Summarization</strong>:</p>
<ul>
<li><strong>Abstractive Summarization</strong>: Generating a concise and coherent summary of a document, potentially rephrasing and synthesizing information from the source text.</li>
</ul>
</li>
<li>
<p><strong>Text-to-Text Tasks</strong>: Treating various NLP tasks as text-to-text problems, where both the input and output are sequences of text. Examples include:</p>
<ul>
<li><strong>Question Answering</strong>: Generating answers to questions based on a provided context or passage.</li>
<li><strong>Text Generation with Constraints</strong>: Generating text based on specific constraints or instructions.</li>
</ul>
</li>
<li>
<p><strong>Image Captioning</strong>: Generating a textual description of an image. The encoder processes features extracted from the image, and the decoder generates a descriptive sentence.</p>
</li>
<li>
<p><strong>Speech-to-Text</strong>: Converting spoken language (speech) into written text. The encoder processes the audio features, while the decoder generates the corresponding text.</p>
</li>
<li>
<p><strong>Text-Based Conversational Systems</strong>:</p>
<ul>
<li><strong>Dialogue Generation</strong>: Generating responses in a conversation where the input may be a previous dialogue context or user query, and the output is a coherent response.</li>
</ul>
</li>
<li>
<p><strong>Paraphrase Generation</strong>: Rewriting or generating paraphrased versions of a given text while preserving its meaning.</p>
</li>
<li>
<p><strong>Story Generation</strong>: Generating complete stories or narratives based on a prompt or initial context.</p>
</li>
<li>
<p><strong>Semantic Parsing</strong>: Converting natural language into a structured format or query (e.g., converting a sentence into a SQL query).</p>
</li>
<li>
<p><strong>Text Style Transfer</strong>: Transforming the style of a given text while preserving its original meaning (e.g., changing a formal text into an informal one).</p>
</li>
<li>
<p><strong>Multi-Modal Tasks</strong>: Combining multiple types of input (e.g., text and images) to generate output in one modality (e.g., generating text from images or audio).</p>
</li>
</ol>
<p>Encoder-decoder architectures are versatile and powerful for tasks that require generating output sequences based on complex input sequences, making them suitable for a wide range of applications in natural language processing and beyond.</p>


<h2 class="relative group">Why do we need encoder-decoder architectures? 
    <div id="why-do-we-need-encoder-decoder-architectures" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#why-do-we-need-encoder-decoder-architectures" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Both encoder-only and decoder-only architectures have their own strengths and are suited to different types of tasks. The choice between them often depends on the specific requirements of the task and the trade-offs in terms of computational resources, complexity, and performance. Here&rsquo;s a comparison of why you might choose an encoder-decoder architecture over a decoder-only one, and considerations about resource usage:</p>
<ol>
<li>
<p><strong>Handling Complex Input-Output Relationships</strong>:</p>
<ul>
<li><strong>Task Complexity</strong>: Encoder-decoder models excel at tasks where the input and output are significantly different in structure or length, such as machine translation or summarization. The encoder captures the complex relationships in the input sequence, and the decoder generates a well-formed output sequence.</li>
<li><strong>Contextual Encoding</strong>: The encoder can effectively represent the entire input sequence in a structured way, allowing the decoder to generate a sequence that reflects the input&rsquo;s context accurately.</li>
</ul>
</li>
<li>
<p><strong>Improved Performance on Sequence-to-Sequence Tasks</strong>:</p>
<ul>
<li><strong>Attention Mechanism</strong>: The encoder-decoder framework allows for sophisticated attention mechanisms that can focus on different parts of the input sequence while generating the output. This is crucial for tasks where the output needs to reference specific parts of the input.</li>
</ul>
</li>
<li>
<p><strong>Versatility</strong>:</p>
<ul>
<li><strong>Generalization</strong>: Encoder-decoder models can be adapted for a variety of tasks beyond just sequence generation, including text-to-text transformations and multi-modal tasks (e.g., generating text from images).</li>
</ul>
</li>
<li>
<p><strong>Decoupling of Representation and Generation</strong>:</p>
<ul>
<li><strong>Modularity</strong>: The separation of encoding and decoding processes allows for specialized models and training procedures. This can be advantageous when tuning models for specific tasks.</li>
</ul>
</li>
</ol>


<h3 class="relative group">Resource Considerations 
    <div id="resource-considerations" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#resource-considerations" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p><strong>Computational Resources</strong>:</p>
<ul>
<li><strong>Encoder-Decoder Models</strong>: Typically require more resources compared to decoder-only models because they need to handle both encoding and decoding processes. This involves more parameters and more complex computations, particularly for tasks with long input sequences.</li>
<li><strong>Decoder-Only Models</strong>: Can be more resource-efficient for tasks that involve generating text based on a fixed context, as they focus solely on the generation process.</li>
</ul>
</li>
<li>
<p><strong>Training and Inference</strong>:</p>
<ul>
<li><strong>Encoder-Decoder Models</strong>: Training can be more resource-intensive due to the dual structure (encoder and decoder). Inference can also be slower because it involves both encoding the input and generating the output.</li>
<li><strong>Decoder-Only Models</strong>: Training might be less complex since there is only one component involved, and inference can be faster for generation tasks due to the lack of a separate encoding step.</li>
</ul>
</li>
</ol>


<h3 class="relative group">Summary 
    <div id="summary" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#summary" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><strong>Encoder-Decoder Models</strong>: Best suited for tasks where the input and output sequences are complex and need sophisticated handling. They provide a structured approach to managing different sequence transformations and can handle a wide range of sequence-to-sequence tasks.</li>
<li><strong>Decoder-Only Models</strong>: More efficient for tasks focused solely on text generation or autoregressive modeling, where the context is provided, and the focus is on generating a continuation or response.</li>
</ul>
<p>Choosing between encoder-decoder and decoder-only architectures depends on the specific task requirements and the trade-offs between performance and computational efficiency. For tasks involving intricate input-output relationships, an encoder-decoder model is often preferred despite the higher resource demands. For tasks centered on generating sequences based on a fixed context, a decoder-only model may be more resource-efficient.</p>


<h2 class="relative group">Popular Transformer Architectures 
    <div id="popular-transformer-architectures" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#popular-transformer-architectures" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Transformer architecture has evolved significantly since the original &ldquo;Attention Is All You Need&rdquo; paper. Here are some notable variations and advancements:</p>
<ol>
<li>
<p><strong>Original Transformer (Vaswani et al., 2017)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Comprises an encoder-decoder architecture with self-attention and multi-head attention mechanisms.</li>
<li><strong>Use Case</strong>: Initially designed for machine translation.</li>
</ul>
</li>
<li>
<p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Uses only the encoder part of the Transformer.</li>
<li><strong>Training</strong>: Pre-trained using a masked language model (MLM) and next sentence prediction (NSP) objectives.</li>
<li><strong>Use Case</strong>: Effective for various NLP tasks like question answering, sentiment analysis, and named entity recognition.</li>
</ul>
</li>
<li>
<p><strong>GPT (Generative Pre-trained Transformer)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Uses only the decoder part of the Transformer.</li>
<li><strong>Training</strong>: Pre-trained using a unidirectional (left-to-right) language model objective.</li>
<li><strong>Variants</strong>: GPT-2 and GPT-3, with GPT-4 being the latest, have scaled up the number of parameters and improved performance significantly.</li>
<li><strong>Use Case</strong>: Text generation, language translation, and more.</li>
</ul>
</li>
<li>
<p><strong>T5 (Text-to-Text Transfer Transformer)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Converts all NLP tasks into a text-to-text format.</li>
<li><strong>Training</strong>: Pre-trained on a diverse mixture of tasks and fine-tuned for specific tasks.</li>
<li><strong>Use Case</strong>: Versatile across different NLP tasks.</li>
</ul>
</li>
<li>
<p><strong>RoBERTa (A Robustly Optimized BERT Pretraining Approach)</strong>:</p>
<ul>
<li><strong>Structure</strong>: An optimized version of BERT with more training data and longer training times.</li>
<li><strong>Use Case</strong>: Improved performance on various NLP benchmarks compared to BERT.</li>
</ul>
</li>
<li>
<p><strong>ALBERT (A Lite BERT)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Reduces the number of parameters using factorized embedding parameterization and cross-layer parameter sharing.</li>
<li><strong>Use Case</strong>: Efficient and lightweight version of BERT for various NLP tasks.</li>
</ul>
</li>
<li>
<p><strong>DistilBERT</strong>:</p>
<ul>
<li><strong>Structure</strong>: A smaller, faster, and cheaper version of BERT.</li>
<li><strong>Training</strong>: Uses knowledge distillation during pre-training.</li>
<li><strong>Use Case</strong>: Suitable for environments with limited computational resources.</li>
</ul>
</li>
<li>
<p><strong>XLNet</strong>:</p>
<ul>
<li><strong>Structure</strong>: Integrates autoregressive and autoencoding approaches.</li>
<li><strong>Training</strong>: Uses permutation-based training to capture bidirectional context.</li>
<li><strong>Use Case</strong>: Effective for language modeling and various downstream NLP tasks.</li>
</ul>
</li>
<li>
<p><strong>Transformer-XL</strong>:</p>
<ul>
<li><strong>Structure</strong>: Introduces a segment-level recurrence mechanism.</li>
<li><strong>Training</strong>: Handles long-term dependencies better than traditional Transformers.</li>
<li><strong>Use Case</strong>: Suitable for tasks requiring long context understanding, like document modeling.</li>
</ul>
</li>
<li>
<p><strong>Vision Transformer (ViT)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Applies Transformer architecture to image classification tasks.</li>
<li><strong>Training</strong>: Treats image patches as tokens and processes them similarly to text.</li>
<li><strong>Use Case</strong>: Effective for various computer vision tasks.</li>
</ul>
</li>
<li>
<p><strong>DeBERTa (Decoding-enhanced BERT with disentangled attention)</strong>:</p>
<ul>
<li><strong>Structure</strong>: Enhances BERT with disentangled attention and improved position embeddings.</li>
<li><strong>Use Case</strong>: Achieves state-of-the-art results on various NLP benchmarks.</li>
</ul>
</li>
<li>
<p><strong>Swin Transformer</strong>:</p>
<ul>
<li><strong>Structure</strong>: Applies hierarchical vision Transformer architecture with shifted windows.</li>
<li><strong>Training</strong>: Designed for image classification and dense prediction tasks.</li>
<li><strong>Use Case</strong>: Effective for object detection, semantic segmentation, and more.</li>
</ul>
</li>
</ol>
<p>These variations demonstrate the versatility and adaptability of Transformer architectures across a wide range of applications in both natural language processing and computer vision.</p>


<h2 class="relative group">Why Multi headed attention? 
    <div id="why-multi-headed-attention" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#why-multi-headed-attention" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>The multi-head attention mechanism in Transformers enables the model to focus on different aspects of the input data simultaneously. Here are examples of various aspects that attention mechanisms can capture:</p>


<h3 class="relative group">Syntactic Relationships: 
    <div id="syntactic-relationships" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#syntactic-relationships" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: In the sentence &ldquo;The cat sat on the mat,&rdquo; different heads might focus on different parts of speech relationships, such as subject-verb (&ldquo;cat&rdquo; and &ldquo;sat&rdquo;) and preposition-object (&ldquo;on&rdquo; and &ldquo;mat&rdquo;).</p>


<h3 class="relative group">Semantic Relationships: 
    <div id="semantic-relationships" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#semantic-relationships" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: In the sentence &ldquo;He played the piano beautifully,&rdquo; one head might focus on the verb &ldquo;played&rdquo; and its direct object &ldquo;piano,&rdquo; while another head focuses on the adverb &ldquo;beautifully&rdquo; modifying &ldquo;played.&rdquo;</p>


<h3 class="relative group">Coreference Resolution: 
    <div id="coreference-resolution" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#coreference-resolution" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: In the text &ldquo;Alice went to the market. She bought apples,&rdquo; one head might track the coreference between &ldquo;Alice&rdquo; and &ldquo;She.&rdquo;</p>


<h3 class="relative group">Long-Range Dependencies: 
    <div id="long-range-dependencies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#long-range-dependencies" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: In the sentence &ldquo;The book that you recommended to me last week was fascinating,&rdquo; one head might focus on the relationship between &ldquo;book&rdquo; and &ldquo;fascinating,&rdquo; which are far apart in the sentence.</p>


<h3 class="relative group">Positional Information: 
    <div id="positional-information" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#positional-information" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: Different heads can capture relative positional information, such as the beginning, middle, and end of a sentence, which is crucial for understanding the structure.</p>


<h3 class="relative group">Named Entity Recognition: 
    <div id="named-entity-recognition" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#named-entity-recognition" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Example: In the sentence &ldquo;Barack Obama was born in Hawaii,&rdquo; one head might focus on identifying &ldquo;Barack Obama&rdquo; as a person.</p>


<h3 class="relative group">Polarity and Sentiment: 
    <div id="polarity-and-sentiment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#polarity-and-sentiment" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li>Could capture positive or negative sentiment associated with different parts of the text.</li>
<li>May focus on identifying subjective vs. objective statements. Ho</li>
</ul>


<h2 class="relative group">How attention works? 
    <div id="how-attention-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#how-attention-works" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>The input embedding is linearly projected into three different spaces to generate queries (𝑄), keys (𝐾), and values (𝑉).</p>
<p>$${Attention}(Q, K, V) = {softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</p>
<p>Where:</p>
<ul>
<li>Q is the query matrix.</li>
<li>K is the key matrix.</li>
<li>V is the value matrix.</li>
<li>$d_k$ is the dimension of the keys.</li>
</ul>


<h2 class="relative group">How multihead attention works. 
    <div id="how-multihead-attention-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#how-multihead-attention-works" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<ul>
<li>Base model has 512 dim embedding vector, large model has 1024 dim embedding vector.</li>
<li>Position vector of the same size is used.</li>
<li>Both vectors are pair wise added.</li>
<li>Base model has 8 heads and large model has 16 heads. Thus each head has 512/8 or 1024/16 i.e. 64 dim vector.</li>
</ul>


<h2 class="relative group">How Q, K, V Calculated? 
    <div id="how-q-k-v-calculated" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#how-q-k-v-calculated" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<ol>
<li>
<p><strong>Input Embedding Dimension for Base Model</strong>: 512</p>
</li>
<li>
<p><strong>Number of Heads</strong>: 8</p>
</li>
<li>
<p><strong>Dimension per Head</strong>: Each head will handle $$\frac{512}{8} = 64$$ dimensions.</p>
</li>
</ol>


<h3 class="relative group">Steps for Multi-Head Attention: 
    <div id="steps-for-multi-head-attention" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#steps-for-multi-head-attention" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p><strong>Linear Projections</strong>:</p>
<ul>
<li>The input embedding (of dimension 512) is linearly projected into three different spaces to generate queries ($$Q$$), keys ($K$), and values ($V$).</li>
<li>Each projection is typically done using separate learned weight matrices.</li>
<li>These projections result in three vectors: $Q$, $K$, and $V$, each of dimension 512.</li>
<li>Example: <strong>Linear Projections</strong>:
<ul>
<li>For the input $X$ of shape $$[ batch_size, sequence_length, 512 ]$:</li>
<li>$Q = XW_Q$, where $W_Q$ is a weight matrix of shape $[512, 512]$.</li>
<li>$K = XW_K$, where $W_K$ is a weight matrix of shape $[512, 512]$.</li>
<li>$V = XW_V$, where $W_V$ is a weight matrix of shape $[512, 512]$.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Splitting into Heads</strong>:</p>
<ul>
<li>After the projection, the $$Q$$, $$K$$, and $$V$$ vectors are split into 8 parts (heads).</li>
<li>Each part will have $$ \frac{512}{8} = 64 $$ dimensions.</li>
<li>This means each head gets a 64-dimensional sub-vector from $$Q$$, $$K$$, and $$V$$.</li>
<li>Example: <strong>Splitting into Heads</strong>:
<ul>
<li>Each of the $$Q$$, $$K$$, and $$V$$ matrices (of shape $$ {batch_size}, {sequence_length}, 512 $$ is reshaped and split into 8 heads.</li>
<li>For each matrix, this reshaping results in shape $$ batch_size, sequence_length, 8, 64 $$.</li>
<li>The 8 heads mean we now have 8 sets of $$Q$$, $$K$$, and $$V$$ vectors, each of dimension 64.</li>
</ul>
</li>
</ul>
</li>
</ol>


<h3 class="relative group">Scaled Dot-Product Attention (Per Head): 
    <div id="scaled-dot-product-attention-per-head" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#scaled-dot-product-attention-per-head" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Each of the 8 heads performs scaled dot-product attention independently:
$$ {Attention}(Q_i, K_i, V_i) = {softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i $$
where $$ d_k = 64 $$ is the dimension of each head.</p>


<h3 class="relative group">Concatenation and Final Linear Layer: 
    <div id="concatenation-and-final-linear-layer" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#concatenation-and-final-linear-layer" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p>The outputs from all 8 heads are concatenated:</p>
<ul>
<li>Concatenated output shape: $$batch_size, sequence_length, 8 \times 64  =  batch_size, sequence_length, 512$$ .</li>
</ul>
</li>
<li>
<p>This concatenated vector is then passed through a final linear layer (with weight matrix of shape $$[512, 512]$$) to produce the final output of the multi-head attention mechanism.</p>
</li>
</ol>
<p>This process ensures that each head independently attends to different parts of the input, capturing diverse aspects of the data.</p>


<h2 class="relative group">Floating Point Value Range 
    <div id="floating-point-value-range" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#floating-point-value-range" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>These embedding vectors holds floating point numbers. These numbers may be 64bit, 32bit, 16bit, 8bit, 4bit. What will be the range of value if we use these different bit size to hold floating number. This is useful when you are doing quantization.</p>
<p>Embedding vectors in many deep learning frameworks typically use IEEE 754 double-precision floating-point format (64-bit) for representing values, but there are also other precision formats used depending on the specific requirements and hardware capabilities.</p>
<p>Apart from number of parameters these floating point precision also make model bulky. To reduce the model size we reduce these floating point precision. This process is called quantization. For edge devices or making model run on low graded machine we can choose quantization of the model to 4bit or 8bit floating points, off course we suffer the quality of output due to this.</p>


<h3 class="relative group">Common Precision Formats for Embeddings 
    <div id="common-precision-formats-for-embeddings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#common-precision-formats-for-embeddings" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ol>
<li>
<p><strong>Double Precision (64-bit)</strong>:</p>
<ul>
<li><strong>Format</strong>: IEEE 754 double-precision floating-point.</li>
<li><strong>Precision</strong>: Provides about 15-17 significant decimal digits of precision.</li>
<li><strong>Range</strong>: Can represent values from approximately $$ \pm 5 \times 10^{-324} $$ to $$ \pm 1.79 \times 10^{308} $$.</li>
<li><strong>Use Case</strong>: Often used when high precision is required, but it&rsquo;s less common in practice for embeddings due to the increased computational and memory overhead.</li>
</ul>
</li>
<li>
<p><strong>Single Precision (32-bit)</strong>:</p>
<ul>
<li><strong>Format</strong>: IEEE 754 single-precision floating-point.</li>
<li><strong>Precision</strong>: Provides about 6-9 significant decimal digits of precision.</li>
<li><strong>Range</strong>: Can represent values from approximately $$ \pm 1.18 \times 10^{-38} $$ to $$ \pm 3.4 \times 10^{38} $$.</li>
<li><strong>Use Case</strong>: More common for embeddings due to a good balance between precision and computational efficiency.</li>
</ul>
</li>
<li>
<p><strong>Half Precision (16-bit)</strong>:</p>
<ul>
<li><strong>Format</strong>: IEEE 754 half-precision floating-point.</li>
<li><strong>Precision</strong>: Provides about 3-4 significant decimal digits of precision.</li>
<li><strong>Range</strong>: Can represent values from approximately $$ \pm 6.1 \times 10^{-5} $$ to $$ \pm 6.5 \times 10^{4} $$.</li>
<li><strong>Use Case</strong>: Used to reduce memory usage and increase computational efficiency, especially during training with GPUs that support mixed precision.</li>
</ul>
</li>
<li>
<p><strong>BFloat16 (16-bit)</strong>:</p>
<ul>
<li><strong>Format</strong>: A variant of 16-bit floating-point with a different exponent and mantissa configuration, designed to be more efficient for certain computations.</li>
<li><strong>Precision</strong>: Similar to half precision but with a larger exponent range.</li>
<li><strong>Use Case</strong>: Used in some TensorFlow models and other frameworks to optimize performance while maintaining acceptable precision.</li>
</ul>
</li>
</ol>


<h3 class="relative group">In Summary 
    <div id="in-summary" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#in-summary" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><strong>IEEE 754 Double Precision</strong> is used when high precision is crucial, but it is less common for embeddings due to the larger memory footprint and computation requirements.</li>
<li><strong>IEEE 754 Single Precision</strong> is the most common format for embeddings in many deep learning applications due to its efficiency and sufficient precision.</li>
<li><strong>Half Precision</strong> and <strong>BFloat16</strong> are used for further optimization, particularly in training scenarios where memory and computational efficiency are critical.</li>
</ul>
<p>The choice of precision format depends on the trade-offs between precision, memory usage, and computational efficiency.</p>


<h2 class="relative group">Illustration of Working of Transformer 
    <div id="illustration-of-working-of-transformer" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#illustration-of-working-of-transformer" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>For the sake of simplicity, let&rsquo;s walk through an example of how a ChatGPT-like architecture generates text, using an 8-dimensional word embedding, a vocabulary size of 100, and a multi-headed attention mechanism with 2 heads and 3 layers. We will also perform the computations for the self-attention using the Query (Q), Key (K), and Value (V) matrices.</p>


<h3 class="relative group">Step 1: Tokenization and Embedding 
    <div id="step-1-tokenization-and-embedding" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-1-tokenization-and-embedding" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p><strong>Vocabulary</strong>: 100 unique tokens.</p>
<p><strong>Input Sentence</strong>: &ldquo;Hello world&rdquo;</p>
<p><strong>Token IDs</strong>:</p>
<ul>
<li>&ldquo;Hello&rdquo; might be token ID 42.</li>
<li>&ldquo;world&rdquo; might be token ID 85.</li>
</ul>
<p><strong>Embedding</strong>: Each token ID is mapped to an 8-dimensional vector.</p>
<ul>
<li>Embedding for &ldquo;Hello&rdquo; (token ID 42): [0.1, -0.2, 0.3, 0.4, -0.5, 0.2, -0.1, 0.0]</li>
<li>Embedding for &ldquo;world&rdquo; (token ID 85): [-0.3, 0.1, 0.2, -0.4, 0.5, -0.2, 0.3, -0.1]</li>
</ul>
<p>We also need to compute position embedding. Refer <a href="#position-embedding-mechanism">Position Embedding Mechanism</a></p>


<h3 class="relative group">Step 2: Input to the Transformer Model 
    <div id="step-2-input-to-the-transformer-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-2-input-to-the-transformer-model" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>These embeddings are passed as input to the model.</p>
<p><strong>Input Matrix</strong>:</p>
<p>$$
\begin{bmatrix}
0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \
-0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \
\end{bmatrix}
$$</p>


<h3 class="relative group">Step 3: <a href="#self-attention-mechanism">Self Attention</a> 
    <div id="step-3-self-attention" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-3-self-attention" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><strong>Multi-Head Self-Attention</strong></li>
<li><strong>Combining Multi-Head Attention</strong></li>
</ul>
<p>For Head 2, similar computations will be performed to obtain $$ Q_2 $$, $$ K_2 $$, $$ V_2 $$, and the attention output. The outputs from both heads will be concatenated and then projected back into the original embedding dimension using a weight matrix $$ W^O $$.</p>


<h3 class="relative group">Step 4. <strong>Processing Through Transformer Layers</strong> 
    <div id="step-4-processing-through-transformer-layers" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-4-processing-through-transformer-layers" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>The output of the multi-head attention layer will then pass through the feed-forward neural network (FFNN) and normalization layers, completing the processing for one layer of the transformer. This process is repeated for the remaining layers, in our example 3 layers.</p>
<ul>
<li><strong>Feedforward Neural Networks</strong></li>
<li><strong>Layer Normalization</strong></li>
<li><strong>Residual Connections</strong></li>
</ul>
<p>Each layer refines the embeddings based on the input context.</p>


<h3 class="relative group">Step 5. <strong>Output Logits</strong> 
    <div id="step-5-output-logits" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-5-output-logits" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>After processing through the Transformer layers, we get output vectors (logits) for each token position.</p>
<p><strong>Output Logits for the Next Token</strong>:
Let&rsquo;s assume our logits for the next token are a 100-dimensional vector (one value per token in the vocabulary). For simplicity:</p>
<p>$$
\begin{bmatrix}
1.5, &amp; -0.3, &amp; \dots, &amp; 0.8 \
\end{bmatrix}
$$</p>


<h3 class="relative group">Step 6. <strong>Softmax Function</strong> 
    <div id="step-6-softmax-function" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-6-softmax-function" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>The logits are converted to probabilities using the softmax function.</p>
<p><strong>Softmax Output</strong>:</p>
<p>$$
\begin{bmatrix}
0.1, &amp; 0.05, &amp; \dots, &amp; 0.15 \
\end{bmatrix}
$$</p>


<h3 class="relative group">Step 7. <strong>Token Selection</strong> 
    <div id="step-7-token-selection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-7-token-selection" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>A token is selected based on the probabilities. Using sampling, top-k, or greedy decoding:</p>
<ul>
<li>Let&rsquo;s say token ID 75 is selected, corresponding to the token &ldquo;everyone&rdquo;.</li>
</ul>


<h3 class="relative group">Step 8. <strong>Generating the Next Token</strong> 
    <div id="step-8-generating-the-next-token" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-8-generating-the-next-token" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>The process repeats. The input now includes the previous tokens &ldquo;Hello world&rdquo; and the new token &ldquo;everyone&rdquo;. The model generates the next token based on this updated context.</p>


<h3 class="relative group">Generating Text 
    <div id="generating-text" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#generating-text" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>To generate text, the model uses these layers iteratively, predicting the next token in the sequence based on the previously generated tokens and the context provided by the input sequence. The output is passed through a final linear layer and softmax to produce probabilities for the next token, from which the next token is sampled or chosen.</p>
<p>By repeating this process, the model generates text token by token until a specified end condition is met.</p>


<h2 class="relative group">What different parameters are learned during transformer training? 
    <div id="what-different-parameters-are-learned-during-transformer-training" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#what-different-parameters-are-learned-during-transformer-training" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Transformer models like GPT3, GPT3.5, GPT4.0, Gemma, PaLM, Llama etc has billions of parameters. What are these parameters which are learned during training?</p>
<p>In large language models like ChatGPT, weights and biases are integral to the model&rsquo;s operation, especially within the transformer architecture. Here’s a detailed breakdown of different weights and biases used in such models, including those related to attention mechanisms:</p>


<h3 class="relative group">1. <strong>Weights and Biases in Transformer Architecture</strong> 
    <div id="1-weights-and-biases-in-transformer-architecture" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#1-weights-and-biases-in-transformer-architecture" aria-label="Anchor">#</a>
    </span>        
    
</h3>


<h4 class="relative group"><strong>Attention Mechanism Weights</strong> 
    <div id="attention-mechanism-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#attention-mechanism-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<ul>
<li>
<p><strong>Query Weights (W_q)</strong>: These weights transform the input embeddings or hidden states into query vectors. In the attention mechanism, the query vector is compared against keys to compute attention scores.</p>
</li>
<li>
<p><strong>Key Weights (W_k)</strong>: These weights transform the input embeddings or hidden states into key vectors. The attention scores are computed by comparing these keys with the query vectors.</p>
</li>
<li>
<p><strong>Value Weights (W_v)</strong>: These weights transform the input embeddings or hidden states into value vectors. The output of the attention mechanism is a weighted sum of these value vectors, based on the attention scores.</p>
</li>
<li>
<p><strong>Output Weights (W_o)</strong>: After applying the attention mechanism, the output vectors are transformed by these weights before being passed to subsequent layers.</p>
</li>
</ul>


<h4 class="relative group"><strong>Feed-Forward Network Weights</strong> 
    <div id="feed-forward-network-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#feed-forward-network-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<ul>
<li>
<p><strong>Weights (W_ff)</strong>: The feed-forward network within each transformer block has its own set of weights for transforming the hidden states. This usually includes two sets of weights:</p>
<ul>
<li><strong>Weight Matrices for Linear Transformations</strong>: These weights perform linear transformations in the feed-forward network, often involving two layers with an activation function (e.g., ReLU) in between.</li>
</ul>
</li>
<li>
<p><strong>Biases (b_ff)</strong>: Biases are used along with the weights in the feed-forward network to adjust the activation values.</p>
</li>
</ul>


<h4 class="relative group"><strong>Layer Normalization Weights</strong> 
    <div id="layer-normalization-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#layer-normalization-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<ul>
<li><strong>Gamma (γ)</strong>: Scaling parameter used in layer normalization to adjust the normalized output.</li>
<li><strong>Beta (β)</strong>: Shifting parameter used in layer normalization to adjust the mean of the normalized output.</li>
</ul>


<h3 class="relative group">2. <strong>Overall Model Weights and Biases</strong> 
    <div id="2-overall-model-weights-and-biases" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#2-overall-model-weights-and-biases" aria-label="Anchor">#</a>
    </span>        
    
</h3>


<h4 class="relative group"><strong>Embedding Weights</strong> 
    <div id="embedding-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#embedding-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<ul>
<li><strong>Token Embedding Weights</strong>: These weights map input tokens (words or subwords) to continuous vector representations (embeddings).</li>
<li><strong>Position Embedding Weights</strong>: These weights add positional information to the embeddings to encode the order of tokens in the sequence.</li>
</ul>


<h4 class="relative group"><strong>Layer Weights</strong> 
    <div id="layer-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#layer-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<ul>
<li>
<p><strong>Weights in Each Transformer Layer</strong>: Each layer of the transformer model has its own set of weights and biases for both the attention mechanism and the feed-forward network.</p>
</li>
<li>
<p><strong>Residual Connection Weights</strong>: Residual connections (or skip connections) within each transformer layer often involve weights for combining the input and output of the layer.</p>
</li>
</ul>


<h3 class="relative group">Summary 
    <div id="summary-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#summary-1" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>In summary, the different weights and biases in a model like ChatGPT are:</p>
<ul>
<li>
<p><strong>Attention Mechanism</strong>:</p>
<ul>
<li>Query Weights (W_q)</li>
<li>Key Weights (W_k)</li>
<li>Value Weights (W_v)</li>
<li>Output Weights (W_o)</li>
</ul>
</li>
<li>
<p><strong>Feed-Forward Network</strong>:</p>
<ul>
<li>Weights (W_ff)</li>
<li>Biases (b_ff)</li>
</ul>
</li>
<li>
<p><strong>Layer Normalization</strong>:</p>
<ul>
<li>Gamma (γ)</li>
<li>Beta (β)</li>
</ul>
</li>
<li>
<p><strong>Embedding Weights</strong>:</p>
<ul>
<li>Token Embedding Weights</li>
<li>Position Embedding Weights</li>
</ul>
</li>
</ul>
<p>These weights and biases are learned during the training phase and are used during inference to generate responses based on the input data. Each component of the model contributes to its ability to understand and generate human-like text.</p>


<h2 class="relative group">Self Attention Mechanism 
    <div id="self-attention-mechanism" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#self-attention-mechanism" aria-label="Anchor">#</a>
    </span>        
    
</h2>


<h3 class="relative group">Compute Q, K, V Matrices for Multi-Head Attention (2 heads) 
    <div id="compute-q-k-v-matrices-for-multi-head-attention-2-heads" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#compute-q-k-v-matrices-for-multi-head-attention-2-heads" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>We&rsquo;ll compute Q, K, and V matrices for each head.</p>
<p><strong>Weight Matrices for Q, K, V for Head 1</strong>:</p>
<p>$$
W^Q_1, W^K_1, W^V_1 \in \mathbb{R}^{8 \times 4}
$$</p>
<p><strong>Weight Matrices for Q, K, V for Head 2</strong>:</p>
<p>$$
W^Q_2, W^K_2, W^V_2 \in \mathbb{R}^{8 \times 4}
$$</p>
<p>For simplicity, let&rsquo;s use random matrices. In practice, these are learned during training.</p>
<p><strong>Embeddings</strong>:</p>
<p>$$
X = \begin{bmatrix}
0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \
-0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \
\end{bmatrix}
$$</p>
<p><strong>Weight Matrices</strong> (randomly initialized for this example):</p>
<p>Head 1:</p>
<p>$$
W^Q_1 = \begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
\end{bmatrix}</p>
<p>W^K_1 = \begin{bmatrix}
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
\end{bmatrix}</p>
<p>W^V_1 = \begin{bmatrix}
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
\end{bmatrix}
$$</p>
<p>Head 2:</p>
<p>$$
W^Q_2 = \begin{bmatrix}
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \
\end{bmatrix}</p>
<p>W^K_2 = \begin{bmatrix}
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
\end{bmatrix}</p>
<p>W^V_2 = \begin{bmatrix}
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \
\end{bmatrix}
$$</p>
<p><strong>Compute Q, K, V for each token for each head</strong>:</p>
<p><strong>Head 1</strong>:</p>
<p>$$
Q_1 = X \cdot W^Q_1 = \begin{bmatrix}
0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \
-0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \
\end{bmatrix} \cdot W^Q_1</p>
<p>K_1 = X \cdot W^K_1</p>
<p>V_1 = X \cdot W^V_1
$$</p>
<p><strong>Head 2</strong>:</p>
<p>$$
Q_2 = X \cdot W^Q_2</p>
<p>K_2 = X \cdot W^K_2</p>
<p>V_2 = X \cdot W^V_2
$$</p>
<p>Let&rsquo;s compute these step by step.</p>


<h3 class="relative group">Compute Q, K, V for Head 1 
    <div id="compute-q-k-v-for-head-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#compute-q-k-v-for-head-1" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p><strong>Input Embeddings</strong>:</p>
<p>$$
X = \begin{bmatrix}
0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \
-0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \
\end{bmatrix}
$$</p>
<p><strong>Weight Matrices for Head 1</strong>:</p>
<p>$$
W^Q_1 = \begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
\end{bmatrix}</p>
<p>W^K_1 = \begin{bmatrix}
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \
\end{bmatrix}</p>
<p>W^V_1 = \begin{bmatrix}
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \
\end{bmatrix}
$$</p>


<h4 class="relative group">Compute Q, K, V for each token for Head 1: 
    <div id="compute-q-k-v-for-each-token-for-head-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#compute-q-k-v-for-each-token-for-head-1" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<p>$$
Q_1 = X \cdot W^Q_1 = \begin{bmatrix}
0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \
-0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \
\end{bmatrix} \cdot \begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \
\end{bmatrix}
$$</p>
<p>Performing the matrix multiplication:</p>
<p>$$
Q_1 = \begin{bmatrix}
(0.1 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (0.4 \times 0.1) + (-0.5 \times 0.1) + (0.2 \times 0.1) + (-0.1 \times 0.1) + (0 \times 0.1) &amp; \dots &amp; \
(-0.3 \times 0.1) + (0.1 \times 0.1) + (0.2 \times 0.1) + (-0.4 \times 0.1) + (0.5 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (-0.1 \times 0.1) &amp; \dots &amp; \
\end{bmatrix}
$$</p>
<p>Simplifying:</p>
<p>$$
Q_1 = \begin{bmatrix}
-0.01 &amp; -0.02 &amp; -0.03 &amp; -0.04 \
0.02 &amp; 0.04 &amp; 0.06 &amp; 0.08 \
\end{bmatrix}
$$</p>
<p>Following the same steps for $$ K_1 $$ and $$ V_1 $$:</p>
<p>$$
K_1 = X \cdot W^K_1</p>
<p>= \begin{bmatrix}
0.15 &amp; 0.30 &amp; 0.45 &amp; 0.60 \
0.10 &amp; 0.20 &amp; 0.30 &amp; 0.40 \
\end{bmatrix}</p>
<p>V_1 = X \cdot W^V_1</p>
<p>= \begin{bmatrix}
0.28 &amp; 0.56 &amp; 0.84 &amp; 1.12 \
0.24 &amp; 0.48 &amp; 0.72 &amp; 0.96 \
\end{bmatrix}
$$</p>
<p><strong>Similarly you compute for head 2. Finally you concatenate both vectors and get 2x8 size matrix (same size which was input for the self attention)</strong></p>


<h3 class="relative group">Step 3: Self-Attention Calculation 
    <div id="step-3-self-attention-calculation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#step-3-self-attention-calculation" aria-label="Anchor">#</a>
    </span>        
    
</h3>


<h4 class="relative group">Compute attention scores: 
    <div id="compute-attention-scores" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#compute-attention-scores" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<p>$$
\text{Scores} = Q_1 \cdot K_1^T = \begin{bmatrix}
-0.01 &amp; -0.02 &amp; -0.03 &amp; -0.04 \
0.02 &amp; 0.04 &amp; 0.06 &amp; 0.08 \
\end{bmatrix} \cdot \begin{bmatrix}
0.15 &amp; 0.10 \
0.30 &amp; 0.20 \
0.45 &amp; 0.30 \
0.60 &amp; 0.40 \
\end{bmatrix}
$$</p>
<p>Performing the matrix multiplication:</p>
<p>$$
\text{Scores} = \begin{bmatrix}
-0.01 \times 0.15 + -0.02 \times 0.30 + -0.03 \times 0.45 + -0.04 \times 0.60 &amp; -0.01 \times 0.10 + -0.02 \times 0.20 + -0.03 \times 0.30 + -0.04 \times 0.40 \
0.02 \times 0.15 + 0.04 \times 0.30 + 0.06 \times 0.45 + 0.08 \times 0.60 &amp; 0.02 \times 0.10 + 0.04 \times 0.20 + 0.06 \times 0.30 + 0.08 \times 0.40 \
\end{bmatrix}
$$</p>
<p>Simplifying:</p>
<p>$$
\text{Scores} = \begin{bmatrix}
-0.015 - 0.006 - 0.0135 - 0.024 &amp; -0.01 - 0.004 - 0.009 - 0.016 \
0.03 + 0.012 + 0.027 + 0.048 &amp; 0.02 + 0.008 + 0.018 + 0.032 \
\end{bmatrix}</p>
<p>\text{Scores} = \begin{bmatrix}
-0.0585 &amp; -0.039 \
0.117 &amp; 0.078 \
\end{bmatrix}
$$</p>


<h4 class="relative group">Apply softmax to obtain attention weights: 
    <div id="apply-softmax-to-obtain-attention-weights" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#apply-softmax-to-obtain-attention-weights" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<p>$$
\text{Attention Weights} = \text{softmax}(\text{Scores})</p>
<p>\text{Attention Weights} = \begin{bmatrix}
\frac{e^{-0.0585}}{e^{-0.0585} + e^{-0.039}} &amp; \frac{e^{-0.039}}{e^{-0.0585} + e^{-0.039}} \
\frac{e^{0.117}}{e^{0.117} + e^{0.078}} &amp; \frac{e^{0.078}}{e^{0.117} + e^{0.078}} \
\end{bmatrix}
$$</p>
<p>Simplifying:</p>
<p>$$
\text{Attention Weights} = \begin{bmatrix}
\frac{1}{1 + e^{0.0195}} &amp; \frac{e^{0.0195}}{1 + e^{0.0195}} \
\frac{1}{1 + e^{-0.039}} &amp; \frac{e^{-0.039}}{1 + e^{-0.039}} \
\end{bmatrix}
$$</p>
<p>Approximating the values:</p>
<p>$$
\text{Attention Weights} = \begin{bmatrix}
0.495 &amp; 0.505 \
0.510 &amp; 0.490 \
\end{bmatrix}
$$</p>


<h4 class="relative group">Compute the weighted sum of the values: 
    <div id="compute-the-weighted-sum-of-the-values" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#compute-the-weighted-sum-of-the-values" aria-label="Anchor">#</a>
    </span>        
    
</h4>
<p>$$
\text{Output} = \text{Attention Weights} \cdot V_1 = \begin{bmatrix}
0.495 &amp; 0.505 \
0.510 &amp; 0.490 \
\end{bmatrix} \cdot \begin{bmatrix}
0.28 &amp; 0.56 &amp; 0.84 &amp; 1.12 \
0.24 &amp; 0.48 &amp; 0.72 &amp; 0.96 \
\end{bmatrix}
$$</p>
<p>Performing the matrix multiplication:</p>
<p>$$
\text{Output} = \begin{bmatrix}
0.495 \times 0.28 + 0.505 \times 0.24 &amp; 0.495 \times 0.56 + 0.505 \times 0.48 &amp; 0.495 \times 0.84 + 0.505 \times 0.72 &amp; 0.495 \times 1.12 + 0.505 \times 0.96 \
0.510 \times 0.28 + 0.490 \times 0.24 &amp; 0.510 \times 0.56 + 0.490 \times 0.48 &amp; 0.510 \times 0.84 + 0.490 \times 0.72 &amp; 0.510 \times 1.12 + 0.490 \times 0.96 \
\end{bmatrix}
$$</p>
<p>Simplifying:</p>
<p>$$
\text{Output} = \begin{bmatrix}
0.1386 + 0.1212 &amp; 0.2772 + 0.2424 &amp; 0.4158 + 0.3636 &amp; 0.5544 + 0.4848 \
0.1428 + 0.1176 &amp; 0.2856 + 0.2304 &amp; 0.4284 + 0.3432 &amp; 0.5712 + 0.456 \
\end{bmatrix}</p>
<p>\text{Output} = \begin{bmatrix}
0.2598 &amp; 0.5196 &amp; 0.7794 &amp; 1.0392 \
0.2604 &amp; 0.516 &amp; 0.7716 &amp; 1.0272 \
\end{bmatrix}
$$</p>


<h2 class="relative group">Position Embedding Mechanism 
    <div id="position-embedding-mechanism" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#position-embedding-mechanism" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Position embeddings are used in transformers to provide information about the order of tokens in a sequence. The most common method for computing position embeddings is using sine and cosine functions of different frequencies. This method was introduced in the original transformer paper &ldquo;Attention Is All You Need&rdquo;. The formulas for the position embeddings are as follows:</p>


<h3 class="relative group">Formulas for Position Embedding 
    <div id="formulas-for-position-embedding" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#formulas-for-position-embedding" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>For a given position $$ pos $$ and embedding dimension $$ i $$:</p>
<ol>
<li><strong>Sine Function for Even Indices:</strong></li>
</ol>
<p>$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$</p>
<ol start="2">
<li><strong>Cosine Function for Odd Indices:</strong></li>
</ol>
<p>$$ PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $$</p>
<p>Where:</p>
<ul>
<li>pos is the position of the token in the sequence (starting from 0).</li>
<li>i is the dimension index (also starting from 0).</li>
<li>$$d_{\text{model}}$$ is the dimensionality of the embeddings.</li>
</ul>


<h3 class="relative group">Explanation 
    <div id="explanation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#explanation" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><strong>Even Index:</strong> For even values of i, the position embedding is computed using the sine function.</li>
<li><strong>Odd Index:</strong> For odd values of i, the position embedding is computed using the cosine function.</li>
<li><strong>Frequency:</strong> The denominator $$10000^{\frac{2i}{d_{\text{model}}}}$$ ensures that different dimensions have different frequencies. The values for sine and cosine vary more slowly for larger dimensions, capturing different levels of granularity.</li>
</ul>


<h3 class="relative group">Example 
    <div id="example" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#example" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Let&rsquo;s assume $$ d_{\text{model}} = 8 $$ (for simplicity), and calculate the position embeddings for $$ pos = 1 $$.</p>
<p>For $$ i = 0 $$:
$$ PE(1, 0) = \sin\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \sin\left(1\right) $$</p>
<p>For $$ i = 1 $$:
$$ PE(1, 1) = \cos\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \cos\left(1\right) $$</p>
<p>For $$ i = 2 $$:
$$ PE(1, 2) = \sin\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \sin\left(\frac{1}{10}\right) $$</p>
<p>For $$ i = 3 $$:
$$ PE(1, 3) = \cos\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \cos\left(\frac{1}{10}\right) $$</p>
<p>For $$ i = 4 $$:
$$ PE(1, 4) = \sin\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \sin\left(\frac{1}{100}\right) $$</p>
<p>For $$ i = 5 $$:
$$ PE(1, 5) = \cos\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \cos\left(\frac{1}{100}\right) $$</p>
<p>For $$ i = 6 $$:
$$ PE(1, 6) = \sin\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \sin\left(\frac{1}{1000}\right) $$</p>
<p>For $$ i = 7 $$:
$$ PE(1, 7) = \cos\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \cos\left(\frac{1}{1000}\right) $$</p>
<p>These values are then added to the corresponding token embeddings to provide the model with information about the position of each token in the sequence.</p>
<p><strong>Author</strong><br>
Dr Hari Thapliyaal<br>
dasarpai.com<br>
linkedin.com/in/harithapliyal</p>


        
        
      </div>

      <style>
 

</style><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Dr. Hari Thapliyaal's avatar" class="author-image" width="25%" >
      </div>
    <div class="td-author-box__links author-image"><b>Follow Me</b>
        <a href="https://join.slack.com/t/dasarpaiworkspace/shared_invite/zt-371kuyco2-gsuhnVgMfQ_9aXPRFgiP3Q" target="_blank" rel="noopener" aria-label="Slack" title="Chat with other project users in #users">
            <i class="fab fa-slack" aria-hidden="true"></i>
        </a>
        <a href="https://groups.google.com/forum/#!forum/agones-discuss" target="_blank" rel="noopener" aria-label="User mailing list" title="Discussion and help from your fellow users">
            <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        <a href="https://twitter.com/dasarpai" target="_blank" rel="noopener" aria-label="Twitter" title="Follow us on Twitter to get the latest news!">
            <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
    </div>
  

    <div class="td-author-box__info">
    <h4 class="td-author-box__name">Dr. Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>
      <div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>
      

      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    <b>Share with :</b> 
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share on LinkedIn"
      aria-label="Share on LinkedIn"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://twitter.com/intent/tweet/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;text=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Tweet on Twitter"
      aria-label="Tweet on Twitter"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://api.whatsapp.com/send?text=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share via WhatsApp"
      aria-label="Share via WhatsApp"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://t.me/share/url?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share via Telegram"
      aria-label="Share via Telegram"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://pinterest.com/pin/create/bookmarklet/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;description=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Pin on Pinterest"
      aria-label="Pin on Pinterest"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M496 256c0 137-111 248-248 248-25.6 0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8 0 128.7-68.8 128.7-154.3 0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1 0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6 0-54.7 41.4-107.6 112-107.6 60.9 0 103.6 41.5 103.6 100.9 0 67.1-33.9 113.6-78 113.6-24.3 0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6 0-19-10.2-34.9-31.4-34.9-24.9 0-44.9 25.7-44.9 60.2 0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9 0 361.1 0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.facebook.com/sharer/sharer.php?u=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;quote=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Share on Facebook"
      aria-label="Share on Facebook"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="mailto:?body=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;subject=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Send via email"
      aria-label="Send via email"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://bsky.app/intent/compose?text=Transformers%20Demystified%20A%20Step-by-Step%20Guide&#43;/dsblog/transformers-demystified-a-step-by-step-guide/"
      title="Post on Bluesky"
      aria-label="Post on Bluesky"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://reddit.com/submit/?url=/dsblog/transformers-demystified-a-step-by-step-guide/&amp;resubmit=true&amp;title=Transformers%20Demystified%20A%20Step-by-Step%20Guide"
      title="Submit to Reddit"
      aria-label="Submit to Reddit"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>


    </a>
      
    
  </section>


    </div>

    
      
      
        
        
      

      <script>
        var oid = "views_dsblog\\2024-07-25-6113-Transformers_Demystified_A_Step-by-Step_Guide.md";
        var oid_likes = "likes_dsblog\\2024-07-25-6113-Transformers_Demystified_A_Step-by-Step_Guide.md";
      </script>
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
    
  </section>

  <footer class="pt-8 print:hidden">
    
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group mr-3" href="/dsblog/Dimensionality-Reduction-and-Visualization/">
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Dimensionality Reduction and Visualization</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-07-24T00:00:00&#43;00:00">July 24, 2024</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/dsblog/Understanding-LLM-GAN-and-Transformers/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Understanding LLM GAN and Transformers</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-07-26T00:00:00&#43;00:00">July 26, 2024</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>

  <div>
    


  
  
    
    
    
    
    
      
    
    
    
    
    
    
    
    
      
    
    
    
    
    
      
    

    
    
    
    

    
    
      <h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2>
      <section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3">
        
            
            


  <a href="/dsblog/roadmap-to-reality/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6286-roadmap-to-reality.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/roadmap-to-reality/">
	   Roadmap to Reality
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-06-14T00:00:00&#43;00:00">June 14, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>916 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-06-14-6286-Roadmap-to-Reality.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/philosophy--cognitive-science/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Philosophy &amp; Cognitive Science
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/interdisciplinary-topics/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Interdisciplinary Topics
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/scientific-journey/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Scientific Journey
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/self-discovery/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Self-Discovery
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/personal-growth/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Growth
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/cosmic-perspective/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cosmic Perspective
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/human-evolution/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Human Evolution
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/technology/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technology
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/biology/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Biology
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/neuroscience/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Neuroscience
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Roadmap to Reality # A Scientific Journey to Know the Universe — and the Self # 🌱 Introduction: The …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/from-being-hacked-to-being-reborn-linkedin-profile/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6285-from-being-hacked-to-being-reborn-linkedin-profile.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/from-being-hacked-to-being-reborn-linkedin-profile/">
	   From Being Hacked to Being Reborn: How I Rebuilt My LinkedIn Identity in 48 Hours
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-06-11T00:00:00&#43;00:00">June 11, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>893 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-06-11-6285-From-Being-Hacked-to-Being-Reborn-Linkedin-Profile.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/personal-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Branding
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/cybersecurity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cybersecurity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/technology-trends--future/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technology Trends &amp; Future
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/personal-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Branding
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/linkedin-profile/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    LinkedIn Profile
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/professional-identity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Professional Identity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/cybersecurity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cybersecurity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/online-presence/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Online Presence
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/digital-identity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Digital Identity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/online-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Online Branding
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     💔 From Being Hacked to Being Reborn: How I Rebuilt My LinkedIn Identity in 48 Hours # &ldquo;In …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/exploring-css-frameworks/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6284-exploring-css-frameworks.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/exploring-css-frameworks/">
	   Exploring CSS Frameworks - A Collection of Lightweight, Responsive, and Themeable Alternatives
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-30T00:00:00&#43;00:00">May 30, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>1378 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">7 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-30-6284-Exploring-CSS-Frameworkds.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/web-development/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Web Development
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/frontend-development/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Frontend Development
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/design-systems/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Design Systems
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/css-frameworks/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    CSS Frameworks
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/lightweight-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Lightweight CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/responsive-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Responsive CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/themeable-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Themeable CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/css-utilities/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    CSS Utilities
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/utility-first-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Utility-First CSS
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Exploring CSS Frameworks # There are many CSS frameworks and approaches you can use besides …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/dimensions-of-software-architecture/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6283-dimensions-of-software-architecture.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/dimensions-of-software-architecture/">
	   Dimensions of Software Architecture: Balancing Concerns
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-27T00:00:00&#43;00:00">May 27, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>871 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-27-6283-Dimensions-of-Software-Architecture.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/software-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Software Architecture
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/software-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Software Architecture
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/technical-debt/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technical Debt
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/maintainability/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Maintainability
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/scalability/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Scalability
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/performance/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Performance
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Dimensions of Software Architecture # Call these &ldquo;Architectural Concern Categories&rdquo; or …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/Understanding-async-await-and-Concurrency/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6282-Understanding-async-await-and-Concurrency.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/Understanding-async-await-and-Concurrency/">
	   Understanding `async`, `await`, and Concurrency in Python
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-27T00:00:00&#43;00:00">May 27, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>637 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">3 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-27-6282-Understanding-async-await-and-Concurrency.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/python/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Python
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/asyncio/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Asyncio
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/concurrency/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Concurrency
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/synchronous-programming/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Synchronous Programming
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/asynchronous-programming/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Asynchronous Programming
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Understanding async, await, and Concurrency # Understanding async, await, and Concurrency in Python …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
      </section>
    
  

  </div>
</article>

      <div id="top-scroller" style="position: fixed; bottom: 2rem; left: 0; right: 0; display: flex; justify-content: center; z-index: 9999;">
   <a href="#the-top"
      style="display: flex; height: 3.5rem; width: 3.5rem; align-items: center; justify-content: center; border-radius: 9999px; background-color: rgba(107, 114, 128, 0.8); color: white; transition: all 0.3s; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);"
      onmouseover="this.style.backgroundColor='rgba(75, 85, 99, 0.9)'; this.style.boxShadow='0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05)';"
      onmouseout="this.style.backgroundColor='rgba(107, 114, 128, 0.8)'; this.style.boxShadow='0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)';"
      aria-label="Scroll to top" 
      title="Scroll to top">
     <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
       <line x1="12" y1="19" x2="12" y2="5"></line>
       <polyline points="5 12 12 5 19 12"></polyline>
     </svg>
   </a>
 </div>
    </main><style>
  footer#site-footer {
    background-color: #0e0517;
  }

  footer .row {
    display: flex;
    flex-wrap: wrap;

  }

  footer .row .col-md-4 {
    flex: 0 0 33.333333%;
    max-width: 33.333333%;
    padding: 15px
  }
</style>


<footer id="site-footer" class="py-10 print:hidden mt-5  rounded-md">
  
  
    <div class="row">
      <div class="container">
        <div class="row g-2">  
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Key Links</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/aboutme" title=""> 
          
          About Me
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/clients" title=""> 
          
          Clients
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/mycertifications" title=""> 
          
          My Certifications
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/testimonials" title=""> 
          
          Testimonial
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmlogy-home" title=""> 
          
          PMLOGY Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wia-home" title=""> 
          
          WIA Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra-home" title=""> 
          
          SamskrutYatra Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/publications-home" title=""> 
          
          Publications
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Services</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dscourses" title=""> 
          
          Data Science Courses/Services
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/project-index-page" title=""> 
          
          Project/Work Catalog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-al-ml-projects" title=""> 
          
          MyWork by Business Domain
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-my-technology-stacks" title=""> 
          
          MyWork by Tech Stack
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-management-projects" title=""> 
          
          MyWork in Project Management
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/management" title=""> 
          
          Management Courses/Services
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">My Blogs</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog" title=""> 
          
          Data Science Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts" title=""> 
          
          Wisdom in Awareness Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/quotations" title=""> 
          
          Wisdom Quotes
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra" title=""> 
          
          Samskrut Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/categories/mychanting" title=""> 
          
          My Chantings
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk" title=""> 
          
          GK Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary" title=""> 
          
          Books/Interviews Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/news" title=""> 
          
          AI and Business News
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog" title=""> 
          
          PMLOGY Blog
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Data Science Resources</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/categories/data-science-resources/" title=""> 
          
          DS Resources
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="https://aibenchmark-explorer.dasarpai.com" title=""> 
          
          AI Benchmark Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/ds-ai-ml-books" title=""> 
          
          Data Science Books
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/data-science-cheatsheets" title=""> 
          
          Data Science/AI Cheatsheets
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/best-youtube-channels-for-ds" title=""> 
          
          Video Channels to Learn DS/AI
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/ds-ai-ml-interview-resources" title=""> 
          
          DS/AI Interview Questions
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="https://github.com/dasarpai/DAI-Datasets" title=""> 
          
          GitHub DAI-Datasets
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/corpus" title=""> 
          
          History Corpus
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">PM Resources</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6" title=""> 
          
          PMBOK6 Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi" title=""> 
          
          PMBOK6 Hindi Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmglossary" title=""> 
          
          PM Glossary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6-summary" title=""> 
          
          PMBOK6 Summary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi-summary" title=""> 
          
          PMBoK6 Hindi Summary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmi-templates" title=""> 
          
          PMBOK6 Templates
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/prince2-templates" title=""> 
          
          PRINCE2 Templates
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/microsoft-pm-templates" title=""> 
          
          Microsoft PM Templates
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Tags</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/tags" title=""> 
          
          Data Science Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts/tags" title=""> 
          
          Wisdom in Awareness Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra/tags" title=""> 
          
          Samskrut Yatra Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog/tags" title=""> 
          
          Project Management Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6/tags" title=""> 
          
          PMBOK6 Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi/tags" title=""> 
          
          PMBOK6hi Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary/tags" title=""> 
          
          Booksummary Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk/tags" title=""> 
          
          GK Tags
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Topics/Categories</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/categories" title=""> 
          
          Data Science Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts/categories" title=""> 
          
          Wisdom in Awareness Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra/categories" title=""> 
          
          Samskrut Yatra Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog/categories" title=""> 
          
          Project Management Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary/categories" title=""> 
          
          Booksummary Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk/categories" title=""> 
          
          GK Categories
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Gallery</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions1" title=""> 
          
          Online AI Classes 1
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions2" title=""> 
          
          Online AI Classes 2
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions3" title=""> 
          
          Online AI Classes 3
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions4" title=""> 
          
          Online AI Classes 4
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-pm-selected-photos" title=""> 
          
          Management Classes
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-pm-workshops" title=""> 
          
          PM &amp; DS Workshop
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="" title=""> 
          
          
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Terms and Policies</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/terms-of-service" title=""> 
          
          Terms &amp; Condition
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/privacy" title=""> 
          
          Privacy Policy
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/comment-policy" title=""> 
          
          Comment Policy
        </a>
      </li>
    
  </ul>
</nav>
</div>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-md-4">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="/assets/images/site-logo.png" alt="dasarpAI" width="100"
            style="border-radius: 12px;">
        </a>
      </div>
    </div>

  
  <div class="flex items-center justify-between p-4">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2025
      Dr. Hari Thapliyaal
    </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    

  </div>



  <script>
    
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: 'rgba(0,0,0,0.5)',
        scrollOffset: 0,
    })
    
  </script>
  
  

  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA==">
  </script>
    
  
  
    <a rel="me" href="https://masto.ai/@blowfish"></a>
  
</footer><div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js"
  data-id="harithapliyal" data-description="Support me on Buy me a coffee!" data-message=""
  data-color="#FFDD00" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

</html>
