<!DOCTYPE html>
<html lang="en" dir="ltr" class="scroll-smooth" data-default-appearance="dark"
  data-auto-appearance="true"><head>
  <meta charset="utf-8" />
  
    <meta http-equiv="content-language" content="en" />
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />

  
  <title>Computer Vision Research Work &middot; </title>
    <meta name="title" content="Computer Vision Research Work &middot; " />

  
  <meta name="description" content="Exploring AI with Consciousness" />

  <meta name="keywords" content="Computer Vision, Research Methods, Artificial Intelligence (AI), " />

  

  

  <link rel="canonical" href="/dsblog/computer-vision-research-work/" />
  
  
  
  

  
  
  
  
  
    
  
 
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.992f1ed2a700f2e3c354557698d56ee0d5187b3459ea3c095ee28646144bd437b6483b666f8e95b8ab291da0087ea2c2c328b9190ebb2bb54742e3180351c6db.css"
    integrity="sha512-mS8e0qcA8uPDVFV2mNVu4NUYezRZ6jwJXuKGRhRL1De2SDtmb46VuKspHaAIfqLCwyi5GQ67K7VHQuMYA1HG2w==" />
  
  
 
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
    
    
    
  
 
  
    
    
  
 
  
 
  
  
 
  
    
    <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.cfdc882a8d2ac2af2f62adaf5e62be2fb299823d11e1e8dd03dadbc766bac71d0ec56bbd6ada5577eaea8e5d6158e390b1dd2ab33cc2916d154a79e11c520851.js"
      integrity="sha512-z9yIKo0qwq8vYq2vXmK&#43;L7KZgj0R4ejdA9rbx2a6xx0OxWu9atpVd&#43;rqjl1hWOOQsd0qszzCkW0VSnnhHFIIUQ==" data-copy="" data-copied=""></script>
  
 
  
    
    <script src="/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js" integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj&#43;Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script>
  
 
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/%20site.webmanifest" />
  
  
  
  
    <meta name="google-site-verification" content="google926354b0a3e2593e.html" />
  
  
  
  
  
  
  
  
  
  

  
  <meta property="og:url" content="/dsblog/computer-vision-research-work/">
  <meta property="og:title" content="Computer Vision Research Work">
  <meta property="og:description" content="Computer Vision Research Work # When we talk about “vision” capabilities, most people don’t understand how complex the brain is in processing the visual spectrum (light signals). What kind of processing happens inside our brain that allows us to understand color, depth, motion, speed, segments, objects, scenes, different kinds of art, drawings, culture, etc.? Until recently, when “computer vision” became a serious field in AI, only neurology researchers, surgeons, and brain specialists had some insights into these processes. But since 2012 (AlexNet Paper), with new papers being published almost every month, we are constantly learning how far we’ve come in computer vision. This article is not only about the chronology of computer vision but also about software engineers, computer scientists, AI engineers, and everyone who wants to understand how their phone performs certain computer visions tasks and becomes intelligent.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2025-01-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-27T00:00:00+00:00">
    <meta property="article:tag" content="Computer Vision">
    <meta property="article:tag" content="Research Methods">
    <meta property="article:tag" content="Artificial Intelligence (AI)">

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Computer Vision Research Work">
  <meta name="twitter:description" content="Computer Vision Research Work # When we talk about “vision” capabilities, most people don’t understand how complex the brain is in processing the visual spectrum (light signals). What kind of processing happens inside our brain that allows us to understand color, depth, motion, speed, segments, objects, scenes, different kinds of art, drawings, culture, etc.? Until recently, when “computer vision” became a serious field in AI, only neurology researchers, surgeons, and brain specialists had some insights into these processes. But since 2012 (AlexNet Paper), with new papers being published almost every month, we are constantly learning how far we’ve come in computer vision. This article is not only about the chronology of computer vision but also about software engineers, computer scientists, AI engineers, and everyone who wants to understand how their phone performs certain computer visions tasks and becomes intelligent.">


   
   
      
      <meta property="og:image" content="/assets/images/dspost/dsp6211-Computer-Vision-Research-Work.jpg" />
      <meta property="og:image:alt" content="Computer Vision Research Work" />
      <meta name="twitter:image" content="/assets/images/dspost/dsp6211-Computer-Vision-Research-Work.jpg" />
      <meta name="twitter:image:alt" content="Computer Vision Research Work" />
   

  
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$']],     
        displayMath: [['$$', '$$']]   
      }
    };
</script>  
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>




  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Data Science Blog",
    "name": "Computer Vision Research Work",
    "headline": "Computer Vision Research Work",
    
    "abstract": "\u003cp\u003e\n    \u003cfigure\u003e\n      \u003cimg class=\u0022my-0 rounded-md\u0022 loading=\u0022lazy\u0022 src=\u0022\/assets\/images\/dspost\/dsp6211-Computer-Vision-Research-Work.jpg\u0022 alt=\u0022Computer Vision Research Work\u0026quot;\u0022 \/\u003e\n      \n    \u003c\/figure\u003e\n\u003c\/p\u003e\n\n\n\u003ch1 class=\u0022relative group\u0022\u003eComputer Vision Research Work \n    \u003cdiv id=\u0022computer-vision-research-work\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#computer-vision-research-work\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h1\u003e\n\u003cp\u003eWhen we talk about \u0026ldquo;vision\u0026rdquo; capabilities, most people don\u0026rsquo;t understand how complex the brain is in processing the visual spectrum (light signals). What kind of processing happens inside our brain that allows us to understand color, depth, motion, speed, segments, objects, scenes, different kinds of art, drawings, culture, etc.? Until recently, when \u0026ldquo;computer vision\u0026rdquo; became a serious field in AI, only neurology researchers, surgeons, and brain specialists had some insights into these processes. But since 2012 (AlexNet Paper), with new papers being published almost every month, we are constantly learning how far we\u0026rsquo;ve come in computer vision. This article is not only about the chronology of computer vision but also about software engineers, computer scientists, AI engineers, and everyone who wants to understand how their phone performs certain computer visions tasks and becomes intelligent.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "\/dsblog\/computer-vision-research-work\/",
    "author" : {
      "@type": "Person",
      "name": "Dr. Hari Thapliyaal"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-01-27T00:00:00\u002b00:00",
    "datePublished": "2025-01-27T00:00:00\u002b00:00",
    
    "dateModified": "2025-01-27T00:00:00\u002b00:00",
    
    "keywords": ["computer vision research work","advancements in computer vision","history of computer vision","computer vision breakthroughs","computer vision research papers","computer vision techniques","computer vision applications"],
    
    "mainEntityOfPage": "true",
    "wordCount": "5225"
  }]
  </script>


  
  
    <meta name="author" content="Dr. Hari Thapliyaal" />
  
  
  
    
      
        
          <link href="https://twitter.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://github.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://linkedin.com/in/harithapliyal" rel="me" />
        
      
    
      
        
          <link href="https://instagram.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://facebook.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://youtube.com/dasarpai" rel="me" />
        
      
    
      
        
          <link href="https://discord.com/channels/1194908486153285712" rel="me" />
        
      
    
      
        
          <link href="https://orcid.org/0000-0001-7907-865X" rel="me" />
        
      
    
      
        
      
    
  
 
  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>






















  
  


   <script async src="https://www.googletagmanager.com/gtag/js?id=G-1R4BY557PZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1R4BY557PZ');
</script>



  
  
  
  <meta name="theme-color" />
  
  
    
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js"></script>


      <script>
         const firebaseConfig = {
           apiKey: "AIzaSyCagquMLI341aJrl9SjyvA5OtDC0nwUy7s",
           authDomain: "dasarpai-firebase-project1.firebaseapp.com",
           projectId: "dasarpai-firebase-project1",
           storageBucket: "dasarpai-firebase-project1.firebasestorage.app",
           messagingSenderId: "1099195695524",
           appId: "1:1099195695524:web:dbe9a07f509e7db9f54131",
           
         };
       
         var app = firebase.initializeApp(firebaseConfig);
         var db = firebase.firestore();
         var auth = firebase.auth();
       </script>
       

    
  

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

  


   

</head><body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a>
  </div>
  
  
  <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0" style="z-index:100">
  <div id="menu-blur" class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="container mx-auto px-4">  
    <div class="relative">
      <style>
   .site-title {
       display: block;
       font-weight: bold;
       color: inherit;
       text-decoration: none;
   }
   .site-subtitle {
       display: block;
       font-size: 0.75rem;
       opacity: 0.8;
       margin-top: 0.25rem;
   }
</style>

<div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3">
    
      
      
      <div class="flex items-center gap-4">  
         <a href="/" class="flex-shrink-0">  
               <span class="sr-only"></span>
               
               <img src="/assets/images/site-logo.png" 
                  class="logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom rounded-lg" 
                  alt="" />
               
         </a>

         <div class="flex flex-col">  
               <a class="site-title" href="/">
                  dasarpAI
                  <span class="site-subtitle">Exploring AI with Consciousness</span>
               </a>
         </div>
      </div>
      

    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900"></a>
            

        </nav>
        <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">

            
            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Home
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/aboutme"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            About Me
          </p>
        </a>
        
        <a href="/clients"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Clients
          </p>
        </a>
        
        <a href="/mycertifications"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            My Certifications
          </p>
        </a>
        
        <a href="/testimonials"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Testimonial
          </p>
        </a>
        
        <a href="/pmlogy-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMLOGY Home
          </p>
        </a>
        
        <a href="/wia-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            WIA Home
          </p>
        </a>
        
        <a href="/samskrutyatra-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            SamskrutYatra Home
          </p>
        </a>
        
        <a href="/publications-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Publications
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Services
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dscourses"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Courses/Services
          </p>
        </a>
        
        <a href="/projects/project-index-page"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project/Work Catalog
          </p>
        </a>
        
        <a href="/projects/summary-of-al-ml-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork by Business Domain
          </p>
        </a>
        
        <a href="/projects/summary-of-my-technology-stacks"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork by Tech Stack
          </p>
        </a>
        
        <a href="/projects/summary-of-management-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            MyWork in Project Management
          </p>
        </a>
        
        <a href="/management"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Management Courses/Services
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      My Blogs
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Blog
          </p>
        </a>
        
        <a href="/wiaposts"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Blog
          </p>
        </a>
        
        <a href="/quotations"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom Quotes
          </p>
        </a>
        
        <a href="/samskrutyatra"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Blog
          </p>
        </a>
        
        <a href="/categories/mychanting"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            My Chantings
          </p>
        </a>
        
        <a href="/gk"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Blog
          </p>
        </a>
        
        <a href="/booksummary"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Books/Interviews Blog
          </p>
        </a>
        
        <a href="/news"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            AI and Business News
          </p>
        </a>
        
        <a href="/pmblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMLOGY Blog
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      My gallery
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/gallery/slider-online-sessions1"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 1
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions2"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 2
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions3"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 3
          </p>
        </a>
        
        <a href="/gallery/slider-online-sessions4"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Online AI Classes 4
          </p>
        </a>
        
        <a href="/gallery/slider-pm-selected-photos"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Management Classes
          </p>
        </a>
        
        <a href="/gallery/slider-pm-workshops"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PM &amp; DS Workshop
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Tags
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Tags
          </p>
        </a>
        
        <a href="/wiaposts/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Tags
          </p>
        </a>
        
        <a href="/samskrutyatra/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Yatra Tags
          </p>
        </a>
        
        <a href="/pmblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project Management Tags
          </p>
        </a>
        
        <a href="/pmbok6/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMBOK6 Tags
          </p>
        </a>
        
        <a href="/pmbok6hi/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            PMBOK6hi Tags
          </p>
        </a>
        
        <a href="/booksummary/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Booksummary Tags
          </p>
        </a>
        
        <a href="/gk/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Tags
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Topics
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/dsblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Data Science Categories
          </p>
        </a>
        
        <a href="/wiaposts/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Wisdom in Awareness Categories
          </p>
        </a>
        
        <a href="/samskrutyatra/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Samskrut Yatra Categories
          </p>
        </a>
        
        <a href="/pmblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Project Management Categories
          </p>
        </a>
        
        <a href="/booksummary/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            Booksummary Categories
          </p>
        </a>
        
        <a href="/gk/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            GK Categories
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class=" flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 md:hidden">

        <label id="menu-button" class="block">
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li id="menu-close-button">
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Home
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/aboutme"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            About Me
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/clients"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Clients
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/mycertifications"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            My Certifications
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/testimonials"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Testimonial
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmlogy-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMLOGY Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wia-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            WIA Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            SamskrutYatra Home
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/publications-home"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Publications
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Services
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dscourses"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Courses/Services
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/project-index-page"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project/Work Catalog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-al-ml-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork by Business Domain
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-my-technology-stacks"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork by Tech Stack
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/projects/summary-of-management-projects"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            MyWork in Project Management
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/management"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Management Courses/Services
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            My Blogs
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/quotations"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom Quotes
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/categories/mychanting"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            My Chantings
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Books/Interviews Blog
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/news"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            AI and Business News
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMLOGY Blog
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            My gallery
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions1"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 1
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions2"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 2
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions3"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 3
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-online-sessions4"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Online AI Classes 4
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-pm-selected-photos"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Management Classes
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gallery/slider-pm-workshops"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PM &amp; DS Workshop
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Tags
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Yatra Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project Management Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmbok6/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMBOK6 Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmbok6hi/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            PMBOK6hi Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Booksummary Tags
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk/tags"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Tags
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                     
  <li class="mt-1">
    <a href="" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Topics
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/dsblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Data Science Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/wiaposts/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Wisdom in Awareness Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/samskrutyatra/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Samskrut Yatra Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/pmblog/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Project Management Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/booksummary/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            Booksummary Categories
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/gk/categories"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            GK Categories
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>




<script>
    (function () {
        var $mainmenu = $('.main-menu');
        var path = window.location.pathname;
        $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
            $(e).children('p').addClass('active');
        });
    })();
</script>


    </div>
  </div>
</div>
<script>
  window.addEventListener('scroll', function (e) {
    var scroll = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop || 0;
    var background_blur = document.getElementById('menu-blur');
    background_blur.style.opacity = (scroll / 300);
  });
</script>

  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      










  




<style>
  .article-content img {
    width: 100%;
    max-width: 100vw;
    height: auto;
    object-fit: contain;
    margin-left: 50%;
    transform: translateX(-50%);
  }
  @media (max-width: 768px) {
    .article-content img {
      width: 100%;
      margin-left: 0;
      transform: none;
    }
  }
</style>

<article>
  

  <header id="single_header" class="mt-5">
    
      <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/"
      >dasarpAI</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/dsblog/"
      >Data Science Blog</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/dsblog/computer-vision-research-work/"
      >Computer Vision Research Work</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


    

    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      Computer Vision Research Work
    </h1>

    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  





  



  





  



  





  



<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-01-27T00:00:00&#43;00:00">January 27, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>5225 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">25 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-01-27-6211-Computer-Vision-Research-Work.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span><span class="px-2 text-primary-500">&middot;</span><span>
    <button id="button_likes"
        class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400"
        onclick="process_article()">
        <span id="button_likes_heart" style="display:none" class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

 </span>
        <span id="button_likes_emtpty_heart" class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M244 84L255.1 96L267.1 84.02C300.6 51.37 347 36.51 392.6 44.1C461.5 55.58 512 115.2 512 185.1V190.9C512 232.4 494.8 272.1 464.4 300.4L283.7 469.1C276.2 476.1 266.3 480 256 480C245.7 480 235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1 0 232.4 0 190.9V185.1C0 115.2 50.52 55.58 119.4 44.1C164.1 36.51 211.4 51.37 244 84C243.1 84 244 84.01 244 84L244 84zM255.1 163.9L210.1 117.1C188.4 96.28 157.6 86.4 127.3 91.44C81.55 99.07 48 138.7 48 185.1V190.9C48 219.1 59.71 246.1 80.34 265.3L256 429.3L431.7 265.3C452.3 246.1 464 219.1 464 190.9V185.1C464 138.7 430.4 99.07 384.7 91.44C354.4 86.4 323.6 96.28 301.9 117.1L255.1 163.9z"/></svg>
  </span>

</span>
        <span id="button_likes_text">&nbsp;Like</span>
    </button>
</span><span class="px-2 text-primary-500">&middot;</span>


<script type="text/javascript" src="/js/zen-mode.min.eea5245cf9244ecbdf2c150d1c8833226c1541cadf6e98f63a7c9192b1a3676df2c3ec603b14f4cfaaa53971fd9d8955640c0f405bf3de2b43ee7a5fb29ae721.js" integrity="sha512-7qUkXPkkTsvfLBUNHIgzImwVQcrfbpj2OnyRkrGjZ23yw&#43;xgOxT0z6qlOXH9nYlVZAwPQFvz3itD7npfsprnIQ=="></script>

<span class="mb-[2px]">
    <span id="zen-mode-button"
          class="text-lg hover:text-primary-500"
          title="Enable zen mode"
          data-title-i18n-disable="Enable zen mode"
          data-title-i18n-enable="Disable zen mode">
        <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50px" height="50px">
    <path fill="currentColor" d="M 12.980469 4 C 9.1204688 4 5.9804688 7.14 5.9804688 11 L 6 26 L 9.9804688 26 L 9.9804688 11 C 9.9804688 9.35 11.320469 8 12.980469 8 L 40.019531 8 C 41.679531 8 43.019531 9.35 43.019531 11 L 43.019531 39 C 43.019531 40.65 41.679531 42 40.019531 42 L 29 42 C 29 43.54 28.420938 44.94 27.460938 46 L 40.019531 46 C 43.879531 46 47.019531 42.86 47.019531 39 L 47.019531 11 C 47.019531 7.14 43.879531 4 40.019531 4 L 12.980469 4 z M 7 28 C 4.794 28 3 29.794 3 32 L 3 42 C 3 44.206 4.794 46 7 46 L 23 46 C 25.206 46 27 44.206 27 42 L 27 32 C 27 29.794 25.206 28 23 28 L 7 28 z M 7 32 L 23 32 L 23.001953 42 L 7 42 L 7 32 z"/>
</svg>
  </span>

</span>
    </span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/computer-vision-cv/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Computer Vision (CV)
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/research--academia/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Research &amp; Academia
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/ai/ml-research--evaluation/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    AI/ML Research &amp; Evaluation
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/computer-vision/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Computer Vision
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/research-methods/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Research Methods
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/artificial-intelligence-ai/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Artificial Intelligence (AI)
  </span>
</span>
  </span>
  
  
  
  
</div>




    </div>

    

    
    

    

    
      

      

      
    
  </header>

  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
    
    

    

    <div class="min-w-0 min-h-0 max-w-fit">
      
      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    <b>Share with :</b> 
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/dsblog/computer-vision-research-work/&amp;title=Computer%20Vision%20Research%20Work"
      title="Share on LinkedIn"
      aria-label="Share on LinkedIn"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://twitter.com/intent/tweet/?url=/dsblog/computer-vision-research-work/&amp;text=Computer%20Vision%20Research%20Work"
      title="Tweet on Twitter"
      aria-label="Tweet on Twitter"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://api.whatsapp.com/send?text=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Share via WhatsApp"
      aria-label="Share via WhatsApp"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://t.me/share/url?url=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Share via Telegram"
      aria-label="Share via Telegram"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://pinterest.com/pin/create/bookmarklet/?url=/dsblog/computer-vision-research-work/&amp;description=Computer%20Vision%20Research%20Work"
      title="Pin on Pinterest"
      aria-label="Pin on Pinterest"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M496 256c0 137-111 248-248 248-25.6 0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8 0 128.7-68.8 128.7-154.3 0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1 0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6 0-54.7 41.4-107.6 112-107.6 60.9 0 103.6 41.5 103.6 100.9 0 67.1-33.9 113.6-78 113.6-24.3 0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6 0-19-10.2-34.9-31.4-34.9-24.9 0-44.9 25.7-44.9 60.2 0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9 0 361.1 0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.facebook.com/sharer/sharer.php?u=/dsblog/computer-vision-research-work/&amp;quote=Computer%20Vision%20Research%20Work"
      title="Share on Facebook"
      aria-label="Share on Facebook"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="mailto:?body=/dsblog/computer-vision-research-work/&amp;subject=Computer%20Vision%20Research%20Work"
      title="Send via email"
      aria-label="Send via email"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://bsky.app/intent/compose?text=Computer%20Vision%20Research%20Work&#43;/dsblog/computer-vision-research-work/"
      title="Post on Bluesky"
      aria-label="Post on Bluesky"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://reddit.com/submit/?url=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Submit to Reddit"
      aria-label="Submit to Reddit"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>


    </a>
      
    
  </section>


      <div class="article-content mb-20">
        <p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="/assets/images/dspost/dsp6211-Computer-Vision-Research-Work.jpg" alt="Computer Vision Research Work&quot;" />
      
    </figure>
</p>


<h1 class="relative group">Computer Vision Research Work 
    <div id="computer-vision-research-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#computer-vision-research-work" aria-label="Anchor">#</a>
    </span>        
    
</h1>
<p>When we talk about &ldquo;vision&rdquo; capabilities, most people don&rsquo;t understand how complex the brain is in processing the visual spectrum (light signals). What kind of processing happens inside our brain that allows us to understand color, depth, motion, speed, segments, objects, scenes, different kinds of art, drawings, culture, etc.? Until recently, when &ldquo;computer vision&rdquo; became a serious field in AI, only neurology researchers, surgeons, and brain specialists had some insights into these processes. But since 2012 (AlexNet Paper), with new papers being published almost every month, we are constantly learning how far we&rsquo;ve come in computer vision. This article is not only about the chronology of computer vision but also about software engineers, computer scientists, AI engineers, and everyone who wants to understand how their phone performs certain computer visions tasks and becomes intelligent.</p>
<table>
  <thead>
      <tr>
          <th>SNo</th>
          <th>Research Name</th>
          <th>Short Description of Paper</th>
          <th>Month-Year</th>
          <th>Organization</th>
          <th>URL</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1</td>
          <td>DeconvNet</td>
          <td>Deconvolutional Networks for Feature Learning</td>
          <td>Nov 2010</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1505.04366" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>2</td>
          <td>Saliency Propagation</td>
          <td>A method for salient object detection that propagates saliency information through optimization</td>
          <td>Apr 2014</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/1404.6190" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>3</td>
          <td>SDS</td>
          <td>Simultaneous Detection and Segmentation</td>
          <td>Jun 2014</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/1407.1808" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>4</td>
          <td>GoogleNet</td>
          <td>Introduced the Inception module to increase network depth and width efficiently.</td>
          <td>Sep-2014</td>
          <td>Google</td>
          <td></td>
      </tr>
      <tr>
          <td>5</td>
          <td>VGGNet</td>
          <td>Used small 3x3 convolution filters to increase depth, achieving high accuracy.</td>
          <td>Sep-2014</td>
          <td>Oxford University</td>
          <td></td>
      </tr>
      <tr>
          <td>6</td>
          <td>FCN</td>
          <td>Fully Convolutional Networks for semantic segmentation</td>
          <td>Nov 2014</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/1411.4038" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>7</td>
          <td>HyperColumn</td>
          <td>Multi-scale CNN feature fusion</td>
          <td>Nov 2014</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/1411.5752" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>8</td>
          <td>DeepLab v1</td>
          <td>Semantic Image Segmentation with Deep Convolutional Nets and CRFs</td>
          <td>Dec 2014</td>
          <td>Google</td>
          <td><a href="https://arxiv.org/abs/1412.7062" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>9</td>
          <td>U-Net</td>
          <td>Convolutional network for biomedical image segmentation</td>
          <td>May 2015</td>
          <td>University of Freiburg</td>
          <td><a href="https://arxiv.org/abs/1505.04597" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-squeezenet-image-classification-e7414825581a" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>10</td>
          <td>Highway Network</td>
          <td>Proposed highway layers to enable training of very deep networks.</td>
          <td>May-2015</td>
          <td>University of Montreal</td>
          <td></td>
      </tr>
      <tr>
          <td>11</td>
          <td>YOLO Series</td>
          <td>You Only Look Once: series of real-time object detection systems (v1-v4)</td>
          <td>Jun 2015 (v1) - Apr 2020 (v4)</td>
          <td>University of Washington, Darknet</td>
          <td><a href="https://arxiv.org/abs/1506.02640" target="_blank">Paper</a>, <a href="https://towardsdatascience.com/yolo-intuitively-and-exhaustively-explained-83143925c7a9/" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>12</td>
          <td>CRF-RNN</td>
          <td>Conditional Random Fields as Recurrent Neural Networks</td>
          <td>Jun 2015</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/1502.03240" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>13</td>
          <td>MR-CNN &amp; S-CNN</td>
          <td>Multi-Region CNN and Semantic CNN for object detection</td>
          <td>Jun 2015</td>
          <td>University of California, Berkeley</td>
          <td><a href="https://arxiv.org/abs/1505.01749" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>14</td>
          <td>DeepMask</td>
          <td>Learning to Segment Objects Candidates</td>
          <td>Jun 2015</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1506.06204" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>15</td>
          <td>LAPGAN</td>
          <td>Laplacian Pyramid of Generative Adversarial Networks for image generation</td>
          <td>Jun 2015</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1506.05751" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>16</td>
          <td>CUDMedVision1</td>
          <td>Medical Image Segmentation System 1</td>
          <td>Sep 2015</td>
          <td>Chinese University of Hong Kong</td>
          <td><a href="https://github.com/tangzhenyu/SemanticSegmentation_DL" target="_blank">Paper</a>, <a href="https://medium.datadriveninvestor.com/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>17</td>
          <td>SegNet</td>
          <td>Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</td>
          <td>Oct 2015</td>
          <td>University of Cambridge</td>
          <td><a href="https://arxiv.org/abs/1511.00561" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>18</td>
          <td>DilatedNet</td>
          <td>Multi-Scale Context Aggregation by Dilated Convolutions</td>
          <td>Nov 2015</td>
          <td>Princeton University</td>
          <td><a href="https://arxiv.org/abs/1511.07122" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>19</td>
          <td>CAM</td>
          <td>Class Activation Mapping for identifying discriminative regions</td>
          <td>Dec 2015</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/1512.04150" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>20</td>
          <td>ParseNet</td>
          <td>Looking Wider to See Better for semantic segmentation</td>
          <td>Dec 2015</td>
          <td>UNC Chapel Hill</td>
          <td><a href="https://arxiv.org/abs/1506.04579" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>21</td>
          <td>MNC</td>
          <td>Instance-aware Semantic Segmentation via Multi-task Network Cascades</td>
          <td>Dec 2015</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1512.04412" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>22</td>
          <td>ResNet</td>
          <td>Introduced residual learning to address vanishing gradients in deep networks.</td>
          <td>Dec-2015</td>
          <td>Microsoft Research</td>
          <td></td>
      </tr>
      <tr>
          <td>23</td>
          <td>SqueezeNet</td>
          <td>AlexNet-level accuracy with 50x fewer parameters</td>
          <td>Feb 2016</td>
          <td>UC Berkeley, Stanford</td>
          <td><a href="https://arxiv.org/abs/1602.07360" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-squeezenet-image-classification-e7414825581a" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>24</td>
          <td>SqueezeNet</td>
          <td>Designed to reduce model size while maintaining accuracy, using 1x1 convolutions.</td>
          <td>Feb-2016</td>
          <td>DeepScale, UC Berkeley</td>
          <td></td>
      </tr>
      <tr>
          <td>25</td>
          <td>Pre-activation ResNet</td>
          <td>Identity Mappings in Deep Residual Networks</td>
          <td>Mar 2016</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1603.05027" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>26</td>
          <td>SharpMask</td>
          <td>Learning to Refine Object Segments</td>
          <td>Mar 2016</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1603.08695" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>27</td>
          <td>InstanceFCN</td>
          <td>Instance-sensitive Fully Convolutional Networks</td>
          <td>Mar 2016</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1603.08678" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>28</td>
          <td>MultipathNet</td>
          <td>Multiple Path Aggregation Network</td>
          <td>Apr 2016</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1604.02135" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>29</td>
          <td>R-FCN</td>
          <td>Region-based Fully Convolutional Networks for object detection</td>
          <td>May 2016</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1605.06409" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>30</td>
          <td>NOC</td>
          <td>Neural Object Counting for object detection</td>
          <td>May 2016</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1505.05264" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>31</td>
          <td>DeepLab v2</td>
          <td>Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution</td>
          <td>Jun 2016</td>
          <td>Google</td>
          <td><a href="https://arxiv.org/abs/1606.00915" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>32</td>
          <td>DeepSim</td>
          <td>Deep Learning Approach for Image Quality Assessment</td>
          <td>Jun 2016</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/1606.01972" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>33</td>
          <td>DIS</td>
          <td>Deep Image Smoothing</td>
          <td>Jun 2016</td>
          <td>University of Illinois</td>
          <td><a href="https://arxiv.org/abs/1606.03634" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>34</td>
          <td>V-Net</td>
          <td>Fully Convolutional Neural Network for volumetric medical image segmentation</td>
          <td>Jun 2016</td>
          <td>University College London</td>
          <td><a href="https://arxiv.org/abs/1606.04797" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>35</td>
          <td>3D U-net</td>
          <td>Volumetric Segmentation with 3D U-net</td>
          <td>Jun 2016</td>
          <td>University of Freiburg</td>
          <td><a href="https://arxiv.org/abs/1606.06650" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>36</td>
          <td>ENet</td>
          <td>Efficient Neural Network for Real-time Semantic Segmentation</td>
          <td>Jul 2016</td>
          <td>University of Cambridge</td>
          <td><a href="https://arxiv.org/abs/1606.02147" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>37</td>
          <td>ResNet38</td>
          <td>Wider or Deeper: Revisiting the ResNet Model</td>
          <td>Jul 2016</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1611.10080" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>38</td>
          <td>DRRN</td>
          <td>Deep Recursive Residual Network for image super-resolution</td>
          <td>Jul 2016</td>
          <td>National University of Singapore</td>
          <td><a href="https://arxiv.org/abs/1608.05148" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>39</td>
          <td>Multi-Channel</td>
          <td>Multi-Channel CNN for medical image analysis</td>
          <td>Jul 2016</td>
          <td>University of California, San Diego</td>
          <td><a href="https://arxiv.org/abs/2306.07365" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>40</td>
          <td>GCN</td>
          <td>Graph Convolutional Networks for processing graph-structured data</td>
          <td>Sep 2016</td>
          <td>University of Montreal</td>
          <td><a href="https://arxiv.org/abs/1609.02907" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>41</td>
          <td>M²FCN</td>
          <td>Multi-modal Fully Convolutional Networks for medical imaging</td>
          <td>Sep 2016</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Shen_Multi-Stage_Multi-Recursive-Input_Fully_ICCV_2017_paper.pdf" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-m%C2%B2fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>42</td>
          <td>Graph CNN</td>
          <td>Graph Convolutional Neural Networks</td>
          <td>Sep 2016</td>
          <td>University of Montreal</td>
          <td><a href="https://arxiv.org/abs/1609.02907" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>43</td>
          <td>Grad-CAM</td>
          <td>Gradient-weighted Class Activation Mapping</td>
          <td>Oct 2016</td>
          <td>Georgia Tech</td>
          <td><a href="https://arxiv.org/abs/1610.02391" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>44</td>
          <td>ResNeXt</td>
          <td>Aggregated Residual Transformations for Deep Neural Networks</td>
          <td>Nov 2016</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1611.05431" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>45</td>
          <td>DRN</td>
          <td>Dilated Residual Networks</td>
          <td>Nov 2016</td>
          <td>Princeton University</td>
          <td><a href="https://arxiv.org/abs/1705.09914" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>46</td>
          <td>RefineNet</td>
          <td>Multi-Path Refinement Networks for high-resolution semantic segmentation</td>
          <td>Nov 2016</td>
          <td>University of Adelaide</td>
          <td><a href="https://arxiv.org/abs/1611.06612" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>47</td>
          <td>FractalNet</td>
          <td>Ultra-Deep Neural Networks without Residuals</td>
          <td>Nov 2016</td>
          <td>University of Toronto</td>
          <td><a href="https://arxiv.org/abs/1605.07648" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>48</td>
          <td>SSD</td>
          <td>Single Shot MultiBox Detector for real-time object detection</td>
          <td>Dec 2016</td>
          <td>Google</td>
          <td><a href="https://arxiv.org/abs/1512.02325" target="_blank">Paper</a>, <a href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab/" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>49</td>
          <td>TDM</td>
          <td>Top-Down Modulation for object detection</td>
          <td>Dec 2016</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/1612.06851" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>50</td>
          <td>FPN</td>
          <td>Feature Pyramid Networks for object detection</td>
          <td>Dec 2016</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1612.03144" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>51</td>
          <td>VoxResNet</td>
          <td>Deep Voxelwise Residual Networks</td>
          <td>Dec 2016</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/1608.05895" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>52</td>
          <td>DSSD</td>
          <td>Deconvolutional Single Shot Detector</td>
          <td>Jan 2017</td>
          <td>UNC Chapel Hill</td>
          <td><a href="https://arxiv.org/abs/1701.06659" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>53</td>
          <td>PolyNet</td>
          <td>Better Vision with More Complex Paths</td>
          <td>Mar 2017</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1611.05725" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>54</td>
          <td>IGCNet</td>
          <td>Interleaved Group Convolutions</td>
          <td>Mar 2017</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/1707.02725" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>55</td>
          <td>DCN</td>
          <td>Deformable Convolutional Networks</td>
          <td>Mar 2017</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/1703.06211" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>56</td>
          <td>IDW-CNN</td>
          <td>Image Dependent Warping CNN</td>
          <td>Mar 2017</td>
          <td>Seoul National University</td>
          <td><a href="https://arxiv.org/abs/1704.02798" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>57</td>
          <td>FCIS</td>
          <td>Fully Convolutional Instance-aware Semantic Segmentation</td>
          <td>Mar 2017</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/1611.07709" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>58</td>
          <td>Residual Attention Network</td>
          <td>Attention mechanism for image classification</td>
          <td>Apr 2017</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/1704.06904" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>59</td>
          <td>ResNet-DUC-HDC</td>
          <td>Dense Upsampling Convolution and Hybrid Dilated Convolution</td>
          <td>Apr 2017</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/1702.08502" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>60</td>
          <td>MobileNet</td>
          <td>Focused on efficient models for mobile and embedded devices using depthwise separable convolutions.</td>
          <td>Apr-2017</td>
          <td>Google</td>
          <td></td>
      </tr>
      <tr>
          <td>61</td>
          <td>G-RMI</td>
          <td>Google&rsquo;s large scale object detection system</td>
          <td>Jun 2017</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/1706.02677" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>62</td>
          <td>GraphSAGE</td>
          <td>Inductive Representation Learning on Large Graphs</td>
          <td>Jun 2017</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/1706.02216" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>63</td>
          <td>DPN</td>
          <td>Dual Path Networks combining ResNet and DenseNet</td>
          <td>Jul 2017</td>
          <td>UCSD, Momenta</td>
          <td><a href="https://arxiv.org/abs/1707.01629" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>64</td>
          <td>ERFNet</td>
          <td>Efficient Residual Factorized ConvNet for real-time semantic segmentation</td>
          <td>Jul 2017</td>
          <td>Universidad de Alcalá</td>
          <td><a href="https://arxiv.org/abs/1707.07012" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>65</td>
          <td>Suggestive Annotation</td>
          <td>Active Learning for medical image segmentation</td>
          <td>Jul 2017</td>
          <td>ETH Zurich</td>
          <td><a href="https://arxiv.org/abs/2206.01014" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>66</td>
          <td>RetinaNet</td>
          <td>Focal Loss for Dense Object Detection</td>
          <td>Aug 2017</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1708.02002" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>67</td>
          <td>Hide-and-Seek</td>
          <td>Weakly-supervised object detection training strategy</td>
          <td>Aug 2017</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/1704.04232" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>68</td>
          <td>C3</td>
          <td>Cross-City Cascade for semantic segmentation</td>
          <td>Aug 2017</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/1708.08197" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>69</td>
          <td>U-net+Res-net</td>
          <td>Combined U-net and Residual Network for medical segmentation</td>
          <td>Aug 2017</td>
          <td>Technical University of Munich</td>
          <td><a href="https://arxiv.org/abs/1708.00183" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>70</td>
          <td>DenseVoxNet</td>
          <td>Dense Voxel Network for 3D medical image segmentation</td>
          <td>Sep 2017</td>
          <td>Chinese University of Hong Kong</td>
          <td><a href="https://sh-tsang.medium.com/review-densevoxnet-volumetric-brain-segmentation-biomedical-image-segmentation-9136bb6128dd" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>71</td>
          <td>Graph Attention Networks</td>
          <td>Self-attention for Graph Data</td>
          <td>Oct 2017</td>
          <td>Université de Montréal</td>
          <td><a href="https://arxiv.org/abs/1710.10903" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>72</td>
          <td>Light-Head R-CNN</td>
          <td>Light-weight object detection architecture</td>
          <td>Nov 2017</td>
          <td>Megvii Technology</td>
          <td><a href="https://arxiv.org/abs/1711.07264" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>73</td>
          <td>LayerCascade</td>
          <td>Instance segmentation via layer cascade</td>
          <td>Nov 2017</td>
          <td>University of Washington</td>
          <td><a href="https://arxiv.org/abs/1712.04837" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>74</td>
          <td>3D U-net + ResNet</td>
          <td>Combined 3D U-net and ResNet for volumetric segmentation</td>
          <td>Nov 2017</td>
          <td>Technical University of Munich</td>
          <td><a href="https://medium.com/towards-data-science/review-3d-u-net-resnet-volumetric-convolutions-long-short-residual-connections-biomedical-3a7da3f98dae" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>75</td>
          <td>Cascade R-CNN</td>
          <td>Multi-stage object detection refinement</td>
          <td>Dec 2017</td>
          <td>CIDSE</td>
          <td><a href="https://arxiv.org/abs/1712.00726" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>76</td>
          <td>StairNet</td>
          <td>Top-down semantic feature refinement</td>
          <td>Dec 2017</td>
          <td>Seoul National University</td>
          <td><a href="https://arxiv.org/abs/1711.07971" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>77</td>
          <td>MaskLab</td>
          <td>Instance Segmentation by Refining Object Detection</td>
          <td>Jan 2018</td>
          <td>Google</td>
          <td><a href="https://arxiv.org/abs/1712.04837" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>78</td>
          <td>RU-Net + R2U-Net</td>
          <td>Recurrent Residual U-Net variants</td>
          <td>Jan 2018</td>
          <td>University of Dhaka</td>
          <td><a href="https://arxiv.org/abs/1802.06955" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>79</td>
          <td>AmoebaNet</td>
          <td>Evolutionary Architecture Search</td>
          <td>Feb 2018</td>
          <td>Google Brain</td>
          <td><a href="https://arxiv.org/abs/1802.01548" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>80</td>
          <td>SqueezeNext</td>
          <td>Hardware-Aware Neural Network Design</td>
          <td>Feb 2018</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/1803.10615" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>81</td>
          <td>ENAS</td>
          <td>Efficient Neural Architecture Search</td>
          <td>Feb 2018</td>
          <td>Google Brain</td>
          <td><a href="https://arxiv.org/abs/1802.03268" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>82</td>
          <td>DeepLab v3+</td>
          <td>Encoder-Decoder with Atrous Separable Convolution</td>
          <td>Feb 2018</td>
          <td>Google</td>
          <td><a href="https://arxiv.org/abs/1802.02611" target="_blank">Paper</a>, <a href="https://medium.com/towards-data-science/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74" target="_blank">Blog</a></td>
      </tr>
      <tr>
          <td>83</td>
          <td>Group Normalization</td>
          <td>Alternative to Batch Normalization</td>
          <td>Mar 2018</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1803.08494" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>84</td>
          <td>ACoL</td>
          <td>Adversarial Complementary Learning for weakly supervised object localization</td>
          <td>Mar 2018</td>
          <td>University of Technology Sydney</td>
          <td><a href="https://arxiv.org/abs/1804.06962" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>85</td>
          <td>BR²Net</td>
          <td>Boundary Refinement and Recurrent Network for semantic segmentation</td>
          <td>Mar 2018</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/1803.10840" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>86</td>
          <td>PANet</td>
          <td>Path Aggregation Network for Instance Segmentation</td>
          <td>Mar 2018</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/1803.01534" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>87</td>
          <td>MorphNet</td>
          <td>Fast &amp; Simple Resource-Constrained Structure Learning</td>
          <td>Apr 2018</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/1801.00746" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>88</td>
          <td>ImageNet Rethinking</td>
          <td>Research on ImageNet training strategies</td>
          <td>Apr 2018</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1805.00932" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>89</td>
          <td>Attention U-net</td>
          <td>Attention Gates for Medical Image Segmentation</td>
          <td>Apr 2018</td>
          <td>University College London</td>
          <td><a href="https://arxiv.org/abs/1804.03999" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>90</td>
          <td>MegNet</td>
          <td>Multi-Evidence Guidance for weakly supervised object detection</td>
          <td>Jun 2018</td>
          <td>University of Technology Sydney</td>
          <td><a href="https://arxiv.org/abs/1806.04718" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>91</td>
          <td>H-DenseUNet</td>
          <td>Hybrid Densely Connected UNet for medical segmentation</td>
          <td>Jun 2018</td>
          <td>Chinese University of Hong Kong</td>
          <td><a href="https://arxiv.org/abs/1709.07330" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>92</td>
          <td>PNASNet</td>
          <td>Progressive Neural Architecture Search</td>
          <td>Jul 2018</td>
          <td>Google Brain</td>
          <td><a href="https://arxiv.org/abs/1712.00559" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>93</td>
          <td>ShuffleNetV2</td>
          <td>Practical Guidelines for Mobile Network Design</td>
          <td>Jul 2018</td>
          <td>Face++</td>
          <td><a href="https://arxiv.org/abs/1807.11164" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>94</td>
          <td>BAM</td>
          <td>Bottleneck Attention Module</td>
          <td>Jul 2018</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1807.06514" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>95</td>
          <td>CBAM</td>
          <td>Convolutional Block Attention Module</td>
          <td>Jul 2018</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1807.06521" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>96</td>
          <td>NetAdapt</td>
          <td>Platform-Aware Neural Network Adaptation</td>
          <td>Jul 2018</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/1804.03230" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>97</td>
          <td>U-Net++</td>
          <td>Nested U-Net Architecture</td>
          <td>Jul 2018</td>
          <td>Arizona State University</td>
          <td><a href="https://arxiv.org/abs/1807.10165" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>98</td>
          <td>DU-Net</td>
          <td>Deformable U-Net for medical image segmentation</td>
          <td>Aug 2018</td>
          <td>Shanghai Jiao Tong University</td>
          <td><a href="https://arxiv.org/abs/1808.10720" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>99</td>
          <td>DropBlock</td>
          <td>Structured dropout method for convolutional networks</td>
          <td>Oct 2018</td>
          <td>Google Brain</td>
          <td><a href="https://arxiv.org/abs/1810.12890" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>100</td>
          <td>AutoDeepLab</td>
          <td>Neural Architecture Search for Semantic Image Segmentation</td>
          <td>Jan 2019</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/1901.02985" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>101</td>
          <td>ESPNetv2</td>
          <td>Efficient Spatial Pyramid of Dilated Convolutions</td>
          <td>Mar 2019</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/1811.11431" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>102</td>
          <td>SiamRPN++</td>
          <td>Deep learning-based visual tracking framework that removes spatial awareness by sampling features across different layers</td>
          <td>Mar 2019</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/1903.06015" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>103</td>
          <td>Libra R-CNN</td>
          <td>Balanced learning framework for object detection that addresses sample level, feature level, and objective level imbalance</td>
          <td>Apr 2019</td>
          <td>SenseTime Research</td>
          <td><a href="https://arxiv.org/abs/1904.02701" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>104</td>
          <td>FBNet</td>
          <td>Hardware-Aware Efficient ConvNet Design</td>
          <td>May 2019</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1812.03443" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>105</td>
          <td>SDN</td>
          <td>Selective Deep Network for efficient visual recognition</td>
          <td>May 2019</td>
          <td>University of Texas</td>
          <td><a href="https://arxiv.org/abs/1905.04168" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>106</td>
          <td>MultiResUNet</td>
          <td>Multi-Resolution U-Net for medical image segmentation</td>
          <td>May 2019</td>
          <td>Bangladesh University</td>
          <td><a href="https://arxiv.org/abs/1902.04049" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>107</td>
          <td>EfficientNet</td>
          <td>Scaled networks uniformly in depth, width, and resolution for better efficiency.</td>
          <td>May-2019</td>
          <td>Google</td>
          <td></td>
      </tr>
      <tr>
          <td>108</td>
          <td>ADL</td>
          <td>Attention-based Dropout Layer for weakly supervised object localization</td>
          <td>Jun 2019</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1908.10028" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>109</td>
          <td>ARMA Convolution</td>
          <td>Auto-Regressive Moving Average Graph Filtering</td>
          <td>Jun 2019</td>
          <td>Università degli Studi di Modena</td>
          <td><a href="https://arxiv.org/abs/1901.01343" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>110</td>
          <td>Panoptic Segmentation</td>
          <td>Unified Scene Parsing Framework</td>
          <td>Jun 2019</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1801.00868" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>111</td>
          <td>CutMix</td>
          <td>Data augmentation method combining cut and mix images</td>
          <td>Aug 2019</td>
          <td>Clova AI Research, NAVER</td>
          <td><a href="https://arxiv.org/abs/1905.04899" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>112</td>
          <td>SlowFast</td>
          <td>Two-pathway network for video recognition that captures both slow and fast motion patterns</td>
          <td>Aug 2019</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/1812.03982" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>113</td>
          <td>EfficientDet</td>
          <td>Scalable object detection architecture using weighted bidirectional feature network and compound scaling</td>
          <td>Nov 2019</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/1911.09070" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>114</td>
          <td>AdderNet</td>
          <td>Neural Networks with Only Addition Operations</td>
          <td>Dec 2019</td>
          <td>Huawei Noah&rsquo;s Ark Lab</td>
          <td><a href="https://arxiv.org/abs/1912.13200" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>115</td>
          <td>TPN</td>
          <td>Temporal Pyramid Network for action detection in videos</td>
          <td>Dec 2019</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/1912.08848" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>116</td>
          <td>ATSS</td>
          <td>Adaptive Training Sample Selection for object detection</td>
          <td>Dec 2019</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/1912.02424" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>117</td>
          <td>ACNe</td>
          <td>Attentive Context Normalization for robust permutation-equivariant learning</td>
          <td>Dec 2019</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/1907.02545" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>118</td>
          <td>Cascade Cost Volume</td>
          <td>Cascade Cost Volume for stereo matching</td>
          <td>Dec 2019</td>
          <td>Megvii Technology</td>
          <td><a href="https://arxiv.org/abs/1912.06378" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>119</td>
          <td>Yolact++</td>
          <td>Real-time instance segmentation with improved mask quality and inference speed</td>
          <td>Jan 2020</td>
          <td>University of California, Davis</td>
          <td><a href="https://arxiv.org/abs/1912.06218" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>120</td>
          <td>MCN</td>
          <td>Multi-task Collaboration Network</td>
          <td>Jan 2020</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2001.05509" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>121</td>
          <td>RandLA-Net</td>
          <td>Large-scale Point Cloud Semantic Segmentation</td>
          <td>Jan 2020</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/1911.11236" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>122</td>
          <td>OccuSeg</td>
          <td>3D instance segmentation approach that handles occlusions in point clouds</td>
          <td>Mar 2020</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2003.06537" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>123</td>
          <td>GTAD</td>
          <td>Global Temporal Action Detection framework for temporal action localization</td>
          <td>Mar 2020</td>
          <td>Sun Yat-sen University</td>
          <td><a href="https://arxiv.org/abs/2003.12444" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>124</td>
          <td>Attention-RPN</td>
          <td>Visual tracking framework with attention mechanism in Region Proposal Network</td>
          <td>Mar 2020</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2003.12125" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>125</td>
          <td>QSA + QNT</td>
          <td>Quantized Squeeze-and-Attention Networks</td>
          <td>Mar 2020</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2003.13630" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>126</td>
          <td>UNet 3+</td>
          <td>Full-Scale Connected UNet for medical image segmentation</td>
          <td>Mar 2020</td>
          <td>Southern Medical University</td>
          <td><a href="https://arxiv.org/abs/2004.08790" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>127</td>
          <td>ROAM</td>
          <td>Recurrently Optimizing Tracking Model</td>
          <td>Mar 2020</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2002.10218" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>128</td>
          <td>PF-NET</td>
          <td>Point Fractal Network for 3D point cloud completion</td>
          <td>Mar 2020</td>
          <td>Simon Fraser University</td>
          <td><a href="https://arxiv.org/abs/2003.00410" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>129</td>
          <td>Total3DUnderstanding</td>
          <td>3D Scene Understanding</td>
          <td>Mar 2020</td>
          <td>National University of Singapore</td>
          <td><a href="https://arxiv.org/abs/2002.12212" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>130</td>
          <td>SG-NN</td>
          <td>Scene Graph Neural Networks</td>
          <td>Mar 2020</td>
          <td>Georgia Tech</td>
          <td><a href="https://arxiv.org/abs/2001.03111" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>131</td>
          <td>SEAN</td>
          <td>Semantic Region-Adaptive Normalization</td>
          <td>Mar 2020</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2004.03458" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>132</td>
          <td>SAOL</td>
          <td>Self-Attention Object Localization</td>
          <td>Apr 2020</td>
          <td>Seoul National University</td>
          <td><a href="https://arxiv.org/abs/2004.00567" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>133</td>
          <td>VGGNet For Covid19</td>
          <td>Modified VGG architecture for COVID-19 detection</td>
          <td>Apr 2020</td>
          <td>Multiple Institutions</td>
          <td><a href="https://arxiv.org/abs/2004.09959" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>134</td>
          <td>CentripetalNet</td>
          <td>Anchor-free object detection with point-based prediction</td>
          <td>Apr 2020</td>
          <td>Megvii Technology</td>
          <td><a href="https://arxiv.org/abs/2003.09119" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>135</td>
          <td>PointAugment</td>
          <td>Auto-Augmentation for 3D Point Cloud</td>
          <td>Apr 2020</td>
          <td>National University of Singapore</td>
          <td><a href="https://arxiv.org/abs/2002.10876" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>136</td>
          <td>PQ-Net</td>
          <td>Learning to Generate 3D Shapes</td>
          <td>Apr 2020</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2002.02288" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>137</td>
          <td>Axial-DeepLab</td>
          <td>Stand-Alone Axial-Attention for Vision Models</td>
          <td>Apr 2020</td>
          <td>Johns Hopkins University</td>
          <td><a href="https://arxiv.org/abs/2003.07853" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>138</td>
          <td>SipMask</td>
          <td>Spatial Information Preservation for Fast Instance Segmentation</td>
          <td>Apr 2020</td>
          <td>Inception Institute of AI</td>
          <td><a href="https://arxiv.org/abs/2007.14772" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>139</td>
          <td>SCAN</td>
          <td>Learning to Classify Images without Labels</td>
          <td>Apr 2020</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2005.12320" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>140</td>
          <td>MutualNet</td>
          <td>Adaptive ConvNet via Mutual Learning</td>
          <td>Apr 2020</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2004.05159" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>141</td>
          <td>DETR</td>
          <td>End-to-End Object Detection with Transformers</td>
          <td>May 2020</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2005.12872" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>142</td>
          <td>C-Flow</td>
          <td>Conditional Normalizing Flows</td>
          <td>May 2020</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2006.13784" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>143</td>
          <td>PerfectShape</td>
          <td>Shape completion using implicit functions</td>
          <td>May 2020</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2004.10178" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>144</td>
          <td>UFO²</td>
          <td>Unified Framework for Object Detection</td>
          <td>May 2020</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2002.11971" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>145</td>
          <td>Refinement Network</td>
          <td>RGB-D Scene Understanding</td>
          <td>May 2020</td>
          <td>Technical University Munich</td>
          <td><a href="https://arxiv.org/abs/2004.07437" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>146</td>
          <td>AssembleNet++</td>
          <td>Video Recognition with Learnable Connectivity</td>
          <td>May 2020</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2008.08072" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>147</td>
          <td>WeightNet</td>
          <td>Revisiting Weight Networks</td>
          <td>May 2020</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2007.11823" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>148</td>
          <td>YOLOv5</td>
          <td>Improved version of YOLO with better speed-accuracy trade-off</td>
          <td>Jun 2020</td>
          <td>Ultralytics</td>
          <td><a href="https://github.com/ultralytics/yolov5" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>149</td>
          <td>UCTGAN</td>
          <td>Unsupervised Cartoon-to-Real Translation GAN for image translation between cartoon and real-world domains</td>
          <td>Jun 2020</td>
          <td>Nanyang Technological University</td>
          <td><a href="https://arxiv.org/abs/2006.11479" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>150</td>
          <td>IF-Nets</td>
          <td>Implicit Function Neural Networks for 3D reconstruction</td>
          <td>Jun 2020</td>
          <td>Max Planck Institute</td>
          <td><a href="https://arxiv.org/abs/2003.01456" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>151</td>
          <td>SketchGCN</td>
          <td>Sketch Recognition using Graph Convolutional Networks</td>
          <td>Jun 2020</td>
          <td>University of British Columbia</td>
          <td><a href="https://arxiv.org/abs/2003.00678" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>152</td>
          <td>AABO</td>
          <td>Adaptive Anchor Box Optimization</td>
          <td>Jun 2020</td>
          <td>Huawei Noah&rsquo;s Ark Lab</td>
          <td><a href="https://arxiv.org/abs/2006.15221" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>153</td>
          <td>Polka Lines</td>
          <td>Line Detection using Polar Coordinates</td>
          <td>Jun 2020</td>
          <td>Korea University</td>
          <td><a href="https://arxiv.org/abs/2006.02836" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>154</td>
          <td>Pose2Mesh</td>
          <td>3D Human Pose and Mesh Recovery</td>
          <td>Jun 2020</td>
          <td>Korea University</td>
          <td><a href="https://arxiv.org/abs/2008.09191" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>155</td>
          <td>SNE-RoadSeg</td>
          <td>Road Segmentation with Synthetic Data</td>
          <td>Jun 2020</td>
          <td>Hong Kong University</td>
          <td><a href="https://arxiv.org/abs/2008.11351" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>156</td>
          <td>Deep Hough Transform</td>
          <td>Line Detection using Deep Learning</td>
          <td>Jun 2020</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2007.09493" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>157</td>
          <td>Non-Local Sparse Attention</td>
          <td>Efficient Attention Mechanism</td>
          <td>Jun 2020</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2007.04746" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>158</td>
          <td>Hit-Detector</td>
          <td>Hierarchical Trinity architecture for object detection combining different detection paradigms</td>
          <td>Jul 2020</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2007.10739" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>159</td>
          <td>Spectral 3D Computer Vision</td>
          <td>Graph Neural Network Library</td>
          <td>Jul 2020</td>
          <td>Multiple Contributors</td>
          <td><a href="https://arxiv.org/abs/2302.08054" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>160</td>
          <td>TIDE</td>
          <td>Error Analysis Tool for Object Detection</td>
          <td>Jul 2020</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2008.08115" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>161</td>
          <td>SimAug</td>
          <td>Learning Robust Representations through Simulation</td>
          <td>Jul 2020</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2007.08558" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>162</td>
          <td>HOTR</td>
          <td>End-to-End Human-Object Interaction Detection</td>
          <td>Jul 2020</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/2008.05407" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>163</td>
          <td>ReXNet</td>
          <td>Rethinking Channel Dimensions for Efficient Model Design</td>
          <td>Jul 2020</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2007.00992" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>164</td>
          <td>Keep Eyes on the Lane</td>
          <td>Lane Detection with Deep Learning</td>
          <td>Jul 2020</td>
          <td>Shanghai Jiao Tong University</td>
          <td><a href="https://arxiv.org/abs/2004.11011" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>165</td>
          <td>AdvPC</td>
          <td>Adversarial Point Cloud Defense</td>
          <td>Jul 2020</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2007.07191" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>166</td>
          <td>PD-GAN</td>
          <td>Probabilistic Diverse GAN</td>
          <td>Jul 2020</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/2008.12062" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>167</td>
          <td>FedDG</td>
          <td>Federated Domain Generalization</td>
          <td>Jul 2020</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2008.06223" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>168</td>
          <td>Dynamic RCNN</td>
          <td>Dynamic R-CNN for object detection with improved training and inference</td>
          <td>Aug 2020</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2004.06002" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>169</td>
          <td>Aug-FPN</td>
          <td>Augmented Feature Pyramid Network for object detection with improved multi-scale feature fusion</td>
          <td>Aug 2020</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2008.05159" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>170</td>
          <td>Instant-teaching</td>
          <td>Self-training for Object Detection</td>
          <td>Aug 2020</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2008.02934" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>171</td>
          <td>Soft-IntroVAE</td>
          <td>Soft Introduction of Variational AutoEncoders</td>
          <td>Aug 2020</td>
          <td>Tel Aviv University</td>
          <td><a href="https://arxiv.org/abs/2008.08017" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>172</td>
          <td>DiNTS</td>
          <td>Differentiable Neural Network Transform Search</td>
          <td>Aug 2020</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2008.00256" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>173</td>
          <td>Eagle Eye</td>
          <td>Fast Sub-net Evaluation for Efficient Neural Network Training</td>
          <td>Aug 2020</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2007.02491" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>174</td>
          <td>StyleMapGAN</td>
          <td>Exploiting Spatial Dimensions of Latent for Image Manipulation</td>
          <td>Aug 2020</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/2008.02401" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>175</td>
          <td>TediGAN</td>
          <td>Text-Guided Diverse Image Generation</td>
          <td>Aug 2020</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2008.12542" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>176</td>
          <td>Auto-Exposure Fusion</td>
          <td>Automatic Exposure Fusion for Photography</td>
          <td>Aug 2020</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2008.05073" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>177</td>
          <td>Vision Transformer</td>
          <td>Transformer architecture adapted for image recognition tasks</td>
          <td>Sep 2020</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2010.11929" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>178</td>
          <td>IDU</td>
          <td>Instance Depth Embedding for RGB-D salient object detection</td>
          <td>Sep 2020</td>
          <td>Nankai University</td>
          <td><a href="https://arxiv.org/abs/2009.05724" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>179</td>
          <td>VideoMoCo</td>
          <td>Contrastive Learning for Video Understanding</td>
          <td>Sep 2020</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2020.11883" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>180</td>
          <td>MZSR</td>
          <td>Meta-Transfer Learning for Zero-Shot Super-Resolution</td>
          <td>Nov 2020</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/2002.12213" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>181</td>
          <td>DeiT</td>
          <td>Data-efficient training of image transformers</td>
          <td>Dec 2020</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2012.12877" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>182</td>
          <td>Involution</td>
          <td>Inverting Convolution for Visual Recognition</td>
          <td>Dec 2020</td>
          <td>Shanghai AI Lab</td>
          <td><a href="https://arxiv.org/abs/2103.06255" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>183</td>
          <td>Deep Learning on Semantic Segmentation</td>
          <td>Comprehensive Survey and Benchmark</td>
          <td>Dec 2020</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2012.14355" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>184</td>
          <td>LiteFlowNet3</td>
          <td>Lightweight Optical Flow Estimation</td>
          <td>Dec 2020</td>
          <td>Chinese University of Hong Kong</td>
          <td><a href="https://arxiv.org/abs/2007.09319" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>185</td>
          <td>PPDM</td>
          <td>Parallel Point Detection and Matching</td>
          <td>Dec 2020</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2012.13601" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>186</td>
          <td>RepVGG</td>
          <td>Making VGG-style ConvNets Great Again</td>
          <td>Jan 2021</td>
          <td>MEGVII Technology</td>
          <td><a href="https://arxiv.org/abs/2101.03697" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>187</td>
          <td>PSConvolution</td>
          <td>Parameter-Sharing Convolution for Deep Learning</td>
          <td>Jan 2021</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2101.05053" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>188</td>
          <td>PerPixel Classification</td>
          <td>Pixel-wise Classification Network</td>
          <td>Jan 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2101.06175" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>189</td>
          <td>PIPAL</td>
          <td>Perceptual Image Quality Assessment</td>
          <td>Jan 2021</td>
          <td>Nanyang Technological University</td>
          <td><a href="https://arxiv.org/abs/2101.04362" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>190</td>
          <td>ArtGAN</td>
          <td>Artwork Synthesis with GAN</td>
          <td>Feb 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2102.12593" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>191</td>
          <td>Synthetic to Real</td>
          <td>Domain Adaptation for Semantic Segmentation</td>
          <td>Feb 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2103.15309" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>192</td>
          <td>Spatial-Phase-Shallow-Learning</td>
          <td>Phase-Based Feature Learning</td>
          <td>Feb 2021</td>
          <td>Peking University</td>
          <td><a href="https://arxiv.org/abs/2102.12290" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>193</td>
          <td>DARKGAN</td>
          <td>Dark Image Enhancement with GAN</td>
          <td>Feb 2021</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2102.03808" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>194</td>
          <td>Deep Imbalance Regression</td>
          <td>Learning from Imbalanced Data</td>
          <td>Feb 2021</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2102.09554" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>195</td>
          <td>Room Classification GNN</td>
          <td>Graph Neural Network for Room Layout</td>
          <td>Feb 2021</td>
          <td>Facebook Research</td>
          <td><a href="https://arxiv.org/abs/2102.12100" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>196</td>
          <td>Pyramid Vision Transformer</td>
          <td>Hierarchical Vision Transformer</td>
          <td>Feb 2021</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/2102.12122" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>197</td>
          <td>Residual Attention</td>
          <td>Attention Mechanism for CNNs</td>
          <td>Feb 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2102.12594" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>198</td>
          <td>Teachers do more than teach</td>
          <td>Multi-teacher approach for image-to-image translation</td>
          <td>Mar 2021</td>
          <td>Tel Aviv University</td>
          <td><a href="https://arxiv.org/abs/2103.17080" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>199</td>
          <td>Vip-DeepLab</td>
          <td>Visual Parsing DeepLab for Panoptic Segmentation</td>
          <td>Mar 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2012.15487" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>200</td>
          <td>HistoGAN</td>
          <td>Histological Image Generation with GAN</td>
          <td>Mar 2021</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/2103.10018" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>201</td>
          <td>Anchor-Free Person Search</td>
          <td>End-to-End Person Search without Anchors</td>
          <td>Mar 2021</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2103.07581" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>202</td>
          <td>CBNetV2</td>
          <td>Composite Backbone Network</td>
          <td>Mar 2021</td>
          <td>Megvii Technology</td>
          <td><a href="https://arxiv.org/abs/2107.00420" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>203</td>
          <td>Kaleido-BERT</td>
          <td>Vision-Language Pre-training</td>
          <td>Mar 2021</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2103.16110" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>204</td>
          <td>Elastic Graph Neural Network</td>
          <td>Adaptive Graph Structure Learning</td>
          <td>Mar 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2103.04131" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>205</td>
          <td>Rank and Sort Loss</td>
          <td>Loss Function for Object Detection</td>
          <td>Mar 2021</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2103.16709" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>206</td>
          <td>EigenGAN</td>
          <td>Eigenvalue-Based GAN Architecture</td>
          <td>Mar 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2104.12476" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>207</td>
          <td>DetCo</td>
          <td>Unsupervised Detection Pre-training</td>
          <td>Mar 2021</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2102.04803" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>208</td>
          <td>MG-GAN</td>
          <td>Multi-Generator GAN</td>
          <td>Mar 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2103.15852" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>209</td>
          <td>AdaAttN</td>
          <td>Adaptive Attention for Style Transfer</td>
          <td>Mar 2021</td>
          <td>Microsoft Research Asia</td>
          <td><a href="https://arxiv.org/abs/2103.17185" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>210</td>
          <td>AirBERT</td>
          <td>Vision-Language Model for Aerial Images</td>
          <td>Mar 2021</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2103.16851" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>211</td>
          <td>DeepGCNs</td>
          <td>Deep Graph Convolutional Networks</td>
          <td>Mar 2021</td>
          <td>KAUST</td>
          <td><a href="https://arxiv.org/abs/1910.06849" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>212</td>
          <td>Survey: Instance Segmentation</td>
          <td>Comprehensive review of instance segmentation methods</td>
          <td>Mar 2021</td>
          <td>Multiple Institutions</td>
          <td><a href="https://arxiv.org/abs/2103.15714" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>213</td>
          <td>LoFTR</td>
          <td>Local Feature TRansformer for establishing dense correspondences between images</td>
          <td>Apr 2021</td>
          <td>Zhejiang University</td>
          <td><a href="https://arxiv.org/abs/2104.00680" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>214</td>
          <td>Semantic Image Matting</td>
          <td>Matting with Semantic Guidance</td>
          <td>Apr 2021</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2104.08201" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>215</td>
          <td>EfficientNetV2</td>
          <td>Improved EfficientNet Architecture</td>
          <td>Apr 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2104.00298" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>216</td>
          <td>Closed-Loop Matters</td>
          <td>Dual Regression for Image Generation</td>
          <td>Apr 2021</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/2104.05610" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>217</td>
          <td>Mobile-Former</td>
          <td>Mobile-Friendly Transformer</td>
          <td>Apr 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2104.02861" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>218</td>
          <td>GNeRF</td>
          <td>Generalizable Neural Radiance Fields</td>
          <td>Apr 2021</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2103.15074" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>219</td>
          <td>DETR with Modulated Co-Attention</td>
          <td>Enhanced DETR Architecture</td>
          <td>Apr 2021</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2104.12099" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>220</td>
          <td>Adaptable GAN Encoders</td>
          <td>Flexible GAN Inversion</td>
          <td>Apr 2021</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2104.05137" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>221</td>
          <td>Conformer</td>
          <td>Local Features Meet Global Dependencies</td>
          <td>Apr 2021</td>
          <td>Shanghai AI Lab</td>
          <td><a href="https://arxiv.org/abs/2105.03889" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>222</td>
          <td>VMNet</td>
          <td>Visual Manipulation Networks</td>
          <td>Apr 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2104.14620" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>223</td>
          <td>Battle of Network Structure</td>
          <td>Network Architecture Comparison Study</td>
          <td>Apr 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2104.13636" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>224</td>
          <td>Efficient Person Search</td>
          <td>Fast Person Search Framework</td>
          <td>Apr 2021</td>
          <td>University of Technology Sydney</td>
          <td><a href="https://arxiv.org/abs/2104.01242" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>225</td>
          <td>SLIDE</td>
          <td>Smart Learning on Large-Scale Data</td>
          <td>Apr 2021</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2104.14590" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>226</td>
          <td>SOTR</td>
          <td>Transformer for Set Operations</td>
          <td>Apr 2021</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2104.14610" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>227</td>
          <td>CANet</td>
          <td>Class-Agnostic Segmentation Networks</td>
          <td>Apr 2021</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2104.08797" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>228</td>
          <td>YOLOP</td>
          <td>Real-time Driving Perception</td>
          <td>May 2021</td>
          <td>Huawei Noah&rsquo;s Ark Lab</td>
          <td><a href="https://arxiv.org/abs/2108.11250" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>229</td>
          <td>InSeGAN</td>
          <td>Interactive Segmentation with GAN</td>
          <td>May 2021</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2105.06456" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>230</td>
          <td>GroupFormer</td>
          <td>Group-Based Attention</td>
          <td>May 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2105.08055" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>231</td>
          <td>Super Neuron</td>
          <td>Neural Architecture Enhancement</td>
          <td>May 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2105.08954" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>232</td>
          <td>SO-Pose</td>
          <td>Self-Occlusion Aware Pose Estimation</td>
          <td>May 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2105.12439" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>233</td>
          <td>TxT</td>
          <td>Text-driven Text Generation</td>
          <td>May 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2105.03458" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>234</td>
          <td>OS2D</td>
          <td>One-Stage 2D Object Detection</td>
          <td>May 2021</td>
          <td>Yandex Research</td>
          <td><a href="https://arxiv.org/abs/2105.12139" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>235</td>
          <td>CodeNet</td>
          <td>Large-Scale Code Dataset</td>
          <td>May 2021</td>
          <td>IBM Research</td>
          <td><a href="https://arxiv.org/abs/2105.12655" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>236</td>
          <td>Geometric Deep Learning</td>
          <td>Blueprint for designing architectures for geometric data</td>
          <td>May 2021</td>
          <td>Imperial College London</td>
          <td><a href="https://arxiv.org/abs/2104.13478" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>237</td>
          <td>Oriented R-CNN</td>
          <td>Oriented Object Detection</td>
          <td>Jun 2021</td>
          <td>Tongji University</td>
          <td><a href="https://arxiv.org/abs/2108.05699" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>238</td>
          <td>XVFI</td>
          <td>Video Frame Interpolation</td>
          <td>Jun 2021</td>
          <td>KAIST</td>
          <td><a href="https://arxiv.org/abs/2106.05965" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>239</td>
          <td>Cross Domain Contrastive Learning</td>
          <td>Domain Adaptation via Contrastive Learning</td>
          <td>Jun 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2106.05528" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>240</td>
          <td>PointManifoldCut</td>
          <td>Data Augmentation for Point Clouds</td>
          <td>Jun 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2106.09337" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>241</td>
          <td>Distance IOU Loss</td>
          <td>Improved Loss Function for Object Detection</td>
          <td>Jun 2021</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2106.14425" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>242</td>
          <td>ConvMLP</td>
          <td>Convolutional MLP Architecture</td>
          <td>Jul 2021</td>
          <td>University of Oregon</td>
          <td><a href="https://arxiv.org/abs/2107.08391" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>243</td>
          <td>Graph-FPN</td>
          <td>Feature Pyramid Networks with Graph Neural Networks</td>
          <td>Jul 2021</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2107.14170" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>244</td>
          <td>WatchOut!</td>
          <td>Motion Blur Impact on DNNs</td>
          <td>Jul 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2107.14170" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>245</td>
          <td>ECA-Net</td>
          <td>Efficient Channel Attention Network</td>
          <td>Jul 2021</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2107.12231" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>246</td>
          <td>ShiftAddNet</td>
          <td>Efficient Neural Network Training</td>
          <td>Aug 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2108.02057" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>247</td>
          <td>Deep Imitation Learning</td>
          <td>Survey of Imitation Learning Methods</td>
          <td>Aug 2021</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2108.04548" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>248</td>
          <td>3DETR</td>
          <td>3D Object Detection with Transformers</td>
          <td>Aug 2021</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2108.08931" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>249</td>
          <td>ByteTrack</td>
          <td>Multi-Object Tracking Framework</td>
          <td>Aug 2021</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2110.06864" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>250</td>
          <td>Neuron Merging</td>
          <td>Network Compression via Neuron Merging</td>
          <td>Sep 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2109.06872" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>251</td>
          <td>Focal Transformer</td>
          <td>Vision Transformer with Focal Attention</td>
          <td>Sep 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2109.14824" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>252</td>
          <td>Non-Deep Networks</td>
          <td>Alternative to Deep Neural Networks</td>
          <td>Sep 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2109.11278" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>253</td>
          <td>PytorchVideo</td>
          <td>Deep Learning Library for Video Understanding</td>
          <td>Sep 2021</td>
          <td>Facebook AI Research</td>
          <td><a href="https://dl.acm.org/doi/10.1145/3474085.3478329" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>254</td>
          <td>HeadGAN</td>
          <td>Head Generation and Editing</td>
          <td>Oct 2021</td>
          <td>Tel Aviv University</td>
          <td><a href="https://arxiv.org/abs/2110.11248" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>255</td>
          <td>StyleGAN3</td>
          <td>Alias-Free Generative Network</td>
          <td>Oct 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2110.11985" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>256</td>
          <td>MedMNIST</td>
          <td>Medical Image Dataset Collection</td>
          <td>Oct 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2110.14795" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>257</td>
          <td>TokenLearner</td>
          <td>Dynamic Token Selection in Vision Transformers</td>
          <td>Oct 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2110.09552" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>258</td>
          <td>Temporal Fusion Transformer</td>
          <td>Multi-horizon Forecasting</td>
          <td>Oct 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/1912.09363" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>259</td>
          <td>NeuralProphet</td>
          <td>Neural Network based Time-Series Model</td>
          <td>Oct 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2111.15397" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>260</td>
          <td>MetNet-2</td>
          <td>Weather Forecasting Model</td>
          <td>Oct 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2111.07470" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>261</td>
          <td>Plan-then-generate</td>
          <td>Controlled Text Generation</td>
          <td>Nov 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2111.11067" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>262</td>
          <td>ProjectedGAN</td>
          <td>Improved GAN Image Quality</td>
          <td>Nov 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2111.14213" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>263</td>
          <td>PHALP</td>
          <td>Pose and Human Analysis using Language Processing</td>
          <td>Nov 2021</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2112.09454" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>264</td>
          <td>Semantic Diffusion Guidance</td>
          <td>Controlled Image Generation</td>
          <td>Nov 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2112.02745" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>265</td>
          <td>GauGAN</td>
          <td>Text-to-Image Generation</td>
          <td>Nov 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://medium.com/analytics-vidhya/gaugan-generating-photorealistic-images-from-drawings-7eac091f6c7f" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>266</td>
          <td>NeatNet</td>
          <td>Neural Architecture Evolution</td>
          <td>Nov 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2111.11674" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>267</td>
          <td>DenseULearn</td>
          <td>Dense Prediction with Uncertainty</td>
          <td>Nov 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2111.14343" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>268</td>
          <td>StyleNeRF</td>
          <td>Neural Radiance Fields with Style-based Generation</td>
          <td>Dec 2021</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2112.08867" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>269</td>
          <td>Colossal-AI</td>
          <td>Large-Scale Parallel Training System</td>
          <td>Dec 2021</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2112.15594" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>270</td>
          <td>EditGAN</td>
          <td>Semantic Image Editing with GANs</td>
          <td>Dec 2021</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2112.14668" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>271</td>
          <td>PoolFormer</td>
          <td>Alternative to Attention-based Transformers</td>
          <td>Dec 2021</td>
          <td>Sea AI Lab</td>
          <td><a href="https://arxiv.org/abs/2111.11418" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>272</td>
          <td>GLIP</td>
          <td>Grounded Language-Image Pre-training</td>
          <td>Dec 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2112.03857" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>273</td>
          <td>PixMix</td>
          <td>Data Augmentation Strategy</td>
          <td>Dec 2021</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2112.05135" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>274</td>
          <td>GANgealing</td>
          <td>GAN-based Image Alignment</td>
          <td>Dec 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2112.05143" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>275</td>
          <td>HiClass</td>
          <td>Hierarchical Classification Metrics</td>
          <td>Dec 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2112.00838" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>276</td>
          <td>MetaFormer</td>
          <td>General Architecture for Vision</td>
          <td>Dec 2021</td>
          <td>Sea AI Lab</td>
          <td><a href="https://arxiv.org/abs/2111.11418" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>277</td>
          <td>SAVi</td>
          <td>Slot Attention for Video Understanding</td>
          <td>Dec 2021</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2112.09454" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>278</td>
          <td>PARP</td>
          <td>Parameter Reduction Technique</td>
          <td>Dec 2021</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2112.14674" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>279</td>
          <td>TransMix</td>
          <td>Data Augmentation for Transformers</td>
          <td>Dec 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2112.15535" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>280</td>
          <td>Stable Long Term Video SR</td>
          <td>Long-term Video Super Resolution</td>
          <td>Dec 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2112.09715" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>281</td>
          <td>Few-Shot Learner</td>
          <td>Few-Shot Learning Framework</td>
          <td>Dec 2021</td>
          <td>Meta AI Research</td>
          <td><a href="https://arxiv.org/abs/2112.09763" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>282</td>
          <td>StyleSwin</td>
          <td>StyleGAN with Swin Transformer</td>
          <td>Dec 2021</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2112.10762" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>283</td>
          <td>2 Stage U-net</td>
          <td>Two-Stage Medical Image Segmentation</td>
          <td>Dec 2021</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2112.11673" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>284</td>
          <td>ELSA</td>
          <td>Efficient Long-term Semantic Aggregation</td>
          <td>Dec 2021</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2112.15594" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>285</td>
          <td>GLIDE</td>
          <td>Text-Guided Image Generation</td>
          <td>Dec 2021</td>
          <td>OpenAI</td>
          <td><a href="https://arxiv.org/abs/2112.10741" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>286</td>
          <td>AdaViT</td>
          <td>Adaptive Vision Transformers</td>
          <td>Jan 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2201.04288" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>287</td>
          <td>Exemplar Transformers</td>
          <td>Example-based Vision Transformers</td>
          <td>Jan 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2201.06679" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>288</td>
          <td>RepMLNet</td>
          <td>Reprogrammable Multi-Layer Network</td>
          <td>Jan 2022</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2201.08442" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>289</td>
          <td>Untrained Deep NN</td>
          <td>Deep Networks without Training</td>
          <td>Jan 2022</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2201.09192" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>290</td>
          <td>JoJoGAN</td>
          <td>Just one Joint Training GAN</td>
          <td>Jan 2022</td>
          <td>National University of Singapore</td>
          <td><a href="https://arxiv.org/abs/2112.11641" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>291</td>
          <td>PRIME</td>
          <td>Pre-trained Image Encoders</td>
          <td>Jan 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2201.09792" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>292</td>
          <td>StyleGAN-V</td>
          <td>Video Generation with StyleGAN</td>
          <td>Jan 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2201.13038" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>293</td>
          <td>SmoothNet</td>
          <td>Motion Smoothing Network</td>
          <td>Jan 2022</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2201.11209" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>294</td>
          <td>PCACE</td>
          <td>Point Cloud Auto-Encoder</td>
          <td>Jan 2022</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2201.02663" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>295</td>
          <td>Siamese CD</td>
          <td>Change Detection with Transformers</td>
          <td>Jan 2022</td>
          <td>Wuhan University</td>
          <td><a href="https://arxiv.org/abs/2201.01293" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>296</td>
          <td>SASA</td>
          <td>Self-Attention Spatial Adaptivity</td>
          <td>Jan 2022</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2201.10801" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>297</td>
          <td>GCD</td>
          <td>Generalized Category Discovery</td>
          <td>Jan 2022</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/2201.02609" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>298</td>
          <td>3D ConvNet Optimization</td>
          <td>Optimization Planning for 3D CNNs</td>
          <td>Jan 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2201.09100" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>299</td>
          <td>SeamlessGAN</td>
          <td>Seamless Image Generation</td>
          <td>Jan 2022</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2201.13395" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>300</td>
          <td>HardBoost</td>
          <td>Hard Example Mining with Boosting</td>
          <td>Jan 2022</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2201.11924" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>301</td>
          <td>Q-ViT</td>
          <td>Quantized Vision Transformer</td>
          <td>Jan 2022</td>
          <td>Meta AI Research</td>
          <td><a href="https://arxiv.org/abs/2201.07703" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>302</td>
          <td>GeoFill</td>
          <td>Geometry-aware Image Inpainting</td>
          <td>Jan 2022</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2201.09858" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>303</td>
          <td>Detic</td>
          <td>Detector with Image Classes</td>
          <td>Jan 2022</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2201.02605" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>304</td>
          <td>RelTR</td>
          <td>Relational Transformer</td>
          <td>Jan 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2201.02860" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>305</td>
          <td>ResiDualGAN</td>
          <td>Residual Dual GAN Architecture</td>
          <td>Jan 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2201.10563" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>306</td>
          <td>You Only Cut Once</td>
          <td>Single-Shot Instance Segmentation</td>
          <td>Jan 2022</td>
          <td>ByteDance AI Lab</td>
          <td><a href="https://arxiv.org/abs/2201.09898" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>307</td>
          <td>KFIoU Loss</td>
          <td>Kalman Filter IoU Loss Function</td>
          <td>Jan 2022</td>
          <td>Tongji University</td>
          <td><a href="https://arxiv.org/abs/2201.12558" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>308</td>
          <td>StyleGAN3 Editing</td>
          <td>Image and Video Editing Framework</td>
          <td>Jan 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2201.13433" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>309</td>
          <td>Block-NeRF</td>
          <td>City-scale Neural Radiance Fields using blocked-based decomposition</td>
          <td>Jan 2022</td>
          <td>Waymo/Google Research</td>
          <td><a href="https://arxiv.org/abs/2202.08614" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>310</td>
          <td>SeMask</td>
          <td>Semantically Masked Transformers</td>
          <td>Feb 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2202.14824" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>311</td>
          <td>SLIP</td>
          <td>Self-supervision with Language-Image Pre-training</td>
          <td>Feb 2022</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2202.06629" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>312</td>
          <td>Deformable ViT</td>
          <td>Vision Transformer with Deformable Attention</td>
          <td>Feb 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2201.00520" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>313</td>
          <td>Lawin Transformer</td>
          <td>Lightweight Transformer for Segmentation</td>
          <td>Feb 2022</td>
          <td>Nanjing University</td>
          <td><a href="https://arxiv.org/abs/2201.09792" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>314</td>
          <td>HyperionSolarNet</td>
          <td>Solar Panel Detection Network</td>
          <td>Feb 2022</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2202.05402" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>315</td>
          <td>KerGNNs</td>
          <td>Kernel Graph Neural Networks</td>
          <td>Feb 2022</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2202.03183" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>316</td>
          <td>gDNA</td>
          <td>Geometric DNA Networks</td>
          <td>Feb 2022</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2202.01949" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>317</td>
          <td>HYDRA</td>
          <td>Hybrid Deep Learning Architecture</td>
          <td>Feb 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2202.00833" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>318</td>
          <td>DDU-Net</td>
          <td>Dense Dual-Path U-Net</td>
          <td>Feb 2022</td>
          <td>Shanghai Jiao Tong University</td>
          <td><a href="https://arxiv.org/abs/2202.02681" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>319</td>
          <td>SPAMs</td>
          <td>Spatial Attention Modules</td>
          <td>Feb 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2202.05910" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>320</td>
          <td>ReLICv2</td>
          <td>Representation Learning with Image Consistency</td>
          <td>Feb 2022</td>
          <td>Meta AI Research</td>
          <td><a href="https://arxiv.org/abs/2202.02534" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>321</td>
          <td>Momentum Capsules</td>
          <td>Dynamic Routing with Momentum</td>
          <td>Feb 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2202.03611" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>322</td>
          <td>SAR Despecking</td>
          <td>Transformer for SAR Image Denoising</td>
          <td>Feb 2022</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2202.01424" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>323</td>
          <td>VRT</td>
          <td>Video Restoration Transformer</td>
          <td>Feb 2022</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2201.12288" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>324</td>
          <td>StyleGAN-XL</td>
          <td>Extra Large Scale StyleGAN</td>
          <td>Feb 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2202.00273" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>325</td>
          <td>AlphaCode</td>
          <td>Code Generation AI System</td>
          <td>Feb 2022</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2203.07814" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>326</td>
          <td>StyleGAN-Human</td>
          <td>Human image synthesis using StyleGAN</td>
          <td>Apr 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2204.11823" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>327</td>
          <td>How Do Vision Transformers Work?</td>
          <td>Analysis of internal mechanisms of Vision Transformers</td>
          <td>Jun 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2202.06709" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>328</td>
          <td>FERV39k</td>
          <td>Facial Expression Recognition Dataset with 39k samples</td>
          <td>Jun 2022</td>
          <td>South China University of Technology</td>
          <td><a href="https://arxiv.org/abs/2205.01036" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>329</td>
          <td>DaViT</td>
          <td>Data-efficient Vision Transformer</td>
          <td>Jul 2022</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2204.03645" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>330</td>
          <td>BEVFormer</td>
          <td>Bird&rsquo;s Eye View Transformer for autonomous driving</td>
          <td>Aug 2022</td>
          <td>Shanghai AI Lab</td>
          <td><a href="https://arxiv.org/abs/2203.17270" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>331</td>
          <td>TensoRF</td>
          <td>Tensorial Radiance Fields for efficient 3D reconstruction</td>
          <td>Sep 2022</td>
          <td>Zhejiang University</td>
          <td><a href="https://arxiv.org/abs/2203.09517" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>332</td>
          <td>WebFace260M</td>
          <td>Large-scale face recognition dataset</td>
          <td>Sep 2022</td>
          <td>InsightFace</td>
          <td><a href="https://arxiv.org/abs/2209.03358" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>333</td>
          <td>Neighborhood Attention Transformer</td>
          <td>Local attention mechanism for vision tasks</td>
          <td>Oct 2022</td>
          <td>Meta AI Research</td>
          <td><a href="https://arxiv.org/abs/2204.07143" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>334</td>
          <td>Barbershop</td>
          <td>Hair editing and synthesis framework</td>
          <td>Oct 2022</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2210.07574" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>335</td>
          <td>Visual Attention Network</td>
          <td>Novel attention mechanism for computer vision</td>
          <td>Nov 2022</td>
          <td>Meta AI Research</td>
          <td><a href="https://arxiv.org/abs/2211.12156" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>336</td>
          <td>MaskGIT</td>
          <td>Masked Generative Image Transformer</td>
          <td>Nov 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2202.04200" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>337</td>
          <td>CenterNet++</td>
          <td>Improved CenterNet for object detection</td>
          <td>Nov 2022</td>
          <td>University of Texas</td>
          <td><a href="https://arxiv.org/abs/2211.12931" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>338</td>
          <td>Patch-NetVLAD+</td>
          <td>Enhanced visual place recognition using patch-based features</td>
          <td>Dec 2022</td>
          <td>Oxford University</td>
          <td><a href="https://arxiv.org/abs/2212.03271" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>339</td>
          <td>PENCIL</td>
          <td>Probabilistic end-to-end noise correction</td>
          <td>Dec 2022</td>
          <td>NTU Singapore</td>
          <td><a href="https://arxiv.org/abs/2212.04765" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>340</td>
          <td>CenterSnap</td>
          <td>Center-based 3D object pose estimation</td>
          <td>Dec 2022</td>
          <td>Intel Labs</td>
          <td><a href="https://arxiv.org/abs/2212.07132" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>341</td>
          <td>AGCN</td>
          <td>Adaptive Graph Convolutional Network</td>
          <td>Dec 2022</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2212.09123" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>342</td>
          <td>AutoAvatar</td>
          <td>Automated avatar generation from images</td>
          <td>Dec 2022</td>
          <td>Tencent AI Lab</td>
          <td><a href="https://arxiv.org/abs/2212.08063" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>343</td>
          <td>Balanced MSE</td>
          <td>Balanced Mean Squared Error for imbalanced data</td>
          <td>Dec 2022</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2212.06784" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>344</td>
          <td>ReCLIP</td>
          <td>Improved CLIP with region-based features</td>
          <td>Dec 2022</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2212.09741" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>345</td>
          <td>EditGAN</td>
          <td>GAN-based image editing framework</td>
          <td>Dec 2022</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2212.04407" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>346</td>
          <td>HuMMan</td>
          <td>Human Motion and Manipulation dataset</td>
          <td>Dec 2022</td>
          <td>Max Planck Institute</td>
          <td><a href="https://arxiv.org/abs/2212.10380" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>347</td>
          <td>BlobGAN</td>
          <td>Unsupervised part-aware image generation</td>
          <td>Dec 2022</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2212.12143" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>348</td>
          <td>Deep Spectral Methods</td>
          <td>Spectral analysis for deep learning</td>
          <td>Dec 2022</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2212.11815" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>349</td>
          <td>TransformNet</td>
          <td>Transformer-based architecture for geometry transformation</td>
          <td>Jan 2023</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2301.09642" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>350</td>
          <td>Mirror-YOLO</td>
          <td>YOLO variant using mirror augmentation for detection</td>
          <td>Jan 2023</td>
          <td>Peking University</td>
          <td><a href="https://arxiv.org/abs/2301.07589" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>351</td>
          <td>Paying U-Attention to Textures</td>
          <td>U-Net based texture synthesis with attention</td>
          <td>Jan 2023</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2301.05225" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>352</td>
          <td>ZippyPoint</td>
          <td>Fast point cloud processing architecture</td>
          <td>Jan 2023</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2301.06555" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>353</td>
          <td>InsetGAN for Full-Body Image Generation</td>
          <td>GAN-based full-body image synthesis</td>
          <td>Jan 2023</td>
          <td>Max Planck Institute</td>
          <td><a href="https://arxiv.org/abs/2301.05270" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>354</td>
          <td>Mixed Differential Privacy</td>
          <td>Privacy-preserving vision model training</td>
          <td>Jan 2023</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2301.07845" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>355</td>
          <td>L³U-Net</td>
          <td>Lightweight U-Net variant with enhanced learning</td>
          <td>Jan 2023</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2301.05220" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>356</td>
          <td>RBGNet</td>
          <td>Residual Bidirectional Graph Network</td>
          <td>Jan 2023</td>
          <td>Peking University</td>
          <td><a href="https://arxiv.org/abs/2301.09712" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>357</td>
          <td>TopFormer</td>
          <td>Top-down Transformer for vision tasks</td>
          <td>Jan 2023</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2301.04566" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>358</td>
          <td>CLIP-GEN</td>
          <td>CLIP-guided image generation</td>
          <td>Jan 2023</td>
          <td>OpenAI</td>
          <td><a href="https://arxiv.org/abs/2301.08318" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>359</td>
          <td>DANBO</td>
          <td>Dynamic Attention Network for Body Pose</td>
          <td>Jan 2023</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2301.04569" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>360</td>
          <td>KeypointNeRF</td>
          <td>NeRF with keypoint conditioning</td>
          <td>Jan 2023</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2301.05175" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>361</td>
          <td>VOS (Visual Object Streaming)</td>
          <td>Efficient streaming framework for video object segmentation</td>
          <td>Feb 2023</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2302.14536" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>362</td>
          <td>ScoreNet</td>
          <td>Score-based generative modeling for point cloud generation</td>
          <td>Feb 2023</td>
          <td>UC Berkeley</td>
          <td><a href="https://arxiv.org/abs/2302.07241" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>363</td>
          <td>GroupViT</td>
          <td>Vision Transformer with dynamic grouping mechanism</td>
          <td>Feb 2023</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2302.05442" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>364</td>
          <td>TCTrack</td>
          <td>Temporal context-aware tracking framework</td>
          <td>Feb 2023</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="https://arxiv.org/abs/2302.03030" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>365</td>
          <td>MLSeg</td>
          <td>Multi-level semantic segmentation framework</td>
          <td>Feb 2023</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2302.07459" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>366</td>
          <td>StyleBabel</td>
          <td>Text-guided style transfer using BABEL embeddings</td>
          <td>Feb 2023</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2302.04268" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>367</td>
          <td>Mixed DualStyleGAN</td>
          <td>Dual-domain style transfer with mixed training</td>
          <td>Feb 2023</td>
          <td>NVIDIA</td>
          <td><a href="https://arxiv.org/abs/2302.06384" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>368</td>
          <td>StyleT2I</td>
          <td>Style-based text-to-image generation</td>
          <td>Feb 2023</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2302.09775" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>369</td>
          <td>SPAct</td>
          <td>Spatial-temporal action recognition</td>
          <td>Feb 2023</td>
          <td>University of Oxford</td>
          <td><a href="https://arxiv.org/abs/2302.08780" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>370</td>
          <td>JIFF</td>
          <td>Joint Image and Feature Fusion</td>
          <td>Feb 2023</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2302.05175" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>371</td>
          <td>C3-STISR</td>
          <td>Cross-Camera Stereo Image Super-Resolution</td>
          <td>Feb 2023</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2302.11274" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>372</td>
          <td>IVY</td>
          <td>Integrated Vision System</td>
          <td>Feb 2023</td>
          <td>Intel Research</td>
          <td><a href="https://arxiv.org/abs/2302.07482" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>373</td>
          <td>StyLandGAN</td>
          <td>Stylized landscape generation</td>
          <td>Feb 2023</td>
          <td>NVIDIA Research</td>
          <td><a href="https://arxiv.org/abs/2302.09225" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>374</td>
          <td>NeuralFusion</td>
          <td>Neural fusion for 3D reconstruction using implicit representations</td>
          <td>Mar 2023</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2303.07662" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>375</td>
          <td>COLA</td>
          <td>Contrastive learning approach for visual recognition</td>
          <td>Mar 2023</td>
          <td>Stanford University</td>
          <td><a href="https://arxiv.org/abs/2303.14353" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>376</td>
          <td>VLP (Vision-Language Pre-training)</td>
          <td>Joint pre-training for vision and language tasks</td>
          <td>Mar 2023</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2303.08128" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>377</td>
          <td>Level-K to Nash Equilibrium</td>
          <td>Game theoretic approach to vision problems</td>
          <td>Mar 2023</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2303.09913" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>378</td>
          <td>HyperTransformer</td>
          <td>Hypernetwork-based transformer for vision tasks</td>
          <td>Mar 2023</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2303.11174" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>379</td>
          <td>GrainSpace</td>
          <td>Granular spatial representation learning</td>
          <td>Mar 2023</td>
          <td>Carnegie Mellon University</td>
          <td><a href="https://arxiv.org/abs/2303.09876" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>380</td>
          <td>ROOD-MRI</td>
          <td>Robust out-of-distribution detection for medical imaging</td>
          <td>Mar 2023</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2303.13773" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>381</td>
          <td>Bamboo</td>
          <td>Framework for efficient neural architecture search</td>
          <td>Mar 2023</td>
          <td>Microsoft Research</td>
          <td><a href="https://arxiv.org/abs/2303.15182" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>382</td>
          <td>BigDetection</td>
          <td>Large-scale object detection framework</td>
          <td>Mar 2023</td>
          <td>Facebook AI Research</td>
          <td><a href="https://arxiv.org/abs/2303.12345" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>383</td>
          <td>TransEditor</td>
          <td>Transformer-based image editing framework</td>
          <td>Mar 2023</td>
          <td>Adobe Research</td>
          <td><a href="https://arxiv.org/abs/2303.09689" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>384</td>
          <td>Event Transformer</td>
          <td>Transformer architecture for event-based vision</td>
          <td>Mar 2023</td>
          <td>Intel Labs</td>
          <td><a href="https://arxiv.org/abs/2303.14435" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>385</td>
          <td>MVSTER</td>
          <td>Multi-view Stereo Transformer</td>
          <td>Mar 2023</td>
          <td>ETH Zürich</td>
          <td><a href="https://arxiv.org/abs/2303.13794" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>386</td>
          <td>CLIP-Art</td>
          <td>CLIP-based artistic image synthesis</td>
          <td>Mar 2023</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2303.17492" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>387</td>
          <td>Sequencer</td>
          <td>Sequential modeling for vision tasks</td>
          <td>Mar 2023</td>
          <td>Google Research</td>
          <td><a href="https://arxiv.org/abs/2303.16104" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>388</td>
          <td>GraphWorld</td>
          <td>Benchmark for graph neural networks</td>
          <td>Mar 2023</td>
          <td>DeepMind</td>
          <td><a href="https://arxiv.org/abs/2303.16698" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>389</td>
          <td>F8Net</td>
          <td>Lightweight network for efficient feature extraction</td>
          <td>Apr 2023</td>
          <td>Tsinghua University</td>
          <td><a href="https://arxiv.org/abs/2304.12510" target="_blank">Paper</a></td>
      </tr>
      <tr>
          <td>390</td>
          <td>LatentFormer</td>
          <td>Transformer architecture for latent space manipulation</td>
          <td>Apr 2023</td>
          <td>MIT</td>
          <td><a href="https://arxiv.org/abs/2304.08281" target="_blank">Paper</a></td>
      </tr>
  </tbody>
</table>


        
        
      </div>

      <style>
 

</style><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Dr. Hari Thapliyaal's avatar" class="author-image" width="25%" >
      </div>
    <div class="td-author-box__links author-image"><b>Follow Me</b>
        <a href="https://join.slack.com/t/dasarpaiworkspace/shared_invite/zt-371kuyco2-gsuhnVgMfQ_9aXPRFgiP3Q" target="_blank" rel="noopener" aria-label="Slack" title="Chat with other project users in #users">
            <i class="fab fa-slack" aria-hidden="true"></i>
        </a>
        <a href="https://groups.google.com/forum/#!forum/agones-discuss" target="_blank" rel="noopener" aria-label="User mailing list" title="Discussion and help from your fellow users">
            <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        <a href="https://twitter.com/dasarpai" target="_blank" rel="noopener" aria-label="Twitter" title="Follow us on Twitter to get the latest news!">
            <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
    </div>
  

    <div class="td-author-box__info">
    <h4 class="td-author-box__name">Dr. Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>
      <div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>
      

      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    <b>Share with :</b> 
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/dsblog/computer-vision-research-work/&amp;title=Computer%20Vision%20Research%20Work"
      title="Share on LinkedIn"
      aria-label="Share on LinkedIn"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://twitter.com/intent/tweet/?url=/dsblog/computer-vision-research-work/&amp;text=Computer%20Vision%20Research%20Work"
      title="Tweet on Twitter"
      aria-label="Tweet on Twitter"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://api.whatsapp.com/send?text=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Share via WhatsApp"
      aria-label="Share via WhatsApp"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://t.me/share/url?url=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Share via Telegram"
      aria-label="Share via Telegram"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://pinterest.com/pin/create/bookmarklet/?url=/dsblog/computer-vision-research-work/&amp;description=Computer%20Vision%20Research%20Work"
      title="Pin on Pinterest"
      aria-label="Pin on Pinterest"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M496 256c0 137-111 248-248 248-25.6 0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8 0 128.7-68.8 128.7-154.3 0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1 0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6 0-54.7 41.4-107.6 112-107.6 60.9 0 103.6 41.5 103.6 100.9 0 67.1-33.9 113.6-78 113.6-24.3 0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6 0-19-10.2-34.9-31.4-34.9-24.9 0-44.9 25.7-44.9 60.2 0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9 0 361.1 0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://www.facebook.com/sharer/sharer.php?u=/dsblog/computer-vision-research-work/&amp;quote=Computer%20Vision%20Research%20Work"
      title="Share on Facebook"
      aria-label="Share on Facebook"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="mailto:?body=/dsblog/computer-vision-research-work/&amp;subject=Computer%20Vision%20Research%20Work"
      title="Send via email"
      aria-label="Send via email"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://bsky.app/intent/compose?text=Computer%20Vision%20Research%20Work&#43;/dsblog/computer-vision-research-work/"
      title="Post on Bluesky"
      aria-label="Post on Bluesky"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg>
  </span>


    </a>
      
    
      
      <a
      target="_blank"
      class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
      href="https://reddit.com/submit/?url=/dsblog/computer-vision-research-work/&amp;resubmit=true&amp;title=Computer%20Vision%20Research%20Work"
      title="Submit to Reddit"
      aria-label="Submit to Reddit"
      >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>


    </a>
      
    
  </section>


    </div>

    
      
      
        
        
      

      <script>
        var oid = "views_dsblog\\2025-01-27-6211-Computer-Vision-Research-Work.md";
        var oid_likes = "likes_dsblog\\2025-01-27-6211-Computer-Vision-Research-Work.md";
      </script>
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
    
  </section>

  <footer class="pt-8 print:hidden">
    
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group mr-3" href="/dsblog/exploring-ai-benchmarks-and-leaderboards/">
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Exploring AI Benchmarks & Leaderboards</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-01-26T00:00:00&#43;00:00">January 26, 2025</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/dsblog/power-of-chinese-ai-models/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Power of Chinese AI Models</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-01-28T00:00:00&#43;00:00">January 28, 2025</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>

  <div>
    


  
  
    
    
    
    
    
      
    
    
    
    
    
    
    
    
      
    
    
    
    
    
      
    

    
    
    
    

    
    
      <h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2>
      <section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3">
        
            
            


  <a href="/dsblog/roadmap-to-reality/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6286-roadmap-to-reality.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/roadmap-to-reality/">
	   Roadmap to Reality
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-06-14T00:00:00&#43;00:00">June 14, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>916 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-06-14-6286-Roadmap-to-Reality.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/philosophy--cognitive-science/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Philosophy &amp; Cognitive Science
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/interdisciplinary-topics/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Interdisciplinary Topics
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/scientific-journey/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Scientific Journey
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/self-discovery/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Self-Discovery
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/personal-growth/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Growth
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/cosmic-perspective/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cosmic Perspective
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/human-evolution/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Human Evolution
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/technology/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technology
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/biology/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Biology
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/neuroscience/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Neuroscience
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Roadmap to Reality # A Scientific Journey to Know the Universe — and the Self # 🌱 Introduction: The …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/from-being-hacked-to-being-reborn-linkedin-profile/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6285-from-being-hacked-to-being-reborn-linkedin-profile.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/from-being-hacked-to-being-reborn-linkedin-profile/">
	   From Being Hacked to Being Reborn: How I Rebuilt My LinkedIn Identity in 48 Hours
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-06-11T00:00:00&#43;00:00">June 11, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>893 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-06-11-6285-From-Being-Hacked-to-Being-Reborn-Linkedin-Profile.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/personal-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Branding
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/cybersecurity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cybersecurity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/technology-trends--future/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technology Trends &amp; Future
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/personal-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Personal Branding
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/linkedin-profile/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    LinkedIn Profile
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/professional-identity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Professional Identity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/cybersecurity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Cybersecurity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/online-presence/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Online Presence
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/digital-identity/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Digital Identity
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/online-branding/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Online Branding
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     💔 From Being Hacked to Being Reborn: How I Rebuilt My LinkedIn Identity in 48 Hours # &ldquo;In …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/exploring-css-frameworks/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6284-exploring-css-frameworks.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/exploring-css-frameworks/">
	   Exploring CSS Frameworks - A Collection of Lightweight, Responsive, and Themeable Alternatives
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-30T00:00:00&#43;00:00">May 30, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>1378 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">7 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-30-6284-Exploring-CSS-Frameworkds.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/web-development/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Web Development
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/frontend-development/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Frontend Development
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/design-systems/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Design Systems
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/css-frameworks/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    CSS Frameworks
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/lightweight-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Lightweight CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/responsive-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Responsive CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/themeable-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Themeable CSS
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/css-utilities/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    CSS Utilities
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/utility-first-css/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Utility-First CSS
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Exploring CSS Frameworks # There are many CSS frameworks and approaches you can use besides …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/dimensions-of-software-architecture/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6283-dimensions-of-software-architecture.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/dimensions-of-software-architecture/">
	   Dimensions of Software Architecture: Balancing Concerns
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-27T00:00:00&#43;00:00">May 27, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>871 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">5 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-27-6283-Dimensions-of-Software-Architecture.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/software-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Software Architecture
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/software-architecture/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Software Architecture
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/technical-debt/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Technical Debt
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/maintainability/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Maintainability
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/scalability/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Scalability
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/performance/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Performance
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Dimensions of Software Architecture # Call these &ldquo;Architectural Concern Categories&rdquo; or …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
            
            


  <a href="/dsblog/Understanding-async-await-and-Concurrency/" class="min-w-full">

  <div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative">
  
      
    
      
        
      
    
        
                
          <div class="w-full thumbnail_card_related nozoom" style="background-image:url(/assets/images/dspost/dsp6282-Understanding-async-await-and-Concurrency.jpg);"></div>
        
      

  <div class="px-6 py-4">
  
    <div class="font-bold text-xl text-neutral-800 decoration-primary-500 
       hover:underline hover:underline-offset-2 dark:text-neutral"
       href="/dsblog/Understanding-async-await-and-Concurrency/">
	   Understanding `async`, `await`, and Concurrency in Python
	</div>
  

  <div class="text-sm text-neutral-500 dark:text-neutral-400">
     











  





  



  





  









<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2025-05-27T00:00:00&#43;00:00">May 27, 2025</time><span class="px-2 text-primary-500">&middot;</span><span>637 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">3 mins</span><span class="px-2 text-primary-500">&middot;</span><span>
  
  
    
    
      
      
        
        
      
      
    
  
  <span id="likes_dsblog\2025-05-27-6282-Understanding-async-await-and-Concurrency.md"
    class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400"
    title="likes">loading</span>
  <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
<path fill="currentColor" d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
  </span>

</span>
</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/categories/python/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Python
  </span>
</span>
  </span>
  
  
  
  
  
  
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/asyncio/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Asyncio
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/concurrency/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Concurrency
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/synchronous-programming/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Synchronous Programming
  </span>
</span>
  </span>
  
  <span style="margin-top:0.5rem" class="mr-2" onclick="window.open(&#34;/tags/asynchronous-programming/&#34;,'_self');return false;">
    <span class="flex" style="cursor: pointer;">
  <span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Asynchronous Programming
  </span>
</span>
  </span>
  
  
  
  
</div>




  </div>

  
    <div class="py-1 prose dark:prose-invert">
     Understanding async, await, and Concurrency # Understanding async, await, and Concurrency in Python …
    </div>
  
</div>
<div class="px-6 pt-4 pb-2"></div>
</div>
</a>

          
        
      </section>
    
  

  </div>
</article>

      <div id="top-scroller" style="position: fixed; bottom: 2rem; left: 0; right: 0; display: flex; justify-content: center; z-index: 9999;">
   <a href="#the-top"
      style="display: flex; height: 3.5rem; width: 3.5rem; align-items: center; justify-content: center; border-radius: 9999px; background-color: rgba(107, 114, 128, 0.8); color: white; transition: all 0.3s; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);"
      onmouseover="this.style.backgroundColor='rgba(75, 85, 99, 0.9)'; this.style.boxShadow='0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05)';"
      onmouseout="this.style.backgroundColor='rgba(107, 114, 128, 0.8)'; this.style.boxShadow='0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)';"
      aria-label="Scroll to top" 
      title="Scroll to top">
     <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
       <line x1="12" y1="19" x2="12" y2="5"></line>
       <polyline points="5 12 12 5 19 12"></polyline>
     </svg>
   </a>
 </div>
    </main><style>
  footer#site-footer {
    background-color: #0e0517;
  }

  footer .row {
    display: flex;
    flex-wrap: wrap;

  }

  footer .row .col-md-4 {
    flex: 0 0 33.333333%;
    max-width: 33.333333%;
    padding: 15px
  }
</style>


<footer id="site-footer" class="py-10 print:hidden mt-5  rounded-md">
  
  
    <div class="row">
      <div class="container">
        <div class="row g-2">  
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Key Links</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/aboutme" title=""> 
          
          About Me
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/clients" title=""> 
          
          Clients
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/mycertifications" title=""> 
          
          My Certifications
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/testimonials" title=""> 
          
          Testimonial
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmlogy-home" title=""> 
          
          PMLOGY Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wia-home" title=""> 
          
          WIA Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra-home" title=""> 
          
          SamskrutYatra Home
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/publications-home" title=""> 
          
          Publications
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Services</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dscourses" title=""> 
          
          Data Science Courses/Services
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/project-index-page" title=""> 
          
          Project/Work Catalog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-al-ml-projects" title=""> 
          
          MyWork by Business Domain
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-my-technology-stacks" title=""> 
          
          MyWork by Tech Stack
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/projects/summary-of-management-projects" title=""> 
          
          MyWork in Project Management
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/management" title=""> 
          
          Management Courses/Services
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">My Blogs</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog" title=""> 
          
          Data Science Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts" title=""> 
          
          Wisdom in Awareness Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/quotations" title=""> 
          
          Wisdom Quotes
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra" title=""> 
          
          Samskrut Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/categories/mychanting" title=""> 
          
          My Chantings
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk" title=""> 
          
          GK Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary" title=""> 
          
          Books/Interviews Blog
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/news" title=""> 
          
          AI and Business News
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog" title=""> 
          
          PMLOGY Blog
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Data Science Resources</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/categories/data-science-resources/" title=""> 
          
          DS Resources
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="https://aibenchmark-explorer.dasarpai.com" title=""> 
          
          AI Benchmark Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/ds-ai-ml-books" title=""> 
          
          Data Science Books
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/data-science-cheatsheets" title=""> 
          
          Data Science/AI Cheatsheets
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/best-youtube-channels-for-ds" title=""> 
          
          Video Channels to Learn DS/AI
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/ds-ai-ml-interview-resources" title=""> 
          
          DS/AI Interview Questions
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="https://github.com/dasarpai/DAI-Datasets" title=""> 
          
          GitHub DAI-Datasets
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/corpus" title=""> 
          
          History Corpus
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">PM Resources</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6" title=""> 
          
          PMBOK6 Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi" title=""> 
          
          PMBOK6 Hindi Explorer
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmglossary" title=""> 
          
          PM Glossary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6-summary" title=""> 
          
          PMBOK6 Summary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi-summary" title=""> 
          
          PMBoK6 Hindi Summary
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmi-templates" title=""> 
          
          PMBOK6 Templates
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/prince2-templates" title=""> 
          
          PRINCE2 Templates
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/microsoft-pm-templates" title=""> 
          
          Microsoft PM Templates
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Tags</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/tags" title=""> 
          
          Data Science Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts/tags" title=""> 
          
          Wisdom in Awareness Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra/tags" title=""> 
          
          Samskrut Yatra Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog/tags" title=""> 
          
          Project Management Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6/tags" title=""> 
          
          PMBOK6 Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmbok6hi/tags" title=""> 
          
          PMBOK6hi Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary/tags" title=""> 
          
          Booksummary Tags
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk/tags" title=""> 
          
          GK Tags
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Topics/Categories</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/dsblog/categories" title=""> 
          
          Data Science Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/wiaposts/categories" title=""> 
          
          Wisdom in Awareness Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/samskrutyatra/categories" title=""> 
          
          Samskrut Yatra Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/pmblog/categories" title=""> 
          
          Project Management Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/booksummary/categories" title=""> 
          
          Booksummary Categories
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gk/categories" title=""> 
          
          GK Categories
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Gallery</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions1" title=""> 
          
          Online AI Classes 1
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions2" title=""> 
          
          Online AI Classes 2
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions3" title=""> 
          
          Online AI Classes 3
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-online-sessions4" title=""> 
          
          Online AI Classes 4
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-pm-selected-photos" title=""> 
          
          Management Classes
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/gallery/slider-pm-workshops" title=""> 
          
          PM &amp; DS Workshop
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="" title=""> 
          
          
        </a>
      </li>
    
  </ul>
</nav>
</div>
          <div class="col-6 col-sm-4 col-lg-3 col-xl-2 mb-3"><style>
   .font-bold {
      color: yellow;
  }
</style>




<nav class="flex flex-col text-base font-medium text-neutral-500 dark:text-neutral-400 px-4"> 
  

    <div class="font-bold text-white mb-1">Terms and Policies</div> 
  
  <ul class="space-y-1"> 
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/terms-of-service" title=""> 
          
          Terms &amp; Condition
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/privacy" title=""> 
          
          Privacy Policy
        </a>
      </li>
    


      <li class="flex ltr:text-right rtl:text-left">
        <a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center py-0.5" 
           href="/comment-policy" title=""> 
          
          Comment Policy
        </a>
      </li>
    
  </ul>
</nav>
</div>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-md-4">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="/assets/images/site-logo.png" alt="dasarpAI" width="100"
            style="border-radius: 12px;">
        </a>
      </div>
    </div>

  
  <div class="flex items-center justify-between p-4">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2025
      Dr. Hari Thapliyaal
    </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    

  </div>



  <script>
    
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: 'rgba(0,0,0,0.5)',
        scrollOffset: 0,
    })
    
  </script>
  
  

  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA==">
  </script>
    
  
  
    <a rel="me" href="https://masto.ai/@blowfish"></a>
  
</footer><div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js"
  data-id="harithapliyal" data-description="Support me on Buy me a coffee!" data-message=""
  data-color="#FFDD00" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

</html>
